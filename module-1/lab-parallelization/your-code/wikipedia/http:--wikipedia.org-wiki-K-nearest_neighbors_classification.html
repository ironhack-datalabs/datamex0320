<!DOCTYPE html>
<html class="client-nojs" dir="ltr" lang="en">
<head>
<meta charset="utf-8"/>
<title>k-nearest neighbors algorithm - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"Xo-CeQpAAEQAAEek3qQAAACJ","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"K-nearest_neighbors_algorithm","wgTitle":"K-nearest neighbors algorithm","wgCurRevisionId":949222834,"wgRevisionId":949222834,"wgArticleId":1775388,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["All articles with unsourced statements","Articles with unsourced statements from March 2013","Articles with unsourced statements from December 2008","Wikipedia articles needing clarification from January 2019","Articles with unsourced statements from September 2019",
"CS1 maint: uses editors parameter","Classification algorithms","Search algorithms","Machine learning algorithms","Statistical classification","Nonparametric statistics"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"K-nearest_neighbors_algorithm","wgRelevantArticleId":1775388,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgRedirectedFrom":"K-nearest_neighbors_classification","wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgInternalRedirectTargetUrl":"/wiki/K-nearest_neighbors_algorithm",
"wgWikibaseItemId":"Q1071612","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","mediawiki.page.gallery.styles":"ready","mediawiki.toc.styles":"ready","skins.vector.styles.legacy":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready"};RLPAGEMODULES=["mediawiki.action.view.redirect","ext.cite.ux-enhancements","ext.math.scripts","site","mediawiki.page.startup","skins.vector.js","mediawiki.page.ready","mediawiki.toc","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups",
"ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.page.gallery.styles%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector" rel="stylesheet"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta content="" name="ResourceLoaderDynamicStyles"/>
<link href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector" rel="stylesheet"/>
<meta content="MediaWiki 1.35.0-wmf.27" name="generator"/>
<meta content="origin" name="referrer"/>
<meta content="origin-when-crossorigin" name="referrer"/>
<meta content="origin-when-cross-origin" name="referrer"/>
<meta content="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/1200px-Kernel_Machine.svg.png" property="og:image"/>
<link href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit" rel="alternate" title="Edit this page" type="application/x-wiki"/>
<link href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit" rel="edit" title="Edit this page"/>
<link href="/static/apple-touch/wikipedia.png" rel="apple-touch-icon"/>
<link href="/static/favicon/wikipedia.ico" rel="shortcut icon"/>
<link href="/w/opensearch_desc.php" rel="search" title="Wikipedia (en)" type="application/opensearchdescription+xml"/>
<link href="//en.wikipedia.org/w/api.php?action=rsd" rel="EditURI" type="application/rsd+xml"/>
<link href="//creativecommons.org/licenses/by-sa/3.0/" rel="license"/>
<link href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" rel="alternate" title="Wikipedia Atom feed" type="application/atom+xml"/>
<link href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" rel="canonical"/>
<link href="//login.wikimedia.org" rel="dns-prefetch"/>
<link href="//meta.wikimedia.org" rel="dns-prefetch"/>
<!--[if lt IE 9]><script src="/w/resources/lib/html5shiv/html5shiv.js"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-K-nearest_neighbors_algorithm rootpage-K-nearest_neighbors_algorithm skin-vector action-view">
<div class="noprint" id="mw-page-base"></div>
<div class="noprint" id="mw-head-base"></div>
<div class="mw-body" id="content" role="main">
<a id="top"></a>
<div class="mw-body-content" id="siteNotice"><!-- CentralNotice --></div>
<div class="mw-indicators mw-body-content">
</div>
<h1 class="firstHeading" id="firstHeading" lang="en"><i>k</i>-nearest neighbors algorithm</h1>
<div class="mw-body-content" id="bodyContent">
<div class="noprint" id="siteSub">From Wikipedia, the free encyclopedia</div>
<div id="contentSub"><span class="mw-redirectedfrom">  (Redirected from <a class="mw-redirect" href="/w/index.php?title=K-nearest_neighbors_classification&amp;redirect=no" title="K-nearest neighbors classification">K-nearest neighbors classification</a>)</span></div>
<div id="jump-to-nav"></div>
<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
<a class="mw-jump-link" href="#p-search">Jump to search</a>
<div class="mw-content-ltr" dir="ltr" id="mw-content-text" lang="en"><div class="mw-parser-output"><div class="hatnote navigation-not-searchable" role="note">Not to be confused with <a href="/wiki/K-means_clustering" title="K-means clustering">k-means clustering</a>.</div>
<table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a> and<br/><a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;"><a class="image" href="/wiki/File:Kernel_Machine.svg"><img alt="Kernel Machine.svg" data-file-height="233" data-file-width="512" decoding="async" height="100" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/220px-Kernel_Machine.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x" width="220"/></a></td></tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><div style="padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br/><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b> • <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>
<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>
<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>
<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>
<li><a class="mw-selflink selflink"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="/wiki/Support-vector_machine" title="Support-vector machine">Support vector machine (SVM)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a class="mw-redirect" href="/wiki/CURE_data_clustering_algorithm" title="CURE data clustering algorithm">CURE</a></li>
<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li>
<li><br/><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a class="mw-redirect" href="/wiki/Mean-shift" title="Mean-shift">Mean-shift</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>
<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>
<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>
<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a class="mw-redirect" href="/wiki/K-nearest_neighbors_classification" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>
<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li></ul></li>
<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>
<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a class="mw-redirect" href="/wiki/Bias%E2%80%93variance_dilemma" title="Bias–variance dilemma">Bias–variance dilemma</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine-learning venues</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>
<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>
<li><a class="external text" href="https://arxiv.org/list/cs.LG/recent" rel="nofollow">ArXiv:cs.LG</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Related articles</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>
<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul>
</div></div></div></td>
</tr><tr><td style="text-align:right;font-size:115%;padding-top: 0.6em;"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<p>In <a href="/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a>, the <b><i>k</i>-nearest neighbors algorithm</b> (<b><i>k</i>-NN</b>) is a <a class="mw-redirect" href="/wiki/Non-parametric_statistics" title="Non-parametric statistics">non-parametric</a> method used for <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> and <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a>.<sup class="reference" id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup> In both cases, the input consists of the <i>k</i> closest training examples in the <a class="mw-redirect" href="/wiki/Feature_space" title="Feature space">feature space</a>. The output depends on whether <i>k</i>-NN is used for classification or regression:
</p>
<dl><dd><ul><li>In <i>k-NN classification</i>, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its <i>k</i> nearest neighbors (<i>k</i> is a positive <a href="/wiki/Integer" title="Integer">integer</a>, typically small). If <i>k</i> = 1, then the object is simply assigned to the class of that single nearest neighbor.</li></ul></dd></dl>
<dl><dd><ul><li>In <i>k-NN regression</i>, the output is the property value for the object. This value is the average of the values of <i>k</i> nearest neighbors.</li></ul></dd></dl>
<p><i>k</i>-NN is a type of <a href="/wiki/Instance-based_learning" title="Instance-based learning">instance-based learning</a>, or <a href="/wiki/Lazy_learning" title="Lazy learning">lazy learning</a>, where the function is only approximated locally and all computation is deferred until function evaluation.
</p><p>Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/<i>d</i>, where <i>d</i> is the distance to the neighbor.<sup class="reference" id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup>
</p><p>The neighbors are taken from a set of objects for which the class (for <i>k</i>-NN classification) or the object property value (for <i>k</i>-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.
</p><p>A peculiarity of the <i>k</i>-NN algorithm is that it is sensitive to the local structure of the data.
</p>
<div aria-labelledby="mw-toc-heading" class="toc" id="toc" role="navigation"><input class="toctogglecheckbox" id="toctogglecheckbox" role="button" style="display:none" type="checkbox"/><div class="toctitle" dir="ltr" lang="en"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Statistical_setting"><span class="tocnumber">1</span> <span class="toctext">Statistical setting</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Algorithm"><span class="tocnumber">2</span> <span class="toctext">Algorithm</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Parameter_selection"><span class="tocnumber">3</span> <span class="toctext">Parameter selection</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#The_1-nearest_neighbor_classifier"><span class="tocnumber">4</span> <span class="toctext">The <span>1</span>-nearest neighbor classifier</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#The_weighted_nearest_neighbour_classifier"><span class="tocnumber">5</span> <span class="toctext">The weighted nearest neighbour classifier</span></a></li>
<li class="toclevel-1 tocsection-6"><a href="#Properties"><span class="tocnumber">6</span> <span class="toctext">Properties</span></a></li>
<li class="toclevel-1 tocsection-7"><a href="#Error_rates"><span class="tocnumber">7</span> <span class="toctext">Error rates</span></a></li>
<li class="toclevel-1 tocsection-8"><a href="#Metric_learning"><span class="tocnumber">8</span> <span class="toctext">Metric learning</span></a></li>
<li class="toclevel-1 tocsection-9"><a href="#Feature_extraction"><span class="tocnumber">9</span> <span class="toctext">Feature extraction</span></a></li>
<li class="toclevel-1 tocsection-10"><a href="#Dimension_reduction"><span class="tocnumber">10</span> <span class="toctext">Dimension reduction</span></a></li>
<li class="toclevel-1 tocsection-11"><a href="#Decision_boundary"><span class="tocnumber">11</span> <span class="toctext">Decision boundary</span></a></li>
<li class="toclevel-1 tocsection-12"><a href="#Data_reduction"><span class="tocnumber">12</span> <span class="toctext">Data reduction</span></a>
<ul>
<li class="toclevel-2 tocsection-13"><a href="#Selection_of_class-outliers"><span class="tocnumber">12.1</span> <span class="toctext">Selection of class-outliers</span></a></li>
<li class="toclevel-2 tocsection-14"><a href="#CNN_for_data_reduction"><span class="tocnumber">12.2</span> <span class="toctext">CNN for data reduction</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-15"><a href="#k-NN_regression"><span class="tocnumber">13</span> <span class="toctext"><i>k</i>-NN regression</span></a></li>
<li class="toclevel-1 tocsection-16"><a href="#k-NN_outlier"><span class="tocnumber">14</span> <span class="toctext"><i>k</i>-NN outlier</span></a></li>
<li class="toclevel-1 tocsection-17"><a href="#Validation_of_results"><span class="tocnumber">15</span> <span class="toctext">Validation of results</span></a></li>
<li class="toclevel-1 tocsection-18"><a href="#See_also"><span class="tocnumber">16</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-19"><a href="#References"><span class="tocnumber">17</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-20"><a href="#Further_reading"><span class="tocnumber">18</span> <span class="toctext">Further reading</span></a></li>
</ul>
</div>
<h2><span class="mw-headline" id="Statistical_setting">Statistical setting</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=1" title="Edit section: Statistical setting">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Suppose we have pairs <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle (X_{1},Y_{1}),(X_{2},Y_{2}),\dots ,(X_{n},Y_{n})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo stretchy="false">(</mo>
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mo>,</mo>
<mo stretchy="false">(</mo>
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<mo stretchy="false">(</mo>
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle (X_{1},Y_{1}),(X_{2},Y_{2}),\dots ,(X_{n},Y_{n})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle (X_{1},Y_{1}),(X_{2},Y_{2}),\dots ,(X_{n},Y_{n})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f16142bcc75fbc4a7fece97df2d93185e831185" style="vertical-align: -0.838ex; width:31.22ex; height:2.843ex;"/></span> taking values in <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbb {R} ^{d}\times \{1,2\}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
</mrow>
</msup>
<mo>×<!-- × --></mo>
<mo fence="false" stretchy="false">{</mo>
<mn>1</mn>
<mo>,</mo>
<mn>2</mn>
<mo fence="false" stretchy="false">}</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbb {R} ^{d}\times \{1,2\}}</annotation>
</semantics>
</math></span><img alt="{\mathbb  {R}}^{d}\times \{1,2\}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/60671efc51865fcfac4e8939ab2acb643539302d" style="vertical-align: -0.838ex; width:11.294ex; height:3.176ex;"/></span>, where <span class="texhtml mvar" style="font-style:italic;">Y</span> is the class label of <span class="texhtml mvar" style="font-style:italic;">X</span>, so that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle X|Y=r\sim P_{r}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mi>Y</mi>
<mo>=</mo>
<mi>r</mi>
<mo>∼<!-- ∼ --></mo>
<msub>
<mi>P</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>r</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle X|Y=r\sim P_{r}}</annotation>
</semantics>
</math></span><img alt="X|Y=r\sim P_{r}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9e1b6a01b179f721be03ed284c21e2d3cbb70d4a" style="vertical-align: -0.838ex; width:14.112ex; height:2.843ex;"/></span> for <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle r=1,2}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>r</mi>
<mo>=</mo>
<mn>1</mn>
<mo>,</mo>
<mn>2</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle r=1,2}</annotation>
</semantics>
</math></span><img alt="r=1,2" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6dcdedf1b83451b9a51dc73b95f854eeaf83dc46" style="vertical-align: -0.671ex; width:7.506ex; height:2.509ex;"/></span> (and probability distributions <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle P_{r}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>P</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>r</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle P_{r}}</annotation>
</semantics>
</math></span><img alt="P_{r}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3e7f814e80c6ff1469112bd4b7430e358e86c7d6" style="vertical-align: -0.671ex; width:2.466ex; height:2.509ex;"/></span>). Given some norm <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \|\cdot \|}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
<mo>⋅<!-- ⋅ --></mo>
<mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \|\cdot \|}</annotation>
</semantics>
</math></span><img alt="\|\cdot \|" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/113f0d8fe6108fc1c5e9802f7c3f634f5480b3d1" style="vertical-align: -0.838ex; width:4.004ex; height:2.843ex;"/></span> on <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mathbb {R} ^{d}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mathbb {R} ^{d}}</annotation>
</semantics>
</math></span><img alt="\mathbb {R} ^{d}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a713426956296f1668fce772df3c60b9dde8a685" style="vertical-align: -0.338ex; width:2.77ex; height:2.676ex;"/></span> and a point <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle x\in \mathbb {R} ^{d}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>x</mi>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">R</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle x\in \mathbb {R} ^{d}}</annotation>
</semantics>
</math></span><img alt="x\in {\mathbb  {R}}^{d}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f351538c1465ec3881164b501f612b1f54cbfe7e" style="vertical-align: -0.338ex; width:6.94ex; height:2.676ex;"/></span>, let <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle (X_{(1)},Y_{(1)}),\dots ,(X_{(n)},Y_{(n)})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo stretchy="false">(</mo>
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<mo stretchy="false">(</mo>
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">(</mo>
<mi>n</mi>
<mo stretchy="false">)</mo>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">(</mo>
<mi>n</mi>
<mo stretchy="false">)</mo>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle (X_{(1)},Y_{(1)}),\dots ,(X_{(n)},Y_{(n)})}</annotation>
</semantics>
</math></span><img alt="(X_{{(1)}},Y_{{(1)}}),\dots ,(X_{{(n)}},Y_{{(n)}})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/961b632956473933c75b8cbfc56f8a83ccd41524" style="vertical-align: -1.171ex; width:27.077ex; height:3.176ex;"/></span> be a reordering of the training data such that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \|X_{(1)}-x\|\leq \dots \leq \|X_{(n)}-x\|}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mrow>
</msub>
<mo>−<!-- − --></mo>
<mi>x</mi>
<mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
<mo>≤<!-- ≤ --></mo>
<mo>⋯<!-- ⋯ --></mo>
<mo>≤<!-- ≤ --></mo>
<mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">(</mo>
<mi>n</mi>
<mo stretchy="false">)</mo>
</mrow>
</msub>
<mo>−<!-- − --></mo>
<mi>x</mi>
<mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \|X_{(1)}-x\|\leq \dots \leq \|X_{(n)}-x\|}</annotation>
</semantics>
</math></span><img alt="\|X_{{(1)}}-x\|\leq \dots \leq \|X_{{(n)}}-x\|" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/364eb25128db023e58f0dc8584f7e6de07e94907" style="vertical-align: -1.171ex; width:30.59ex; height:3.176ex;"/></span>
</p>
<h2><span class="mw-headline" id="Algorithm">Algorithm</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=2" title="Edit section: Algorithm">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a class="image" href="/wiki/File:KnnClassification.svg"><img alt="" class="thumbimage" data-file-height="252" data-file-width="279" decoding="async" height="199" src="//upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/220px-KnnClassification.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/330px-KnnClassification.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/440px-KnnClassification.svg.png 2x" width="220"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:KnnClassification.svg" title="Enlarge"></a></div>Example of <i>k</i>-NN classification. The test sample (green dot) should be classified either to blue squares or to red triangles.  If <i>k = 3</i> (solid line circle) it is assigned to the red triangles because there are 2 triangles and only 1 square inside the inner circle.  If <i>k = 5</i> (dashed line circle) it is assigned to the blue squares (3 squares vs. 2 triangles inside the outer circle).</div></div></div>
<p>The training examples are vectors in a multidimensional feature space, each with a class label. The training phase of the algorithm consists only of storing the <a class="mw-redirect" href="/wiki/Feature_vector" title="Feature vector">feature vectors</a> and class labels of the training samples.
</p><p>In the classification phase, <i>k</i> is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the <i>k</i> training samples nearest to that query point.
</p><p>A commonly used distance metric for <a class="mw-redirect" href="/wiki/Continuous_variable" title="Continuous variable">continuous variables</a> is <a href="/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a>. For discrete variables, such as for text classification, another metric can be used, such as the <b>overlap metric</b> (or <a href="/wiki/Hamming_distance" title="Hamming distance">Hamming distance</a>). In the context of gene expression microarray data, for example, <i>k</i>-NN has been employed with correlation coefficients, such as Pearson and Spearman, as a metric.<sup class="reference" id="cite_ref-3"><a href="#cite_note-3">[3]</a></sup> Often, the classification accuracy of <i>k</i>-NN can be improved significantly if the distance metric is learned with specialized algorithms such as <a class="mw-redirect" href="/wiki/Large_Margin_Nearest_Neighbor" title="Large Margin Nearest Neighbor">Large Margin Nearest Neighbor</a> or <a href="/wiki/Neighbourhood_components_analysis" title="Neighbourhood components analysis">Neighbourhood components analysis</a>.
</p><p>A drawback of the basic "majority voting" classification occurs when the class distribution is skewed. That is, examples of a more frequent class tend to dominate the prediction of the new example, because they tend to be common among the <i>k</i> nearest neighbors due to their large number.<sup class="reference" id="cite_ref-Coomans_Massart1982_4-0"><a href="#cite_note-Coomans_Massart1982-4">[4]</a></sup> One way to overcome this problem is to weight the classification, taking into account the distance from the test point to each of its <i>k</i> nearest neighbors. The class (or value, in regression problems) of each of the <i>k</i> nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point. Another way to overcome skew is by abstraction in data representation. For example, in a <a href="/wiki/Self-organizing_map" title="Self-organizing map">self-organizing map</a> (SOM), each node is a representative (a center) of a cluster of similar points, regardless of their density in the original training data. <i>K</i>-NN can then be applied to the SOM.
</p>
<h2><span class="mw-headline" id="Parameter_selection">Parameter selection</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=3" title="Edit section: Parameter selection">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The best choice of <i>k</i> depends upon the data; generally, larger values of <i>k</i> reduces effect of the noise on the classification,<sup class="reference" id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup> but make boundaries between classes less distinct. A good <i>k</i> can be selected by various <a href="/wiki/Heuristic_(computer_science)" title="Heuristic (computer science)">heuristic</a> techniques (see <a href="/wiki/Hyperparameter_optimization" title="Hyperparameter optimization">hyperparameter optimization</a>). The special case where the class is predicted to be the class of the closest training sample (i.e. when <i>k</i> = 1) is called the nearest neighbor algorithm.
</p><p>The accuracy of the <i>k</i>-NN algorithm can be severely degraded by the presence of noisy or irrelevant features, or if the feature scales are not consistent with their importance. Much research effort has been put into <a href="/wiki/Feature_selection" title="Feature selection">selecting</a> or <a href="/wiki/Feature_scaling" title="Feature scaling">scaling</a> features to improve classification. A particularly popular<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (March 2013)">citation needed</span></a></i>]</sup> approach is the use of <a href="/wiki/Evolutionary_algorithm" title="Evolutionary algorithm">evolutionary algorithms</a> to optimize feature scaling.<sup class="reference" id="cite_ref-6"><a href="#cite_note-6">[6]</a></sup> Another popular approach is to scale features by the <a href="/wiki/Mutual_information" title="Mutual information">mutual information</a> of the training data with the training classes.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (December 2008)">citation needed</span></a></i>]</sup>
</p><p>In binary (two class) classification problems, it is helpful to choose <i>k</i> to be an odd number as this avoids tied votes. One popular way of choosing the empirically optimal <i>k</i> in this setting is via bootstrap method.<sup class="reference" id="cite_ref-HPS2008_7-0"><a href="#cite_note-HPS2008-7">[7]</a></sup>
</p>
<h2><span class="mw-headline" id="The_1-nearest_neighbor_classifier">The <span class="texhtml">1</span>-nearest neighbor classifier</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=4" title="Edit section: The 1-nearest neighbor classifier">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The most intuitive nearest neighbour type classifier is the one nearest neighbour classifier that assigns a point <span class="texhtml mvar" style="font-style:italic;">x</span> to the class of its closest neighbour in the feature space, that is <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle C_{n}^{1nn}(x)=Y_{(1)}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msubsup>
<mi>C</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
<mi>n</mi>
<mi>n</mi>
</mrow>
</msubsup>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
<mo>=</mo>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle C_{n}^{1nn}(x)=Y_{(1)}}</annotation>
</semantics>
</math></span><img alt="C_{n}^{{1nn}}(x)=Y_{{(1)}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f815904edbff2ce82502172ec0dce3311d57f2bb" style="vertical-align: -1.171ex; width:14.746ex; height:3.343ex;"/></span>.
</p><p>As the size of training data set approaches infinity, the one nearest neighbour classifier guarantees an error rate of no worse than twice the <a href="/wiki/Bayes_error_rate" title="Bayes error rate">Bayes error rate</a> (the minimum achievable error rate given the distribution of the data).
</p>
<h2><span class="mw-headline" id="The_weighted_nearest_neighbour_classifier">The weighted nearest neighbour classifier</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=5" title="Edit section: The weighted nearest neighbour classifier">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The <span class="texhtml mvar" style="font-style:italic;">k</span>-nearest neighbour classifier can be viewed as assigning the <span class="texhtml mvar" style="font-style:italic;">k</span> nearest neighbours a weight <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle 1/k}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mn>1</mn>
<mrow class="MJX-TeXAtom-ORD">
<mo>/</mo>
</mrow>
<mi>k</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle 1/k}</annotation>
</semantics>
</math></span><img alt="1/k" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a7e9fedad8c70c6331b2640b56c23cef8c884e1f" style="vertical-align: -0.838ex; width:3.536ex; height:2.843ex;"/></span> and all others <span class="texhtml mvar" style="font-style:italic;">0</span> weight. This can be generalised to weighted nearest neighbour classifiers. That is, where the <span class="texhtml mvar" style="font-style:italic;">i</span>th nearest neighbour is assigned a weight <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle w_{ni}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>w</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
<mi>i</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle w_{ni}}</annotation>
</semantics>
</math></span><img alt="w_{{ni}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f6d76326293e410139d081d073068b9eb32a0777" style="vertical-align: -0.671ex; width:3.45ex; height:2.009ex;"/></span>, with <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \sum _{i=1}^{n}w_{ni}=1}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</munderover>
<msub>
<mi>w</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
<mi>i</mi>
</mrow>
</msub>
<mo>=</mo>
<mn>1</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \sum _{i=1}^{n}w_{ni}=1}</annotation>
</semantics>
</math></span><img alt="\sum _{{i=1}}^{n}w_{{ni}}=1" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/167dcdfe9d31faea6f1e6c157cc23fe6e3b39fb7" style="vertical-align: -3.005ex; width:11.453ex; height:6.843ex;"/></span>. An analogous result on the strong consistency of weighted nearest neighbour classifiers also holds.<sup class="reference" id="cite_ref-Stone_8-0"><a href="#cite_note-Stone-8">[8]</a></sup>
</p><p>Let <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle C_{n}^{wnn}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msubsup>
<mi>C</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>w</mi>
<mi>n</mi>
<mi>n</mi>
</mrow>
</msubsup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle C_{n}^{wnn}}</annotation>
</semantics>
</math></span><img alt="C_{n}^{{wnn}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e88b657ee88d912408396f8c9ef6af3483bfdf01" style="vertical-align: -0.671ex; width:5.179ex; height:2.509ex;"/></span> denote the weighted nearest classifier with weights <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \{w_{ni}\}_{i=1}^{n}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo fence="false" stretchy="false">{</mo>
<msub>
<mi>w</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
<mi>i</mi>
</mrow>
</msub>
<msubsup>
<mo fence="false" stretchy="false">}</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msubsup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \{w_{ni}\}_{i=1}^{n}}</annotation>
</semantics>
</math></span><img alt="\{w_{{ni}}\}_{{i=1}}^{n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/efaf258e02ccae2b27c279885d6fb898adaf331d" style="vertical-align: -1.005ex; width:8.675ex; height:3.009ex;"/></span>. Subject to regularity conditions<sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The text near this tag needs further explanation. (January 2019)">further explanation needed</span></a></i>]</sup> on the class distributions the excess risk has the following asymptotic expansion<sup class="reference" id="cite_ref-Samworth12_9-0"><a href="#cite_note-Samworth12-9">[9]</a></sup>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\mathcal {R}}_{\mathcal {R}}(C_{n}^{wnn})-{\mathcal {R}}_{\mathcal {R}}(C^{Bayes})=\left(B_{1}s_{n}^{2}+B_{2}t_{n}^{2}\right)\{1+o(1)\},}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
</mrow>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msubsup>
<mi>C</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>w</mi>
<mi>n</mi>
<mi>n</mi>
</mrow>
</msubsup>
<mo stretchy="false">)</mo>
<mo>−<!-- − --></mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
</mrow>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msup>
<mi>C</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>B</mi>
<mi>a</mi>
<mi>y</mi>
<mi>e</mi>
<mi>s</mi>
</mrow>
</msup>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mrow>
<mo>(</mo>
<mrow>
<msub>
<mi>B</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<msubsup>
<mi>s</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msubsup>
<mo>+</mo>
<msub>
<mi>B</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msub>
<msubsup>
<mi>t</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msubsup>
</mrow>
<mo>)</mo>
</mrow>
<mo fence="false" stretchy="false">{</mo>
<mn>1</mn>
<mo>+</mo>
<mi>o</mi>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
<mo fence="false" stretchy="false">}</mo>
<mo>,</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\mathcal {R}}_{\mathcal {R}}(C_{n}^{wnn})-{\mathcal {R}}_{\mathcal {R}}(C^{Bayes})=\left(B_{1}s_{n}^{2}+B_{2}t_{n}^{2}\right)\{1+o(1)\},}</annotation>
</semantics>
</math></span><img alt="{\mathcal  {R}}_{{\mathcal  {R}}}(C_{{n}}^{{wnn}})-{\mathcal  {R}}_{{{\mathcal  {R}}}}(C^{{Bayes}})=\left(B_{1}s_{n}^{2}+B_{2}t_{n}^{2}\right)\{1+o(1)\}," aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/02f064c830dbca4fcd695427bcc45c7aeb1b1196" style="vertical-align: -1.005ex; width:54.866ex; height:3.343ex;"/></span></dd></dl>
<p>for constants <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle B_{1}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>B</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle B_{1}}</annotation>
</semantics>
</math></span><img alt="B_{1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1fa091eb428443c9c5c5fcf32a69d3665c89e00c" style="vertical-align: -0.671ex; width:2.818ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle B_{2}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>B</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle B_{2}}</annotation>
</semantics>
</math></span><img alt="B_{2}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/199944d59dcc18842dfd1deab6000a1d1dadcbae" style="vertical-align: -0.671ex; width:2.818ex; height:2.509ex;"/></span> where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle s_{n}^{2}=\sum _{i=1}^{n}w_{ni}^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msubsup>
<mi>s</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msubsup>
<mo>=</mo>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</munderover>
<msubsup>
<mi>w</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
<mi>i</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msubsup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle s_{n}^{2}=\sum _{i=1}^{n}w_{ni}^{2}}</annotation>
</semantics>
</math></span><img alt="s_{n}^{2}=\sum _{{i=1}}^{n}w_{{ni}}^{2}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ed6dbd702b3141f1649ce10ccff3bac0acd55299" style="vertical-align: -3.005ex; width:12.599ex; height:6.843ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle t_{n}=n^{-2/d}\sum _{i=1}^{n}w_{ni}\{i^{1+2/d}-(i-1)^{1+2/d}\}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>t</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>=</mo>
<msup>
<mi>n</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>−<!-- − --></mo>
<mn>2</mn>
<mrow class="MJX-TeXAtom-ORD">
<mo>/</mo>
</mrow>
<mi>d</mi>
</mrow>
</msup>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</munderover>
<msub>
<mi>w</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
<mi>i</mi>
</mrow>
</msub>
<mo fence="false" stretchy="false">{</mo>
<msup>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
<mo>+</mo>
<mn>2</mn>
<mrow class="MJX-TeXAtom-ORD">
<mo>/</mo>
</mrow>
<mi>d</mi>
</mrow>
</msup>
<mo>−<!-- − --></mo>
<mo stretchy="false">(</mo>
<mi>i</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
<mo>+</mo>
<mn>2</mn>
<mrow class="MJX-TeXAtom-ORD">
<mo>/</mo>
</mrow>
<mi>d</mi>
</mrow>
</msup>
<mo fence="false" stretchy="false">}</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle t_{n}=n^{-2/d}\sum _{i=1}^{n}w_{ni}\{i^{1+2/d}-(i-1)^{1+2/d}\}}</annotation>
</semantics>
</math></span><img alt="t_{n}=n^{{-2/d}}\sum _{{i=1}}^{n}w_{{ni}}\{i^{{1+2/d}}-(i-1)^{{1+2/d}}\}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cd32f71ab3cd0784e73324108ecb05be734cd7de" style="vertical-align: -3.005ex; width:40.4ex; height:6.843ex;"/></span>.
</p><p>The optimal weighting scheme <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \{w_{ni}^{*}\}_{i=1}^{n}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo fence="false" stretchy="false">{</mo>
<msubsup>
<mi>w</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
<mi>i</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo>∗<!-- ∗ --></mo>
</mrow>
</msubsup>
<msubsup>
<mo fence="false" stretchy="false">}</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msubsup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \{w_{ni}^{*}\}_{i=1}^{n}}</annotation>
</semantics>
</math></span><img alt="\{w_{{ni}}^{*}\}_{{i=1}}^{n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f97b387c9e937fac91f0644ac895c5c95d9a4921" style="vertical-align: -1.005ex; width:8.675ex; height:3.009ex;"/></span>, that balances the two terms in the display above, is given as follows: set <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle k^{*}=\lfloor Bn^{\frac {4}{d+4}}\rfloor }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>∗<!-- ∗ --></mo>
</mrow>
</msup>
<mo>=</mo>
<mo fence="false" stretchy="false">⌊<!-- ⌊ --></mo>
<mi>B</mi>
<msup>
<mi>n</mi>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>4</mn>
<mrow>
<mi>d</mi>
<mo>+</mo>
<mn>4</mn>
</mrow>
</mfrac>
</mrow>
</msup>
<mo fence="false" stretchy="false">⌋<!-- ⌋ --></mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle k^{*}=\lfloor Bn^{\frac {4}{d+4}}\rfloor }</annotation>
</semantics>
</math></span><img alt="k^{*}=\lfloor Bn^{{{\frac  4{d+4}}}}\rfloor " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5cbecc881f1b8637b3d4d4527fd1671f5be252fa" style="vertical-align: -0.838ex; width:14.059ex; height:4.176ex;"/></span>, 
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle w_{ni}^{*}={\frac {1}{k^{*}}}\left[1+{\frac {d}{2}}-{\frac {d}{2{k^{*}}^{2/d}}}\{i^{1+2/d}-(i-1)^{1+2/d}\}\right]}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msubsup>
<mi>w</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
<mi>i</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo>∗<!-- ∗ --></mo>
</mrow>
</msubsup>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<msup>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>∗<!-- ∗ --></mo>
</mrow>
</msup>
</mfrac>
</mrow>
<mrow>
<mo>[</mo>
<mrow>
<mn>1</mn>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mi>d</mi>
<mn>2</mn>
</mfrac>
</mrow>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mi>d</mi>
<mrow>
<mn>2</mn>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>∗<!-- ∗ --></mo>
</mrow>
</msup>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
<mrow class="MJX-TeXAtom-ORD">
<mo>/</mo>
</mrow>
<mi>d</mi>
</mrow>
</msup>
</mrow>
</mfrac>
</mrow>
<mo fence="false" stretchy="false">{</mo>
<msup>
<mi>i</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
<mo>+</mo>
<mn>2</mn>
<mrow class="MJX-TeXAtom-ORD">
<mo>/</mo>
</mrow>
<mi>d</mi>
</mrow>
</msup>
<mo>−<!-- − --></mo>
<mo stretchy="false">(</mo>
<mi>i</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
<mo>+</mo>
<mn>2</mn>
<mrow class="MJX-TeXAtom-ORD">
<mo>/</mo>
</mrow>
<mi>d</mi>
</mrow>
</msup>
<mo fence="false" stretchy="false">}</mo>
</mrow>
<mo>]</mo>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle w_{ni}^{*}={\frac {1}{k^{*}}}\left[1+{\frac {d}{2}}-{\frac {d}{2{k^{*}}^{2/d}}}\{i^{1+2/d}-(i-1)^{1+2/d}\}\right]}</annotation>
</semantics>
</math></span><img alt="w_{{ni}}^{*}={\frac  1{k^{*}}}\left[1+{\frac  d2}-{\frac  d{2{k^{*}}^{{2/d}}}}\{i^{{1+2/d}}-(i-1)^{{1+2/d}}\}\right]" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fbfa4058134234385c31544db3e657c4b242ab34" style="vertical-align: -2.505ex; width:50.643ex; height:6.176ex;"/></span> for <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i=1,2,\dots ,k^{*}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
<mo>,</mo>
<mn>2</mn>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msup>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>∗<!-- ∗ --></mo>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle i=1,2,\dots ,k^{*}}</annotation>
</semantics>
</math></span><img alt="i=1,2,\dots ,k^{*}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4ea7e974f7466c9dedfe409ea013bc719264872b" style="vertical-align: -0.671ex; width:14.703ex; height:2.676ex;"/></span> and</dd>
<dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle w_{ni}^{*}=0}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msubsup>
<mi>w</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
<mi>i</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo>∗<!-- ∗ --></mo>
</mrow>
</msubsup>
<mo>=</mo>
<mn>0</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle w_{ni}^{*}=0}</annotation>
</semantics>
</math></span><img alt="w_{{ni}}^{*}=0" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6bd3d5b77d7fef0dabd4326ee85b04fa244fa988" style="vertical-align: -1.005ex; width:7.711ex; height:2.843ex;"/></span> for <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i=k^{*}+1,\dots ,n}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>i</mi>
<mo>=</mo>
<msup>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>∗<!-- ∗ --></mo>
</mrow>
</msup>
<mo>+</mo>
<mn>1</mn>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<mi>n</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle i=k^{*}+1,\dots ,n}</annotation>
</semantics>
</math></span><img alt="i=k^{*}+1,\dots ,n" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b7c2f2e5c12f3febcefe0aae189d44031daf8e79" style="vertical-align: -0.671ex; width:16.742ex; height:2.676ex;"/></span>.</dd></dl>
<p>With optimal weights the dominant term in the asymptotic expansion of the excess risk is <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\mathcal {O}}(n^{-{\frac {4}{d+4}}})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<msup>
<mi>n</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>4</mn>
<mrow>
<mi>d</mi>
<mo>+</mo>
<mn>4</mn>
</mrow>
</mfrac>
</mrow>
</mrow>
</msup>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\mathcal {O}}(n^{-{\frac {4}{d+4}}})}</annotation>
</semantics>
</math></span><img alt="{\mathcal  {O}}(n^{{-{\frac  4{d+4}}}})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/07280735a8852d609ffd3942647d7e3255697f05" style="vertical-align: -0.838ex; width:9.804ex; height:4.176ex;"/></span>. Similar results are true when using a <a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">bagged nearest neighbour classifier</a>.
</p>
<h2><span class="mw-headline" id="Properties">Properties</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=6" title="Edit section: Properties">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><i>k</i>-NN is a special case of a <a href="/wiki/Variable_kernel_density_estimation" title="Variable kernel density estimation">variable-bandwidth, kernel density "balloon" estimator</a> with a uniform <a href="/wiki/Kernel_(statistics)" title="Kernel (statistics)">kernel</a>.<sup class="reference" id="cite_ref-Terrell_Scott1992_10-0"><a href="#cite_note-Terrell_Scott1992-10">[10]</a></sup>
<sup class="reference" id="cite_ref-Mills2010_11-0"><a href="#cite_note-Mills2010-11">[11]</a></sup>
</p><p>The naive version of the algorithm is easy to implement by computing the distances from the test example to all stored examples, but it is computationally intensive for large training sets. Using an approximate <a href="/wiki/Nearest_neighbor_search" title="Nearest neighbor search">nearest neighbor search</a> algorithm makes <i>k-</i>NN computationally tractable even for large data sets. Many nearest neighbor search algorithms have been proposed over the years; these generally seek to reduce the number of distance evaluations actually performed.
</p><p><i>k-</i>NN has some strong <a href="/wiki/Consistency_(statistics)" title="Consistency (statistics)">consistency</a> results. As the amount of data approaches infinity, the two-class <i>k-</i>NN algorithm is guaranteed to yield an error rate no worse than twice the <a href="/wiki/Bayes_error_rate" title="Bayes error rate">Bayes error rate</a> (the minimum achievable error rate given the distribution of the data).<sup class="reference" id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup> Various improvements to the <i>k</i>-NN speed are possible by using proximity graphs.<sup class="reference" id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup>
</p><p>For multi-class <i>k-</i>NN classification, <a href="/wiki/Thomas_M._Cover" title="Thomas M. Cover">Cover</a> and <a href="/wiki/Peter_E._Hart" title="Peter E. Hart">Hart</a> (1967) prove an upper bound error rate of
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle R^{*}\ \leq \ R_{k\mathrm {NN} }\ \leq \ R^{*}\left(2-{\frac {MR^{*}}{M-1}}\right)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>R</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>∗<!-- ∗ --></mo>
</mrow>
</msup>
<mtext> </mtext>
<mo>≤<!-- ≤ --></mo>
<mtext> </mtext>
<msub>
<mi>R</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="normal">N</mi>
<mi mathvariant="normal">N</mi>
</mrow>
</mrow>
</msub>
<mtext> </mtext>
<mo>≤<!-- ≤ --></mo>
<mtext> </mtext>
<msup>
<mi>R</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>∗<!-- ∗ --></mo>
</mrow>
</msup>
<mrow>
<mo>(</mo>
<mrow>
<mn>2</mn>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mrow>
<mi>M</mi>
<msup>
<mi>R</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>∗<!-- ∗ --></mo>
</mrow>
</msup>
</mrow>
<mrow>
<mi>M</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</mfrac>
</mrow>
</mrow>
<mo>)</mo>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle R^{*}\ \leq \ R_{k\mathrm {NN} }\ \leq \ R^{*}\left(2-{\frac {MR^{*}}{M-1}}\right)}</annotation>
</semantics>
</math></span><img alt="{\displaystyle R^{*}\ \leq \ R_{k\mathrm {NN} }\ \leq \ R^{*}\left(2-{\frac {MR^{*}}{M-1}}\right)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3a20a21397df93ca13f8098a25731888f763efe7" style="vertical-align: -2.505ex; width:34.566ex; height:6.176ex;"/></span></dd></dl>
<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle R^{*}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>R</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>∗<!-- ∗ --></mo>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle R^{*}}</annotation>
</semantics>
</math></span><img alt="R^{*}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7a74b1bc0fa98794b6460254044f8e7b75e6d84f" style="vertical-align: -0.338ex; width:2.818ex; height:2.343ex;"/></span>is the Bayes error rate (which is the minimal error rate possible),  <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle R_{kNN}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>R</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>k</mi>
<mi>N</mi>
<mi>N</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle R_{kNN}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle R_{kNN}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1f3666637c81f02c1bfeb4e1969828745062aeba" style="vertical-align: -0.671ex; width:5.771ex; height:2.509ex;"/></span> is the <i>k-</i>NN error rate, and <span class="texhtml mvar" style="font-style:italic;">M</span> is the number of classes in the problem.  For <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle M=2}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>M</mi>
<mo>=</mo>
<mn>2</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle M=2}</annotation>
</semantics>
</math></span><img alt="M=2" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2801ac77396e68de0b640087e1531a2329067e9f" style="vertical-align: -0.338ex; width:6.703ex; height:2.176ex;"/></span> and as the Bayesian error rate <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle R^{*}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>R</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>∗<!-- ∗ --></mo>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle R^{*}}</annotation>
</semantics>
</math></span><img alt="R^{*}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7a74b1bc0fa98794b6460254044f8e7b75e6d84f" style="vertical-align: -0.338ex; width:2.818ex; height:2.343ex;"/></span> approaches zero, this limit reduces to "not more than twice the Bayesian error rate".
</p>
<h2><span class="mw-headline" id="Error_rates">Error rates</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=7" title="Edit section: Error rates">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>There are many results on the error rate of the <span class="texhtml mvar" style="font-style:italic;">k</span> nearest neighbour classifiers.<sup class="reference" id="cite_ref-PTPR_14-0"><a href="#cite_note-PTPR-14">[14]</a></sup>  The <span class="texhtml mvar" style="font-style:italic;">k</span>-nearest neighbour classifier is strongly (that is for any joint distribution on <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle (X,Y)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo stretchy="false">(</mo>
<mi>X</mi>
<mo>,</mo>
<mi>Y</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle (X,Y)}</annotation>
</semantics>
</math></span><img alt="(X,Y)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/41f29b9537685f499713112d6802e811cbf51bba" style="vertical-align: -0.838ex; width:6.597ex; height:2.843ex;"/></span>) <a href="/wiki/Bayes_classifier" title="Bayes classifier">consistent</a> provided <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle k:=k_{n}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>k</mi>
<mo>:=</mo>
<msub>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle k:=k_{n}}</annotation>
</semantics>
</math></span><img alt="k:=k_{n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0fccbcda1dd871e437cca392a4294e6affdb5692" style="vertical-align: -0.671ex; width:7.386ex; height:2.509ex;"/></span> diverges and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle k_{n}/n}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mrow class="MJX-TeXAtom-ORD">
<mo>/</mo>
</mrow>
<mi>n</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle k_{n}/n}</annotation>
</semantics>
</math></span><img alt="k_{n}/n" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4e9a4dfc54979e5190d6baa3cc552824357edca1" style="vertical-align: -0.838ex; width:4.987ex; height:2.843ex;"/></span> converges to zero as <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle n\to \infty }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>n</mi>
<mo stretchy="false">→<!-- → --></mo>
<mi mathvariant="normal">∞<!-- ∞ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle n\to \infty }</annotation>
</semantics>
</math></span><img alt="n\to \infty " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a0d55d9b32f6fa8fab6a84ea444a6b5a24bb45e1" style="vertical-align: -0.338ex; width:7.333ex; height:1.843ex;"/></span>.
</p><p>Let <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle C_{n}^{knn}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msubsup>
<mi>C</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>k</mi>
<mi>n</mi>
<mi>n</mi>
</mrow>
</msubsup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle C_{n}^{knn}}</annotation>
</semantics>
</math></span><img alt="C_{n}^{{knn}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3f2588b76da9c7429b56cec54535a18a8cae6386" style="vertical-align: -0.671ex; width:4.859ex; height:2.843ex;"/></span> denote the <span class="texhtml mvar" style="font-style:italic;">k</span> nearest neighbour classifier based on a training set of size <span class="texhtml mvar" style="font-style:italic;">n</span>. Under certain regularity conditions, the <a href="/wiki/Bayes_classifier" title="Bayes classifier">excess risk</a> yields the following asymptotic expansion<sup class="reference" id="cite_ref-Samworth12_9-1"><a href="#cite_note-Samworth12-9">[9]</a></sup>
</p>
<dl><dd><dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\mathcal {R}}_{\mathcal {R}}(C_{n}^{knn})-{\mathcal {R}}_{\mathcal {R}}(C^{Bayes})=\left\{B_{1}{\frac {1}{k}}+B_{2}\left({\frac {k}{n}}\right)^{4/d}\right\}\{1+o(1)\},}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
</mrow>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msubsup>
<mi>C</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>k</mi>
<mi>n</mi>
<mi>n</mi>
</mrow>
</msubsup>
<mo stretchy="false">)</mo>
<mo>−<!-- − --></mo>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
</mrow>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<msup>
<mi>C</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>B</mi>
<mi>a</mi>
<mi>y</mi>
<mi>e</mi>
<mi>s</mi>
</mrow>
</msup>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mrow>
<mo>{</mo>
<mrow>
<msub>
<mi>B</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>k</mi>
</mfrac>
</mrow>
<mo>+</mo>
<msub>
<mi>B</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msub>
<msup>
<mrow>
<mo>(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mi>k</mi>
<mi>n</mi>
</mfrac>
</mrow>
<mo>)</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>4</mn>
<mrow class="MJX-TeXAtom-ORD">
<mo>/</mo>
</mrow>
<mi>d</mi>
</mrow>
</msup>
</mrow>
<mo>}</mo>
</mrow>
<mo fence="false" stretchy="false">{</mo>
<mn>1</mn>
<mo>+</mo>
<mi>o</mi>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
<mo fence="false" stretchy="false">}</mo>
<mo>,</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\mathcal {R}}_{\mathcal {R}}(C_{n}^{knn})-{\mathcal {R}}_{\mathcal {R}}(C^{Bayes})=\left\{B_{1}{\frac {1}{k}}+B_{2}\left({\frac {k}{n}}\right)^{4/d}\right\}\{1+o(1)\},}</annotation>
</semantics>
</math></span><img alt="{\mathcal  {R}}_{{\mathcal  {R}}}(C_{{n}}^{{knn}})-{\mathcal  {R}}_{{{\mathcal  {R}}}}(C^{{Bayes}})=\left\{B_{1}{\frac  1k}+B_{2}\left({\frac  kn}\right)^{{4/d}}\right\}\{1+o(1)\}," aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0d37bdd6d06251f082b0fe52c3f7da579b11cc85" style="vertical-align: -3.171ex; width:62.23ex; height:7.509ex;"/></span></dd></dl></dd></dl>
<p>for some constants <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle B_{1}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>B</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle B_{1}}</annotation>
</semantics>
</math></span><img alt="B_{1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1fa091eb428443c9c5c5fcf32a69d3665c89e00c" style="vertical-align: -0.671ex; width:2.818ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle B_{2}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>B</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle B_{2}}</annotation>
</semantics>
</math></span><img alt="B_{2}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/199944d59dcc18842dfd1deab6000a1d1dadcbae" style="vertical-align: -0.671ex; width:2.818ex; height:2.509ex;"/></span>.
</p><p>The choice <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle k^{*}=\lfloor Bn^{\frac {4}{d+4}}\rfloor }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>∗<!-- ∗ --></mo>
</mrow>
</msup>
<mo>=</mo>
<mo fence="false" stretchy="false">⌊<!-- ⌊ --></mo>
<mi>B</mi>
<msup>
<mi>n</mi>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>4</mn>
<mrow>
<mi>d</mi>
<mo>+</mo>
<mn>4</mn>
</mrow>
</mfrac>
</mrow>
</msup>
<mo fence="false" stretchy="false">⌋<!-- ⌋ --></mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle k^{*}=\lfloor Bn^{\frac {4}{d+4}}\rfloor }</annotation>
</semantics>
</math></span><img alt="k^{*}=\lfloor Bn^{{{\frac  4{d+4}}}}\rfloor " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5cbecc881f1b8637b3d4d4527fd1671f5be252fa" style="vertical-align: -0.838ex; width:14.059ex; height:4.176ex;"/></span> offers a trade off between the two terms in the above display, for which the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle k^{*}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>k</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>∗<!-- ∗ --></mo>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle k^{*}}</annotation>
</semantics>
</math></span><img alt="k^{*}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cc02328d1105a031ca024abcb86629ea4edb3cc8" style="vertical-align: -0.338ex; width:2.265ex; height:2.343ex;"/></span>-nearest neighbour error converges to the Bayes error at the optimal (<a href="/wiki/Minimax" title="Minimax">minimax</a>) rate <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\mathcal {O}}(n^{-{\frac {4}{d+4}}})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<msup>
<mi>n</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>4</mn>
<mrow>
<mi>d</mi>
<mo>+</mo>
<mn>4</mn>
</mrow>
</mfrac>
</mrow>
</mrow>
</msup>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\mathcal {O}}(n^{-{\frac {4}{d+4}}})}</annotation>
</semantics>
</math></span><img alt="{\mathcal  {O}}(n^{{-{\frac  4{d+4}}}})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/07280735a8852d609ffd3942647d7e3255697f05" style="vertical-align: -0.838ex; width:9.804ex; height:4.176ex;"/></span>.
</p>
<h2><span class="mw-headline" id="Metric_learning">Metric learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=8" title="Edit section: Metric learning">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The K-nearest neighbor classification performance can often be significantly improved through (supervised) metric learning. Popular algorithms are <a href="/wiki/Neighbourhood_components_analysis" title="Neighbourhood components analysis">neighbourhood components analysis</a> and <a href="/wiki/Large_margin_nearest_neighbor" title="Large margin nearest neighbor">large margin nearest neighbor</a>. Supervised metric learning algorithms use the label information to learn a new <a href="/wiki/Metric_(mathematics)" title="Metric (mathematics)">metric</a> or <a href="/wiki/Pseudometric_space" title="Pseudometric space">pseudo-metric</a>.
</p>
<h2><span class="mw-headline" id="Feature_extraction">Feature extraction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=9" title="Edit section: Feature extraction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters) then the input data will be transformed into a reduced representation set of features (also named features vector). Transforming the input data into the set of features is called <a href="/wiki/Feature_extraction" title="Feature extraction">feature extraction</a>. If the features extracted are carefully chosen it is expected that the features set will extract the relevant information from the input data in order to perform the desired task using this reduced representation instead of the full size input. Feature extraction is performed on raw data prior to applying <i>k</i>-NN algorithm on the transformed data in <a class="mw-redirect" href="/wiki/Feature_space" title="Feature space">feature space</a>.
</p><p>An example of a typical <a href="/wiki/Computer_vision" title="Computer vision">computer vision</a> computation pipeline for <a href="/wiki/Facial_recognition_system" title="Facial recognition system">face recognition</a> using <i>k</i>-NN including feature extraction and dimension reduction pre-processing steps (usually implemented with <a href="/wiki/OpenCV" title="OpenCV">OpenCV</a>):
</p>
<ol><li><a href="/wiki/Haar_wavelet" title="Haar wavelet">Haar</a> face detection</li>
<li><a class="mw-redirect" href="/wiki/Mean-shift" title="Mean-shift">Mean-shift</a> tracking analysis</li>
<li><a class="mw-redirect" href="/wiki/Principal_Component_Analysis" title="Principal Component Analysis">PCA</a> or <a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">Fisher LDA</a> projection into feature space, followed by <i>k</i>-NN classification</li></ol>
<h2><span class="mw-headline" id="Dimension_reduction">Dimension reduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=10" title="Edit section: Dimension reduction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>For high-dimensional data (e.g., with number of dimensions more than 10) <a class="mw-redirect" href="/wiki/Dimension_reduction" title="Dimension reduction">dimension reduction</a> is usually performed prior to applying the <i>k</i>-NN algorithm in order to avoid the effects of the <a class="mw-redirect" href="/wiki/Curse_of_Dimensionality" title="Curse of Dimensionality">curse of dimensionality</a>.
<sup class="reference" id="cite_ref-15"><a href="#cite_note-15">[15]</a></sup>
</p><p>The curse of dimensionality in the <i>k</i>-NN context basically means that <a href="/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a> is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector (imagine multiple points lying more or less on a circle with the query point at the center; the distance from the query to all data points in the search space is almost the same).
</p><p><a href="/wiki/Feature_extraction" title="Feature extraction">Feature extraction</a> and dimension reduction can be combined in one step using <a class="mw-redirect" href="/wiki/Principal_Component_Analysis" title="Principal Component Analysis">principal component analysis</a> (PCA),  <a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">linear discriminant analysis</a> (LDA), or <a href="/wiki/Canonical_correlation" title="Canonical correlation">canonical correlation analysis</a> (CCA) techniques as a pre-processing step, followed by clustering by <i>k</i>-NN on <a href="/wiki/Feature_(machine_learning)" title="Feature (machine learning)">feature vectors</a> in reduced-dimension space. In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> this process is also called low-dimensional <a href="/wiki/Embedding" title="Embedding">embedding</a>.<sup class="reference" id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup>
</p><p>For very-high-dimensional datasets (e.g. when performing a similarity search on live video streams, DNA data or high-dimensional <a href="/wiki/Time_series" title="Time series">time series</a>) running a fast <b>approximate</b> <i>k</i>-NN search using <a class="mw-redirect" href="/wiki/Locality_Sensitive_Hashing" title="Locality Sensitive Hashing">locality sensitive hashing</a>, "random projections",<sup class="reference" id="cite_ref-17"><a href="#cite_note-17">[17]</a></sup> "sketches" <sup class="reference" id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup> or other high-dimensional similarity search techniques from the <a class="mw-redirect" href="/wiki/VLDB_conference" title="VLDB conference">VLDB</a> toolbox might be the only feasible option.
</p>
<h2><span class="mw-headline" id="Decision_boundary">Decision boundary</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=11" title="Edit section: Decision boundary">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Nearest neighbor rules in effect implicitly compute the <a href="/wiki/Decision_boundary" title="Decision boundary">decision boundary</a>. It is also possible to compute the decision boundary explicitly, and to do so efficiently, so that the computational complexity is a function of the boundary complexity.<sup class="reference" id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup>
</p>
<h2><span class="mw-headline" id="Data_reduction">Data reduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=12" title="Edit section: Data reduction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Data reduction is one of the most important problems for work with huge data sets. Usually, only some of the data points are needed for accurate classification. Those data are called the <i>prototypes</i> and can be found as follows:
</p>
<ol><li>Select the <i>class-outliers</i>, that is, training data that are classified incorrectly by <i>k</i>-NN (for a given <i>k</i>)</li>
<li>Separate the rest of the data into two sets: (i) the prototypes that are used for the classification decisions and (ii) the <i>absorbed points</i> that can be correctly classified by <i>k</i>-NN using prototypes. The absorbed points can then be removed from the training set.</li></ol>
<h3><span class="mw-headline" id="Selection_of_class-outliers">Selection of class-outliers</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=13" title="Edit section: Selection of class-outliers">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>A training example surrounded by examples of other classes is called a class outlier. Causes of class outliers include:
</p>
<ul><li>random error</li>
<li>insufficient training examples of this class (an isolated example appears instead of a cluster)</li>
<li>missing important features (the classes are separated in other dimensions which we do not know)</li>
<li>too many training examples of other classes (unbalanced classes) that create a "hostile" background for the given small class</li></ul>
<p>Class outliers with <i>k</i>-NN produce noise. They can be detected and separated for future analysis. Given two natural numbers, <i>k&gt;r&gt;0</i>, a training example is called a <i>(k,r)</i>NN class-outlier if its <i>k</i> nearest neighbors include more than <i>r</i> examples of other classes.
</p>
<h3><span class="mw-headline" id="CNN_for_data_reduction">CNN for data reduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=14" title="Edit section: CNN for data reduction">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Condensed nearest neighbor (CNN, the <i><a href="/wiki/Peter_E._Hart" title="Peter E. Hart">Hart</a> algorithm</i>) is an algorithm designed to reduce the data set for <i>k</i>-NN classification.<sup class="reference" id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup> It selects the set of prototypes <i>U</i> from the training data, such that 1NN with <i>U</i> can classify the examples almost as accurately as 1NN does with the whole data set.
</p>
<div class="thumb tright"><div class="thumbinner" style="width:132px;"><a class="image" href="/wiki/File:BorderRAtio.PNG"><img alt="" class="thumbimage" data-file-height="127" data-file-width="159" decoding="async" height="104" src="//upload.wikimedia.org/wikipedia/commons/thumb/e/e6/BorderRAtio.PNG/130px-BorderRAtio.PNG" srcset="//upload.wikimedia.org/wikipedia/commons/e/e6/BorderRAtio.PNG 1.5x" width="130"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:BorderRAtio.PNG" title="Enlarge"></a></div>Calculation of the border ratio.</div></div></div>
<div class="thumb tright"><div class="thumbinner" style="width:132px;"><a class="image" href="/wiki/File:PointsTypes.png"><img alt="" class="thumbimage" data-file-height="59" data-file-width="130" decoding="async" height="59" src="//upload.wikimedia.org/wikipedia/commons/e/e7/PointsTypes.png" width="130"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:PointsTypes.png" title="Enlarge"></a></div>Three types of points: prototypes, class-outliers, and absorbed points.</div></div></div>
<p>Given a training set <i>X</i>, CNN works iteratively:
</p>
<ol><li>Scan all elements of <i>X</i>, looking for an element <i>x</i> whose nearest prototype from <i>U</i> has a different label than <i>x</i>.</li>
<li>Remove <i>x</i> from <i>X</i> and add it to <i>U</i></li>
<li>Repeat the scan until no more prototypes are added to <i>U</i>.</li></ol>
<p>Use <i>U</i> instead of <i>X</i> for classification. The examples that are not prototypes are called "absorbed" points.
</p><p>It is efficient to scan the training examples in order of decreasing border ratio.<sup class="reference" id="cite_ref-MirkesKnn_21-0"><a href="#cite_note-MirkesKnn-21">[21]</a></sup> The border ratio of a training example <i>x</i> is defined as 
</p>
<dl><dd><span class="texhtml"><i>a</i>(<i>x</i>) = <span class="sfrac nowrap tion" role="math" style="display:inline-block; vertical-align:-0.5em; font-size:85%; text-align:center;"><span class="num" style="display:block; line-height:1em; margin:0 0.1em;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x'-y</i></span>|</span>|</span><span class="slash visualhide">/</span><span class="den" style="display:block; line-height:1em; margin:0 0.1em; border-top:1px solid;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x-y</i></span>|</span>|</span></span></span></dd></dl>
<p>where <span class="texhtml">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x-y</i></span>|</span>|</span> is the distance to the closest example <i>y</i> having a different color than <i>x</i>, and <span class="texhtml">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x'-y</i></span>|</span>|</span> is the distance from <i>y</i> to its closest example <i>x' </i> with the same label as <i>x</i>.
</p><p>The border ratio is in the interval [0,1] because <span class="texhtml">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x'-y</i></span>|</span>|</span>never exceeds <span class="texhtml">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x-y</i></span>|</span>|</span>. This ordering gives preference to the borders of the classes for inclusion in the set of prototypes <i>U</i>. A point of a different label than <i>x</i> is called external to <i>x</i>. The calculation of the border ratio is illustrated by the figure on the right. The data points are labeled by colors: the initial point is <i>x</i> and its label is red. External points are blue and green. The closest to <i>x</i> external point is <i>y</i>. The closest to <i>y</i> red point is <i>x' </i>. The border ratio <span class="texhtml"><i>a</i>(<i>x</i>) = |<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x'-y</i></span>|</span>| / |<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>x-y</i></span>|</span>|</span>is the attribute of the initial point <i>x</i>.
</p><p>Below is an illustration of CNN in a series of figures. There are three classes (red, green and blue). Fig. 1: initially there are 60 points in each class. Fig. 2 shows the 1NN classification map: each pixel is classified by 1NN using all the data. Fig. 3 shows the 5NN classification map. White areas correspond to the unclassified regions, where 5NN voting is tied (for example, if there are two green, two red and one blue points among 5 nearest neighbors). Fig. 4 shows the reduced data set. The crosses are the class-outliers selected by the (3,2)NN rule (all the three nearest neighbors of these instances belong to other classes); the squares are the prototypes, and the empty circles are the absorbed points. The left bottom corner shows the numbers of the class-outliers, prototypes and absorbed points for all three classes. The number of prototypes varies from 15% to 20% for different classes in this example. Fig. 5 shows that the 1NN classification map with the prototypes is very similar to that with the initial data set. The figures were produced using the Mirkes applet.<sup class="reference" id="cite_ref-MirkesKnn_21-1"><a href="#cite_note-MirkesKnn-21">[21]</a></sup>
</p>
<ul class="gallery mw-gallery-traditional">
<li class="gallerycaption">CNN model reduction for k-NN classifiers</li>
<li class="gallerybox" style="width: 235px"><div style="width: 235px">
<div class="thumb" style="width: 230px;"><div style="margin:15px auto;"><a class="image" href="/wiki/File:Data3classes.png"><img alt="" data-file-height="397" data-file-width="602" decoding="async" height="120" src="//upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Data3classes.png/182px-Data3classes.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Data3classes.png/273px-Data3classes.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Data3classes.png/364px-Data3classes.png 2x" width="182"/></a></div></div>
<div class="gallerytext">
<p>Fig. 1. The dataset.
</p>
</div>
</div></li>
<li class="gallerybox" style="width: 235px"><div style="width: 235px">
<div class="thumb" style="width: 230px;"><div style="margin:15px auto;"><a class="image" href="/wiki/File:Map1NN.png"><img alt="" data-file-height="397" data-file-width="603" decoding="async" height="120" src="//upload.wikimedia.org/wikipedia/commons/thumb/5/52/Map1NN.png/183px-Map1NN.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/5/52/Map1NN.png/274px-Map1NN.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/5/52/Map1NN.png/365px-Map1NN.png 2x" width="183"/></a></div></div>
<div class="gallerytext">
<p>Fig. 2. The 1NN classification map.
</p>
</div>
</div></li>
<li class="gallerybox" style="width: 235px"><div style="width: 235px">
<div class="thumb" style="width: 230px;"><div style="margin:15px auto;"><a class="image" href="/wiki/File:Map5NN.png"><img alt="" data-file-height="396" data-file-width="602" decoding="async" height="120" src="//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Map5NN.png/183px-Map5NN.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Map5NN.png/274px-Map5NN.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Map5NN.png/365px-Map5NN.png 2x" width="183"/></a></div></div>
<div class="gallerytext">
<p>Fig. 3. The 5NN classification map.
</p>
</div>
</div></li>
<li class="gallerybox" style="width: 235px"><div style="width: 235px">
<div class="thumb" style="width: 230px;"><div style="margin:15px auto;"><a class="image" href="/wiki/File:ReducedDataSet.png"><img alt="" data-file-height="402" data-file-width="608" decoding="async" height="120" src="//upload.wikimedia.org/wikipedia/commons/thumb/b/b3/ReducedDataSet.png/182px-ReducedDataSet.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/b/b3/ReducedDataSet.png/272px-ReducedDataSet.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/b3/ReducedDataSet.png/363px-ReducedDataSet.png 2x" width="182"/></a></div></div>
<div class="gallerytext">
<p>Fig. 4. The CNN reduced dataset.
</p>
</div>
</div></li>
<li class="gallerybox" style="width: 235px"><div style="width: 235px">
<div class="thumb" style="width: 230px;"><div style="margin:15px auto;"><a class="image" href="/wiki/File:Map1NNReducedDataSet.png"><img alt="" data-file-height="401" data-file-width="611" decoding="async" height="120" src="//upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Map1NNReducedDataSet.png/183px-Map1NNReducedDataSet.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Map1NNReducedDataSet.png/275px-Map1NNReducedDataSet.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Map1NNReducedDataSet.png/366px-Map1NNReducedDataSet.png 2x" width="183"/></a></div></div>
<div class="gallerytext">
<p>Fig. 5. The 1NN classification map based on the CNN extracted prototypes.
</p>
</div>
</div></li>
</ul>
<h2><span class="mw-headline" id="k-NN_regression"><i>k</i>-NN regression</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=15" title="Edit section: k-NN regression">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>In <i>k</i>-NN regression, the <i>k</i>-NN algorithm<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (September 2019)">citation needed</span></a></i>]</sup> is used for estimating continuous variables. One such algorithm uses a weighted average of the <i>k</i> nearest neighbors, weighted by the inverse of their distance. This algorithm works as follows:
</p>
<ol><li>Compute the Euclidean or <a href="/wiki/Mahalanobis_distance" title="Mahalanobis distance">Mahalanobis distance</a> from the query example to the labeled examples.</li>
<li>Order the labeled examples by increasing distance.</li>
<li>Find a heuristically optimal number <i>k</i> of nearest neighbors, based on <a class="mw-redirect" href="/wiki/RMSE" title="RMSE">RMSE</a>. This is done using cross validation.</li>
<li>Calculate an inverse distance weighted average with the <i>k</i>-nearest multivariate neighbors.</li></ol>
<h2><span class="mw-headline" id="k-NN_outlier"><i>k</i>-NN outlier</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=16" title="Edit section: k-NN outlier">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The distance to the <i>k</i>th nearest neighbor can also be seen as a local density estimate and thus is also a popular outlier score in <a href="/wiki/Anomaly_detection" title="Anomaly detection">anomaly detection</a>. The larger the distance to the <i>k</i>-NN, the lower the local density, the more likely the query point is an outlier.<sup class="reference" id="cite_ref-22"><a href="#cite_note-22">[22]</a></sup> Although quite simple, this outlier model, along with another classic data mining method, <a href="/wiki/Local_outlier_factor" title="Local outlier factor">local outlier factor</a>, works quite well also in comparison to more recent and more complex approaches, according to a large scale experimental analysis.<sup class="reference" id="cite_ref-CamposZimek2016_23-0"><a href="#cite_note-CamposZimek2016-23">[23]</a></sup>
</p>
<h2><span class="mw-headline" id="Validation_of_results">Validation of results</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=17" title="Edit section: Validation of results">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A <a href="/wiki/Confusion_matrix" title="Confusion matrix">confusion matrix</a> or "matching matrix" is often used as a tool to validate the accuracy of <i>k</i>-NN classification. More robust statistical methods such as <a href="/wiki/Likelihood-ratio_test" title="Likelihood-ratio test">likelihood-ratio test</a> can also be applied.
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=18" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<style data-mw-deduplicate="TemplateStyles:r936637989">.mw-parser-output .portal{border:solid #aaa 1px;padding:0}.mw-parser-output .portal.tleft{margin:0.5em 1em 0.5em 0}.mw-parser-output .portal.tright{margin:0.5em 0 0.5em 1em}.mw-parser-output .portal>ul{display:table;box-sizing:border-box;padding:0.1em;max-width:175px;background:#f9f9f9;font-size:85%;line-height:110%;font-style:italic;font-weight:bold}.mw-parser-output .portal>ul>li{display:table-row}.mw-parser-output .portal>ul>li>span:first-child{display:table-cell;padding:0.2em;vertical-align:middle;text-align:center}.mw-parser-output .portal>ul>li>span:last-child{display:table-cell;padding:0.2em 0.2em 0.2em 0.3em;vertical-align:middle}</style><div aria-label="Portals" class="noprint portal plainlist tright" role="navigation">
<ul>
<li><span><a class="image" href="/wiki/File:Nuvola_apps_edu_mathematics_blue-p.svg"><img alt="icon" class="noviewer" data-file-height="128" data-file-width="128" decoding="async" height="28" src="//upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Nuvola_apps_edu_mathematics_blue-p.svg/28px-Nuvola_apps_edu_mathematics_blue-p.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Nuvola_apps_edu_mathematics_blue-p.svg/42px-Nuvola_apps_edu_mathematics_blue-p.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Nuvola_apps_edu_mathematics_blue-p.svg/56px-Nuvola_apps_edu_mathematics_blue-p.svg.png 2x" width="28"/></a></span><span><a href="/wiki/Portal:Mathematics" title="Portal:Mathematics">Mathematics portal</a></span></li></ul></div>
<ul><li><a href="/wiki/Nearest_centroid_classifier" title="Nearest centroid classifier">Nearest centroid classifier</a></li>
<li><a href="/wiki/Closest_pair_of_points_problem" title="Closest pair of points problem">Closest pair of points problem</a></li></ul>
<div style="clear:right;"></div>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=19" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="/wiki/Naomi_Altman" title="Naomi Altman">Altman, Naomi S.</a> (1992). <a class="external text" href="https://ecommons.cornell.edu/bitstream/1813/31637/1/BU-1065-MA.pdf" rel="nofollow">"An introduction to kernel and nearest-neighbor nonparametric regression"</a> <span class="cs1-format">(PDF)</span>. <i>The American Statistician</i>. <b>46</b> (3): 175–185. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1080%2F00031305.1992.10475879" rel="nofollow">10.1080/00031305.1992.10475879</a>. <a href="/wiki/Handle_System" title="Handle System">hdl</a>:<a class="external text" href="//hdl.handle.net/1813%2F31637" rel="nofollow">1813/31637</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+American+Statistician&amp;rft.atitle=An+introduction+to+kernel+and+nearest-neighbor+nonparametric+regression&amp;rft.volume=46&amp;rft.issue=3&amp;rft.pages=175-185&amp;rft.date=1992&amp;rft_id=info%3Ahdl%2F1813%2F31637&amp;rft_id=info%3Adoi%2F10.1080%2F00031305.1992.10475879&amp;rft.aulast=Altman&amp;rft.aufirst=Naomi+S.&amp;rft_id=https%3A%2F%2Fecommons.cornell.edu%2Fbitstream%2F1813%2F31637%2F1%2FBU-1065-MA.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><style data-mw-deduplicate="TemplateStyles:r935243608">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text">This scheme is a generalization of linear interpolation.</span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><cite class="citation journal">Jaskowiak, Pablo A.; Campello, Ricardo J. G. B. "Comparing Correlation Coefficients as Dissimilarity Measures for Cancer Classification in Gene Expression Data". <i>Brazilian Symposium on Bioinformatics (BSB 2011)</i>: 1–8. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.208.993" rel="nofollow">10.1.1.208.993</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Brazilian+Symposium+on+Bioinformatics+%28BSB+2011%29&amp;rft.atitle=Comparing+Correlation+Coefficients+as+Dissimilarity+Measures+for+Cancer+Classification+in+Gene+Expression+Data&amp;rft.pages=1-8&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.208.993&amp;rft.aulast=Jaskowiak&amp;rft.aufirst=Pablo+A.&amp;rft.au=Campello%2C+Ricardo+J.+G.+B.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Coomans_Massart1982-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-Coomans_Massart1982_4-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Coomans, Danny; Massart, Desire L. (1982). "Alternative k-nearest neighbour rules in supervised pattern recognition : Part 1. k-Nearest neighbour classification by using alternative voting rules". <i><a href="/wiki/Analytica_Chimica_Acta" title="Analytica Chimica Acta">Analytica Chimica Acta</a></i>. <b>136</b>: 15–27. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1016%2FS0003-2670%2801%2995359-0" rel="nofollow">10.1016/S0003-2670(01)95359-0</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Analytica+Chimica+Acta&amp;rft.atitle=Alternative+k-nearest+neighbour+rules+in+supervised+pattern+recognition+%3A+Part+1.+k-Nearest+neighbour+classification+by+using+alternative+voting+rules&amp;rft.volume=136&amp;rft.pages=15-27&amp;rft.date=1982&amp;rft_id=info%3Adoi%2F10.1016%2FS0003-2670%2801%2995359-0&amp;rft.aulast=Coomans&amp;rft.aufirst=Danny&amp;rft.au=Massart%2C+Desire+L.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text">Everitt, Brian S.; Landau, Sabine; Leese, Morven; and Stahl, Daniel (2011) "Miscellaneous Clustering Methods", in <i>Cluster Analysis</i>, 5th Edition, John Wiley &amp; Sons, Ltd., Chichester, UK</span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><cite class="citation journal">Nigsch, Florian; Bender, Andreas; van Buuren, Bernd; Tissen, Jos; Nigsch, Eduard; Mitchell, John B. O. (2006). "Melting point prediction employing k-nearest neighbor algorithms and genetic parameter optimization". <i>Journal of Chemical Information and Modeling</i>. <b>46</b> (6): 2412–2422. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1021%2Fci060149f" rel="nofollow">10.1021/ci060149f</a>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/17125183" rel="nofollow">17125183</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Chemical+Information+and+Modeling&amp;rft.atitle=Melting+point+prediction+employing+k-nearest+neighbor+algorithms+and+genetic+parameter+optimization&amp;rft.volume=46&amp;rft.issue=6&amp;rft.pages=2412-2422&amp;rft.date=2006&amp;rft_id=info%3Adoi%2F10.1021%2Fci060149f&amp;rft_id=info%3Apmid%2F17125183&amp;rft.aulast=Nigsch&amp;rft.aufirst=Florian&amp;rft.au=Bender%2C+Andreas&amp;rft.au=van+Buuren%2C+Bernd&amp;rft.au=Tissen%2C+Jos&amp;rft.au=Nigsch%2C+Eduard&amp;rft.au=Mitchell%2C+John+B.+O.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-HPS2008-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-HPS2008_7-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Hall, Peter; Park, Byeong U.; Samworth, Richard J. (2008). "Choice of neighbor order in nearest-neighbor classification". <i><a href="/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>. <b>36</b> (5): 2135–2152. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/0810.5276" rel="nofollow">0810.5276</a></span>. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2008arXiv0810.5276H" rel="nofollow">2008arXiv0810.5276H</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1214%2F07-AOS537" rel="nofollow">10.1214/07-AOS537</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annals+of+Statistics&amp;rft.atitle=Choice+of+neighbor+order+in+nearest-neighbor+classification&amp;rft.volume=36&amp;rft.issue=5&amp;rft.pages=2135-2152&amp;rft.date=2008&amp;rft_id=info%3Aarxiv%2F0810.5276&amp;rft_id=info%3Adoi%2F10.1214%2F07-AOS537&amp;rft_id=info%3Abibcode%2F2008arXiv0810.5276H&amp;rft.aulast=Hall&amp;rft.aufirst=Peter&amp;rft.au=Park%2C+Byeong+U.&amp;rft.au=Samworth%2C+Richard+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Stone-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-Stone_8-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Stone, Charles J. (1977). "Consistent nonparametric regression". <i><a href="/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>. <b>5</b> (4): 595–620. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1214%2Faos%2F1176343886" rel="nofollow">10.1214/aos/1176343886</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annals+of+Statistics&amp;rft.atitle=Consistent+nonparametric+regression&amp;rft.volume=5&amp;rft.issue=4&amp;rft.pages=595-620&amp;rft.date=1977&amp;rft_id=info%3Adoi%2F10.1214%2Faos%2F1176343886&amp;rft.aulast=Stone&amp;rft.aufirst=Charles+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Samworth12-9"><span class="mw-cite-backlink">^ <a href="#cite_ref-Samworth12_9-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Samworth12_9-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Samworth, Richard J. (2012). "Optimal weighted nearest neighbour classifiers". <i><a href="/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>. <b>40</b> (5): 2733–2763. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1101.5783" rel="nofollow">1101.5783</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1214%2F12-AOS1049" rel="nofollow">10.1214/12-AOS1049</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annals+of+Statistics&amp;rft.atitle=Optimal+weighted+nearest+neighbour+classifiers&amp;rft.volume=40&amp;rft.issue=5&amp;rft.pages=2733-2763&amp;rft.date=2012&amp;rft_id=info%3Aarxiv%2F1101.5783&amp;rft_id=info%3Adoi%2F10.1214%2F12-AOS1049&amp;rft.aulast=Samworth&amp;rft.aufirst=Richard+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Terrell_Scott1992-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-Terrell_Scott1992_10-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Terrell, George R.; Scott, David W. (1992). "Variable kernel density estimation". <i><a href="/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>. <b>20</b> (3): 1236–1265. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1214%2Faos%2F1176348768" rel="nofollow">10.1214/aos/1176348768</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annals+of+Statistics&amp;rft.atitle=Variable+kernel+density+estimation&amp;rft.volume=20&amp;rft.issue=3&amp;rft.pages=1236-1265&amp;rft.date=1992&amp;rft_id=info%3Adoi%2F10.1214%2Faos%2F1176348768&amp;rft.aulast=Terrell&amp;rft.aufirst=George+R.&amp;rft.au=Scott%2C+David+W.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Mills2010-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-Mills2010_11-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Mills, Peter. "Efficient statistical classification of satellite measurements". <i>International Journal of Remote Sensing</i>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Remote+Sensing&amp;rft.atitle=Efficient+statistical+classification+of+satellite+measurements&amp;rft.aulast=Mills&amp;rft.aufirst=Peter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="/wiki/Thomas_M._Cover" title="Thomas M. Cover">Cover, Thomas M.</a>; <a href="/wiki/Peter_E._Hart" title="Peter E. Hart">Hart, Peter E.</a> (1967). <a class="external text" href="http://ssg.mit.edu/cal/abs/2000_spring/np_dens/classification/cover67.pdf" rel="nofollow">"Nearest neighbor pattern classification"</a> <span class="cs1-format">(PDF)</span>. <i>IEEE Transactions on Information Theory</i>. <b>13</b> (1): 21–27. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.68.2616" rel="nofollow">10.1.1.68.2616</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1109%2FTIT.1967.1053964" rel="nofollow">10.1109/TIT.1967.1053964</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Information+Theory&amp;rft.atitle=Nearest+neighbor+pattern+classification&amp;rft.volume=13&amp;rft.issue=1&amp;rft.pages=21-27&amp;rft.date=1967&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.68.2616&amp;rft_id=info%3Adoi%2F10.1109%2FTIT.1967.1053964&amp;rft.aulast=Cover&amp;rft.aufirst=Thomas+M.&amp;rft.au=Hart%2C+Peter+E.&amp;rft_id=http%3A%2F%2Fssg.mit.edu%2Fcal%2Fabs%2F2000_spring%2Fnp_dens%2Fclassification%2Fcover67.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><cite class="citation journal">Toussaint, Godfried T. (April 2005). "Geometric proximity graphs for improving nearest neighbor methods in instance-based learning and data mining". <i>International Journal of Computational Geometry and Applications</i>. <b>15</b> (2): 101–150. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1142%2FS0218195905001622" rel="nofollow">10.1142/S0218195905001622</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Computational+Geometry+and+Applications&amp;rft.atitle=Geometric+proximity+graphs+for+improving+nearest+neighbor+methods+in+instance-based+learning+and+data+mining&amp;rft.volume=15&amp;rft.issue=2&amp;rft.pages=101-150&amp;rft.date=2005-04&amp;rft_id=info%3Adoi%2F10.1142%2FS0218195905001622&amp;rft.aulast=Toussaint&amp;rft.aufirst=Godfried+T.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-PTPR-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-PTPR_14-0">^</a></b></span> <span class="reference-text"><cite class="citation book">Devroye, Luc; Gyorfi, Laszlo; Lugosi, Gabor (1996). <i>A probabilistic theory of pattern recognition</i>. Springer. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-0-3879-4618-4" title="Special:BookSources/978-0-3879-4618-4"><bdi>978-0-3879-4618-4</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=A+probabilistic+theory+of+pattern+recognition&amp;rft.pub=Springer&amp;rft.date=1996&amp;rft.isbn=978-0-3879-4618-4&amp;rft.aulast=Devroye&amp;rft.aufirst=Luc&amp;rft.au=Gyorfi%2C+Laszlo&amp;rft.au=Lugosi%2C+Gabor&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><cite class="citation journal">Beyer, Kevin;  et al. <a class="external text" href="https://minds.wisconsin.edu/bitstream/handle/1793/60174/TR1377.pdf?sequence=1" rel="nofollow">"When is "nearest neighbor" meaningful?"</a> <span class="cs1-format">(PDF)</span>. <i>Database Theory—ICDT'99</i>. <b>1999</b>: 217–235.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Database+Theory%E2%80%94ICDT%2799&amp;rft.atitle=When+is+%22nearest+neighbor%22+meaningful%3F&amp;rft.volume=1999&amp;rft.pages=217-235&amp;rft.aulast=Beyer&amp;rft.aufirst=Kevin&amp;rft_id=https%3A%2F%2Fminds.wisconsin.edu%2Fbitstream%2Fhandle%2F1793%2F60174%2FTR1377.pdf%3Fsequence%3D1&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text">Shaw, Blake; and Jebara, Tony; "Structure preserving embedding", in <i>Proceedings of the 26th Annual International Conference on Machine Learning</i>, ACM, 2009</span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text">Bingham, Ella; and Mannila, Heikki; <a class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.5135&amp;rep=rep1&amp;type=pdf" rel="nofollow">"Random projection in dimensionality reduction: applications to image and text data"</a>, in <i>Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining</i>, ACM, 2001</span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text">Ryan, Donna (editor); <i>High Performance Discovery in Time Series</i>, Berlin: Springer, 2004, <link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/><a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/0-387-00857-8" title="Special:BookSources/0-387-00857-8">0-387-00857-8</a></span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><cite class="citation journal">Bremner, David; <a href="/wiki/Erik_Demaine" title="Erik Demaine">Demaine, Erik</a>; Erickson, Jeff; <a href="/wiki/John_Iacono" title="John Iacono">Iacono, John</a>; <a href="/wiki/Stefan_Langerman" title="Stefan Langerman">Langerman, Stefan</a>; <a href="/wiki/Pat_Morin" title="Pat Morin">Morin, Pat</a>; <a href="/wiki/Godfried_Toussaint" title="Godfried Toussaint">Toussaint, Godfried T.</a> (2005). "Output-sensitive algorithms for computing nearest-neighbor decision boundaries". <i>Discrete and Computational Geometry</i>. <b>33</b> (4): 593–604. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1007%2Fs00454-004-1152-0" rel="nofollow">10.1007/s00454-004-1152-0</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Discrete+and+Computational+Geometry&amp;rft.atitle=Output-sensitive+algorithms+for+computing+nearest-neighbor+decision+boundaries&amp;rft.volume=33&amp;rft.issue=4&amp;rft.pages=593-604&amp;rft.date=2005&amp;rft_id=info%3Adoi%2F10.1007%2Fs00454-004-1152-0&amp;rft.aulast=Bremner&amp;rft.aufirst=David&amp;rft.au=Demaine%2C+Erik&amp;rft.au=Erickson%2C+Jeff&amp;rft.au=Iacono%2C+John&amp;rft.au=Langerman%2C+Stefan&amp;rft.au=Morin%2C+Pat&amp;rft.au=Toussaint%2C+Godfried+T.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="/wiki/Peter_E._Hart" title="Peter E. Hart">Hart, Peter E.</a> (1968). "The Condensed Nearest Neighbor Rule". <i>IEEE Transactions on Information Theory</i>. <b>18</b>: 515–516. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1109%2FTIT.1968.1054155" rel="nofollow">10.1109/TIT.1968.1054155</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Information+Theory&amp;rft.atitle=The+Condensed+Nearest+Neighbor+Rule&amp;rft.volume=18&amp;rft.pages=515-516&amp;rft.date=1968&amp;rft_id=info%3Adoi%2F10.1109%2FTIT.1968.1054155&amp;rft.aulast=Hart&amp;rft.aufirst=Peter+E.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-MirkesKnn-21"><span class="mw-cite-backlink">^ <a href="#cite_ref-MirkesKnn_21-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-MirkesKnn_21-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">Mirkes, Evgeny M.; <a class="external text" href="http://www.math.le.ac.uk/people/ag153/homepage/KNN/KNN3.html" rel="nofollow"><i>KNN and Potential Energy: applet</i></a>, University of Leicester, 2011</span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><cite class="citation conference">Ramaswamy, Sridhar; Rastogi, Rajeev; Shim, Kyuseok (2000). <i>Efficient algorithms for mining outliers from large data sets</i>. Proceedings of the 2000 ACM SIGMOD international conference on Management of data – SIGMOD '00. p. 427. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1145%2F342009.335437" rel="nofollow">10.1145/342009.335437</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/1-58113-217-4" title="Special:BookSources/1-58113-217-4"><bdi>1-58113-217-4</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Efficient+algorithms+for+mining+outliers+from+large+data+sets&amp;rft.pages=427&amp;rft.date=2000&amp;rft_id=info%3Adoi%2F10.1145%2F342009.335437&amp;rft.isbn=1-58113-217-4&amp;rft.aulast=Ramaswamy&amp;rft.aufirst=Sridhar&amp;rft.au=Rastogi%2C+Rajeev&amp;rft.au=Shim%2C+Kyuseok&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-CamposZimek2016-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-CamposZimek2016_23-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Campos, Guilherme O.; Zimek, Arthur; Sander, Jörg; Campello, Ricardo J. G. B.; Micenková, Barbora; Schubert, Erich; Assent, Ira; Houle, Michael E. (2016). "On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study". <i>Data Mining and Knowledge Discovery</i>. <b>30</b> (4): 891–927. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1007%2Fs10618-015-0444-8" rel="nofollow">10.1007/s10618-015-0444-8</a>. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a> <a class="external text" href="//www.worldcat.org/issn/1384-5810" rel="nofollow">1384-5810</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Data+Mining+and+Knowledge+Discovery&amp;rft.atitle=On+the+evaluation+of+unsupervised+outlier+detection%3A+measures%2C+datasets%2C+and+an+empirical+study&amp;rft.volume=30&amp;rft.issue=4&amp;rft.pages=891-927&amp;rft.date=2016&amp;rft_id=info%3Adoi%2F10.1007%2Fs10618-015-0444-8&amp;rft.issn=1384-5810&amp;rft.aulast=Campos&amp;rft.aufirst=Guilherme+O.&amp;rft.au=Zimek%2C+Arthur&amp;rft.au=Sander%2C+J%C3%B6rg&amp;rft.au=Campello%2C+Ricardo+J.+G.+B.&amp;rft.au=Micenkov%C3%A1%2C+Barbora&amp;rft.au=Schubert%2C+Erich&amp;rft.au=Assent%2C+Ira&amp;rft.au=Houle%2C+Michael+E.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
</ol></div>
<h2><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=20" title="Edit section: Further reading">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><cite class="citation book"><a href="/wiki/Belur_V._Dasarathy" title="Belur V. Dasarathy">Dasarathy, Belur V.</a>, ed. (1991). <i>Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques</i>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-0-8186-8930-7" title="Special:BookSources/978-0-8186-8930-7"><bdi>978-0-8186-8930-7</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Nearest+Neighbor+%28NN%29+Norms%3A+NN+Pattern+Classification+Techniques&amp;rft.date=1991&amp;rft.isbn=978-0-8186-8930-7&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></li>
<li><cite class="citation book">Shakhnarovich, Gregory; Darrell, Trevor; Indyk, Piotr, eds. (2005). <i>Nearest-Neighbor Methods in Learning and Vision</i>. <a href="/wiki/MIT_Press" title="MIT Press">MIT Press</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-0-262-19547-8" title="Special:BookSources/978-0-262-19547-8"><bdi>978-0-262-19547-8</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Nearest-Neighbor+Methods+in+Learning+and+Vision&amp;rft.pub=MIT+Press&amp;rft.date=2005&amp;rft.isbn=978-0-262-19547-8&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm"></span><span class="cs1-maint citation-comment">CS1 maint: uses editors parameter (<a href="/wiki/Category:CS1_maint:_uses_editors_parameter" title="Category:CS1 maint: uses editors parameter">link</a>)</span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></li></ul>
<!-- 
NewPP limit report
Parsed by mw1361
Cached time: 20200405075407
Cache expiry: 2592000
Dynamic content: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.568 seconds
Real time usage: 0.790 seconds
Preprocessor visited node count: 2584/1000000
Post‐expand include size: 80714/2097152 bytes
Template argument size: 7293/2097152 bytes
Highest expansion depth: 15/40
Expensive parser function count: 11/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 72097/5000000 bytes
Number of Wikibase entities loaded: 6/400
Lua time usage: 0.273/10.000 seconds
Lua memory usage: 6.01 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  586.706      1 -total
 59.15%  347.043      1 Template:Reflist
 42.72%  250.632     15 Template:Cite_journal
 12.57%   73.765      3 Template:Citation_needed
 11.97%   70.248      4 Template:Fix
  7.87%   46.152      1 Template:ISBN
  7.39%   43.379      7 Template:Category_handler
  7.11%   41.744      1 Template:Machine_learning_bar
  6.71%   39.377      1 Template:Sidebar_with_collapsible_lists
  6.64%   38.983      1 Template:Distinguish
-->
<!-- Saved in parser cache with key enwiki:pcache:idhash:1775388-0!canonical!math=5 and timestamp 20200405075407 and revision id 949222834
 -->
</div><noscript><img alt="" height="1" src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" style="border: none; position: absolute;" title="" width="1"/></noscript></div>
<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;oldid=949222834">https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;oldid=949222834</a>"</div>
<div class="catlinks" data-mw="interface" id="catlinks"><div class="mw-normal-catlinks" id="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Classification_algorithms" title="Category:Classification algorithms">Classification algorithms</a></li><li><a href="/wiki/Category:Search_algorithms" title="Category:Search algorithms">Search algorithms</a></li><li><a href="/wiki/Category:Machine_learning_algorithms" title="Category:Machine learning algorithms">Machine learning algorithms</a></li><li><a href="/wiki/Category:Statistical_classification" title="Category:Statistical classification">Statistical classification</a></li><li><a href="/wiki/Category:Nonparametric_statistics" title="Category:Nonparametric statistics">Nonparametric statistics</a></li></ul></div><div class="mw-hidden-catlinks mw-hidden-cats-hidden" id="mw-hidden-catlinks">Hidden categories: <ul><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_March_2013" title="Category:Articles with unsourced statements from March 2013">Articles with unsourced statements from March 2013</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_December_2008" title="Category:Articles with unsourced statements from December 2008">Articles with unsourced statements from December 2008</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_January_2019" title="Category:Wikipedia articles needing clarification from January 2019">Wikipedia articles needing clarification from January 2019</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_September_2019" title="Category:Articles with unsourced statements from September 2019">Articles with unsourced statements from September 2019</a></li><li><a href="/wiki/Category:CS1_maint:_uses_editors_parameter" title="Category:CS1 maint: uses editors parameter">CS1 maint: uses editors parameter</a></li></ul></div></div>
<div class="visualClear"></div>
</div>
</div>
<div id="mw-data-after-content">
<div class="read-more-container"></div>
</div>
<div id="mw-navigation">
<h2>Navigation menu</h2>
<div id="mw-head">
<div aria-labelledby="p-personal-label" class="" id="p-personal" role="navigation">
<h3 id="p-personal-label">Personal tools</h3>
<ul>
<li id="pt-anonuserpage">Not logged in</li>
<li id="pt-anontalk"><a accesskey="n" href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]">Talk</a></li><li id="pt-anoncontribs"><a accesskey="y" href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=K-nearest+neighbors+algorithm" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a accesskey="o" href="/w/index.php?title=Special:UserLogin&amp;returnto=K-nearest+neighbors+algorithm" title="You're encouraged to log in; however, it's not mandatory. [o]">Log in</a></li>
</ul>
</div>
<div id="left-navigation">
<div aria-labelledby="p-namespaces-label" class="vectorTabs" id="p-namespaces" role="navigation">
<h3 id="p-namespaces-label">Namespaces</h3>
<ul>
<li class="selected" id="ca-nstab-main"><a accesskey="c" href="/wiki/K-nearest_neighbors_algorithm" title="View the content page [c]">Article</a></li><li id="ca-talk"><a accesskey="t" href="/wiki/Talk:K-nearest_neighbors_algorithm" rel="discussion" title="Discussion about the content page [t]">Talk</a></li>
</ul>
</div>
<div aria-labelledby="p-variants-label" class="vectorMenu emptyPortlet" id="p-variants" role="navigation">
<input aria-labelledby="p-variants-label" class="vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-variants-label">
<span>Variants</span>
</h3>
<ul class="menu">
</ul>
</div>
</div>
<div id="right-navigation">
<div aria-labelledby="p-views-label" class="vectorTabs" id="p-views" role="navigation">
<h3 id="p-views-label">Views</h3>
<ul>
<li class="collapsible selected" id="ca-view"><a href="/wiki/K-nearest_neighbors_algorithm">Read</a></li><li class="collapsible" id="ca-edit"><a accesskey="e" href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit" title="Edit this page [e]">Edit</a></li><li class="collapsible" id="ca-history"><a accesskey="h" href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=history" title="Past revisions of this page [h]">View history</a></li>
</ul>
</div>
<div aria-labelledby="p-cactions-label" class="vectorMenu emptyPortlet" id="p-cactions" role="navigation">
<input aria-labelledby="p-cactions-label" class="vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-cactions-label">
<span>More</span>
</h3>
<ul class="menu">
</ul>
</div>
<div id="p-search" role="search">
<h3>
<label for="searchInput">Search</label>
</h3>
<form action="/w/index.php" id="searchform">
<div id="simpleSearch">
<input accesskey="f" id="searchInput" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" type="search"/>
<input name="title" type="hidden" value="Special:Search"/>
<input class="searchButton mw-fallbackSearchButton" id="mw-searchButton" name="fulltext" title="Search Wikipedia for this text" type="submit" value="Search"/>
<input class="searchButton" id="searchButton" name="go" title="Go to a page with this exact name if it exists" type="submit" value="Go"/>
</div>
</form>
</div>
</div>
</div>
<div id="mw-panel">
<div id="p-logo" role="banner">
<a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a>
</div>
<div aria-labelledby="p-navigation-label" class="portal" id="p-navigation" role="navigation">
<h3 id="p-navigation-label">
    			Navigation
    		</h3>
<div class="body">
<ul><li id="n-mainpage-description"><a accesskey="z" href="/wiki/Main_Page" title="Visit the main page [z]">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Wikipedia:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a accesskey="x" href="/wiki/Special:Random" title="Load a random article [x]">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li></ul>
</div>
</div>
<div aria-labelledby="p-interaction-label" class="portal" id="p-interaction" role="navigation">
<h3 id="p-interaction-label">
    			Interaction
    		</h3>
<div class="body">
<ul><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a accesskey="r" href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li></ul>
</div>
</div>
<div aria-labelledby="p-tb-label" class="portal" id="p-tb" role="navigation">
<h3 id="p-tb-label">
    			Tools
    		</h3>
<div class="body">
<ul><li id="t-whatlinkshere"><a accesskey="j" href="/wiki/Special:WhatLinksHere/K-nearest_neighbors_algorithm" title="List of all English Wikipedia pages containing links to this page [j]">What links here</a></li><li id="t-recentchangeslinked"><a accesskey="k" href="/wiki/Special:RecentChangesLinked/K-nearest_neighbors_algorithm" rel="nofollow" title="Recent changes in pages linked from this page [k]">Related changes</a></li><li id="t-upload"><a accesskey="u" href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]">Upload file</a></li><li id="t-specialpages"><a accesskey="q" href="/wiki/Special:SpecialPages" title="A list of all special pages [q]">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;oldid=949222834" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a accesskey="g" href="https://www.wikidata.org/wiki/Special:EntityPage/Q1071612" title="Link to connected data repository item [g]">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=K-nearest_neighbors_algorithm&amp;id=949222834&amp;wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li></ul>
</div>
</div>
<div aria-labelledby="p-coll-print_export-label" class="portal" id="p-coll-print_export" role="navigation">
<h3 id="p-coll-print_export-label">
    			Print/export
    		</h3>
<div class="body">
<ul><li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=K-nearest+neighbors+algorithm">Create a book</a></li><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=K-nearest+neighbors+algorithm&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a accesskey="p" href="/w/index.php?title=K-nearest_neighbors_algorithm&amp;printable=yes" title="Printable version of this page [p]">Printable version</a></li></ul>
</div>
</div>
<div aria-labelledby="p-lang-label" class="portal" id="p-lang" role="navigation">
<h3 id="p-lang-label">
    			Languages
    		</h3>
<div class="body">
<ul><li class="interlanguage-link interwiki-ca"><a class="interlanguage-link-target" href="https://ca.wikipedia.org/wiki/Knn" hreflang="ca" lang="ca" title="Knn – Catalan">Català</a></li><li class="interlanguage-link interwiki-cs"><a class="interlanguage-link-target" href="https://cs.wikipedia.org/wiki/Algoritmus_k-nejbli%C5%BE%C5%A1%C3%ADch_soused%C5%AF" hreflang="cs" lang="cs" title="Algoritmus k-nejbližších sousedů – Czech">Čeština</a></li><li class="interlanguage-link interwiki-da"><a class="interlanguage-link-target" href="https://da.wikipedia.org/wiki/K-n%C3%A6rmeste_naboer" hreflang="da" lang="da" title="K-nærmeste naboer – Danish">Dansk</a></li><li class="interlanguage-link interwiki-de"><a class="interlanguage-link-target" href="https://de.wikipedia.org/wiki/N%C3%A4chste-Nachbarn-Klassifikation" hreflang="de" lang="de" title="Nächste-Nachbarn-Klassifikation – German">Deutsch</a></li><li class="interlanguage-link interwiki-es"><a class="interlanguage-link-target" href="https://es.wikipedia.org/wiki/K_vecinos_m%C3%A1s_pr%C3%B3ximos" hreflang="es" lang="es" title="K vecinos más próximos – Spanish">Español</a></li><li class="interlanguage-link interwiki-eu"><a class="interlanguage-link-target" href="https://eu.wikipedia.org/wiki/K_auzokide_hurbilenak" hreflang="eu" lang="eu" title="K auzokide hurbilenak – Basque">Euskara</a></li><li class="interlanguage-link interwiki-fa"><a class="interlanguage-link-target" href="https://fa.wikipedia.org/wiki/%D8%A7%D9%84%DA%AF%D9%88%D8%B1%DB%8C%D8%AA%D9%85_%DA%A9%DB%8C-%D9%86%D8%B2%D8%AF%DB%8C%DA%A9%E2%80%8C%D8%AA%D8%B1%DB%8C%D9%86_%D9%87%D9%85%D8%B3%D8%A7%DB%8C%D9%87" hreflang="fa" lang="fa" title="الگوریتم کی-نزدیک‌ترین همسایه – Persian">فارسی</a></li><li class="interlanguage-link interwiki-fr"><a class="interlanguage-link-target" href="https://fr.wikipedia.org/wiki/M%C3%A9thode_des_k_plus_proches_voisins" hreflang="fr" lang="fr" title="Méthode des k plus proches voisins – French">Français</a></li><li class="interlanguage-link interwiki-ko"><a class="interlanguage-link-target" href="https://ko.wikipedia.org/wiki/K-%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98" hreflang="ko" lang="ko" title="K-최근접 이웃 알고리즘 – Korean">한국어</a></li><li class="interlanguage-link interwiki-id"><a class="interlanguage-link-target" href="https://id.wikipedia.org/wiki/KNN" hreflang="id" lang="id" title="KNN – Indonesian">Bahasa Indonesia</a></li><li class="interlanguage-link interwiki-it"><a class="interlanguage-link-target" href="https://it.wikipedia.org/wiki/K-nearest_neighbors" hreflang="it" lang="it" title="K-nearest neighbors – Italian">Italiano</a></li><li class="interlanguage-link interwiki-he"><a class="interlanguage-link-target" href="https://he.wikipedia.org/wiki/%D7%90%D7%9C%D7%92%D7%95%D7%A8%D7%99%D7%AA%D7%9D_%D7%A9%D7%9B%D7%9F_%D7%A7%D7%A8%D7%95%D7%91" hreflang="he" lang="he" title="אלגוריתם שכן קרוב – Hebrew">עברית</a></li><li class="interlanguage-link interwiki-ja"><a class="interlanguage-link-target" href="https://ja.wikipedia.org/wiki/K%E8%BF%91%E5%82%8D%E6%B3%95" hreflang="ja" lang="ja" title="K近傍法 – Japanese">日本語</a></li><li class="interlanguage-link interwiki-no"><a class="interlanguage-link-target" href="https://no.wikipedia.org/wiki/K-NN" hreflang="nb" lang="nb" title="K-NN – Norwegian Bokmål">Norsk bokmål</a></li><li class="interlanguage-link interwiki-pl"><a class="interlanguage-link-target" href="https://pl.wikipedia.org/wiki/K_najbli%C5%BCszych_s%C4%85siad%C3%B3w" hreflang="pl" lang="pl" title="K najbliższych sąsiadów – Polish">Polski</a></li><li class="interlanguage-link interwiki-ru"><a class="interlanguage-link-target" href="https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_k-%D0%B1%D0%BB%D0%B8%D0%B6%D0%B0%D0%B9%D1%88%D0%B8%D1%85_%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B5%D0%B9" hreflang="ru" lang="ru" title="Метод k-ближайших соседей – Russian">Русский</a></li><li class="interlanguage-link interwiki-sr"><a class="interlanguage-link-target" href="https://sr.wikipedia.org/wiki/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%B0%D0%BC_%D0%BA_%D0%BD%D0%B0%D1%98%D0%B1%D0%BB%D0%B8%D0%B6%D0%B8%D1%85_%D1%81%D1%83%D1%81%D0%B5%D0%B4%D0%B0" hreflang="sr" lang="sr" title="Алгоритам к најближих суседа – Serbian">Српски / srpski</a></li><li class="interlanguage-link interwiki-th"><a class="interlanguage-link-target" href="https://th.wikipedia.org/wiki/%E0%B8%82%E0%B8%B1%E0%B9%89%E0%B8%99%E0%B8%95%E0%B8%AD%E0%B8%99%E0%B8%A7%E0%B8%B4%E0%B8%98%E0%B8%B5%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%84%E0%B9%89%E0%B8%99%E0%B8%AB%E0%B8%B2%E0%B9%80%E0%B8%9E%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B8%99%E0%B8%9A%E0%B9%89%E0%B8%B2%E0%B8%99%E0%B9%83%E0%B8%81%E0%B8%A5%E0%B9%89%E0%B8%AA%E0%B8%B8%E0%B8%94_k_%E0%B8%95%E0%B8%B1%E0%B8%A7" hreflang="th" lang="th" title="ขั้นตอนวิธีการค้นหาเพื่อนบ้านใกล้สุด k ตัว – Thai">ไทย</a></li><li class="interlanguage-link interwiki-uk"><a class="interlanguage-link-target" href="https://uk.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_k-%D0%BD%D0%B0%D0%B9%D0%B1%D0%BB%D0%B8%D0%B6%D1%87%D0%B8%D1%85_%D1%81%D1%83%D1%81%D1%96%D0%B4%D1%96%D0%B2" hreflang="uk" lang="uk" title="Метод k-найближчих сусідів – Ukrainian">Українська</a></li><li class="interlanguage-link interwiki-vi"><a class="interlanguage-link-target" href="https://vi.wikipedia.org/wiki/Gi%E1%BA%A3i_thu%E1%BA%ADt_k_h%C3%A0ng_x%C3%B3m_g%E1%BA%A7n_nh%E1%BA%A5t" hreflang="vi" lang="vi" title="Giải thuật k hàng xóm gần nhất – Vietnamese">Tiếng Việt</a></li><li class="interlanguage-link interwiki-zh"><a class="interlanguage-link-target" href="https://zh.wikipedia.org/wiki/K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95" hreflang="zh" lang="zh" title="K-近邻算法 – Chinese">中文</a></li></ul>
<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a class="wbc-editpage" href="https://www.wikidata.org/wiki/Special:EntityPage/Q1071612#sitelinks-wikipedia" title="Edit interlanguage links">Edit links</a></span></div>
</div>
</div>
</div>
</div>
<div id="footer" role="contentinfo">
<ul class="" id="footer-info">
<li id="footer-info-lastmod"> This page was last edited on 5 April 2020, at 07:54<span class="anonymous-show"> (UTC)</span>.</li>
<li id="footer-info-copyright">Text is available under the <a href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" rel="license">Creative Commons Attribution-ShareAlike License</a><a href="//creativecommons.org/licenses/by-sa/3.0/" rel="license" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>
<ul class="" id="footer-places">
<li id="footer-places-privacy"><a class="extiw" href="https://foundation.wikimedia.org/wiki/Privacy_policy" title="wmf:Privacy policy">Privacy policy</a></li>
<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
<li id="footer-places-mobileview"><a class="noprint stopMobileRedirectToggle" href="//en.m.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;mobileaction=toggle_view_mobile">Mobile view</a></li>
</ul>
<ul class="noprint" id="footer-icons">
<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img alt="Wikimedia Foundation" height="31" src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88"/></a></li>
<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img alt="Powered by MediaWiki" height="31" src="/static/images/poweredby_mediawiki_88x31.png" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88"/></a></li>
</ul>
<div style="clear: both;"></div>
</div>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.568","walltime":"0.790","ppvisitednodes":{"value":2584,"limit":1000000},"postexpandincludesize":{"value":80714,"limit":2097152},"templateargumentsize":{"value":7293,"limit":2097152},"expansiondepth":{"value":15,"limit":40},"expensivefunctioncount":{"value":11,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":72097,"limit":5000000},"entityaccesscount":{"value":6,"limit":400},"timingprofile":["100.00%  586.706      1 -total"," 59.15%  347.043      1 Template:Reflist"," 42.72%  250.632     15 Template:Cite_journal"," 12.57%   73.765      3 Template:Citation_needed"," 11.97%   70.248      4 Template:Fix","  7.87%   46.152      1 Template:ISBN","  7.39%   43.379      7 Template:Category_handler","  7.11%   41.744      1 Template:Machine_learning_bar","  6.71%   39.377      1 Template:Sidebar_with_collapsible_lists","  6.64%   38.983      1 Template:Distinguish"]},"scribunto":{"limitreport-timeusage":{"value":"0.273","limit":"10.000"},"limitreport-memusage":{"value":6299530,"limit":52428800}},"cachereport":{"origin":"mw1361","timestamp":"20200405075407","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"K-nearest neighbors algorithm","url":"https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm","sameAs":"http:\/\/www.wikidata.org\/entity\/Q1071612","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q1071612","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2005-04-21T15:50:19Z","dateModified":"2020-04-05T07:54:02Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Kernel_Machine.svg","headline":"algorithm"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":95,"wgHostname":"mw1273"});});</script></body></html>
