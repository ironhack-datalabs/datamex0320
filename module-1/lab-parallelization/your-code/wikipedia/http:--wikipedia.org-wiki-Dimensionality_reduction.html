<!DOCTYPE html>
<html class="client-nojs" dir="ltr" lang="en">
<head>
<meta charset="utf-8"/>
<title>Dimensionality reduction - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"XotOagpAMNMAAALFsOcAAABT","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Dimensionality_reduction","wgTitle":"Dimensionality reduction","wgCurRevisionId":949231135,"wgRevisionId":949231135,"wgArticleId":579867,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Articles with short description","Articles needing additional references from November 2010","All articles needing additional references","All articles with unsourced statements","Articles with unsourced statements from September 2017",
"Wikipedia articles needing clarification from September 2017","Articles needing additional references from June 2017","Articles with unsourced statements from June 2017","Dimension reduction","Machine learning"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Dimensionality_reduction","wgRelevantArticleId":579867,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgWikibaseItemId":"Q16000077","wgCentralAuthMobileDomain":!1,
"wgEditSubmitButtonLabelPublish":!0};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","mediawiki.toc.styles":"ready","skins.vector.styles.legacy":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","site","mediawiki.page.startup","skins.vector.js","mediawiki.page.ready","mediawiki.toc","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface"
,"ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector" rel="stylesheet"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta content="" name="ResourceLoaderDynamicStyles"/>
<link href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector" rel="stylesheet"/>
<meta content="MediaWiki 1.35.0-wmf.26" name="generator"/>
<meta content="origin" name="referrer"/>
<meta content="origin-when-crossorigin" name="referrer"/>
<meta content="origin-when-cross-origin" name="referrer"/>
<meta content="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/1200px-Kernel_Machine.svg.png" property="og:image"/>
<link href="/w/index.php?title=Dimensionality_reduction&amp;action=edit" rel="alternate" title="Edit this page" type="application/x-wiki"/>
<link href="/w/index.php?title=Dimensionality_reduction&amp;action=edit" rel="edit" title="Edit this page"/>
<link href="/static/apple-touch/wikipedia.png" rel="apple-touch-icon"/>
<link href="/static/favicon/wikipedia.ico" rel="shortcut icon"/>
<link href="/w/opensearch_desc.php" rel="search" title="Wikipedia (en)" type="application/opensearchdescription+xml"/>
<link href="//en.wikipedia.org/w/api.php?action=rsd" rel="EditURI" type="application/rsd+xml"/>
<link href="//creativecommons.org/licenses/by-sa/3.0/" rel="license"/>
<link href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" rel="alternate" title="Wikipedia Atom feed" type="application/atom+xml"/>
<link href="https://en.wikipedia.org/wiki/Dimensionality_reduction" rel="canonical"/>
<link href="//login.wikimedia.org" rel="dns-prefetch"/>
<link href="//meta.wikimedia.org" rel="dns-prefetch"/>
<!--[if lt IE 9]><script src="/w/resources/lib/html5shiv/html5shiv.js"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Dimensionality_reduction rootpage-Dimensionality_reduction skin-vector action-view">
<div class="noprint" id="mw-page-base"></div>
<div class="noprint" id="mw-head-base"></div>
<div class="mw-body" id="content" role="main">
<a id="top"></a>
<div class="mw-body-content" id="siteNotice"><!-- CentralNotice --></div>
<div class="mw-indicators mw-body-content">
</div>
<h1 class="firstHeading" id="firstHeading" lang="en">Dimensionality reduction</h1>
<div class="mw-body-content" id="bodyContent">
<div class="noprint" id="siteSub">From Wikipedia, the free encyclopedia</div>
<div id="contentSub"></div>
<div id="jump-to-nav"></div>
<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
<a class="mw-jump-link" href="#p-search">Jump to search</a>
<div class="mw-content-ltr" dir="ltr" id="mw-content-text" lang="en"><div class="mw-parser-output"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Process of reducing the number of random variables under consideration</div>
<div class="hatnote navigation-not-searchable" role="note">For dimensional reduction in physics, see <a href="/wiki/Dimensional_reduction" title="Dimensional reduction">Dimensional reduction</a>.</div>
<table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a> and<br/><a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;"><a class="image" href="/wiki/File:Kernel_Machine.svg"><img alt="Kernel Machine.svg" data-file-height="233" data-file-width="512" decoding="async" height="100" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/220px-Kernel_Machine.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x" width="220"/></a></td></tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><div style="padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br/><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b> • <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>
<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>
<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>
<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="/wiki/Support-vector_machine" title="Support-vector machine">Support vector machine (SVM)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a class="mw-redirect" href="/wiki/CURE_data_clustering_algorithm" title="CURE data clustering algorithm">CURE</a></li>
<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li>
<li><br/><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a class="mw-redirect" href="/wiki/Mean-shift" title="Mean-shift">Mean-shift</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a class="mw-selflink selflink">Dimensionality reduction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>
<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>
<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>
<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a class="mw-redirect" href="/wiki/K-nearest_neighbors_classification" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>
<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li></ul></li>
<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>
<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a class="mw-redirect" href="/wiki/Bias%E2%80%93variance_dilemma" title="Bias–variance dilemma">Bias–variance dilemma</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine-learning venues</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>
<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>
<li><a class="external text" href="https://arxiv.org/list/cs.LG/recent" rel="nofollow">ArXiv:cs.LG</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Related articles</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>
<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul>
</div></div></div></td>
</tr><tr><td style="text-align:right;font-size:115%;padding-top: 0.6em;"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<table class="box-More_citations_needed plainlinks metadata ambox ambox-content ambox-Refimprove" role="presentation"><tbody><tr><td class="mbox-image"><div style="width:52px"><a class="image" href="/wiki/File:Question_book-new.svg"><img alt="" data-file-height="399" data-file-width="512" decoding="async" height="39" src="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/75px-Question_book-new.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/100px-Question_book-new.svg.png 2x" width="50"/></a></div></td><td class="mbox-text"><div class="mbox-text-span">This article <b>needs additional citations for <a href="/wiki/Wikipedia:Verifiability" title="Wikipedia:Verifiability">verification</a></b>.<span class="hide-when-compact"> Please help <a class="external text" href="https://en.wikipedia.org/w/index.php?title=Dimensionality_reduction&amp;action=edit">improve this article</a> by <a href="/wiki/Help:Introduction_to_referencing_with_Wiki_Markup/1" title="Help:Introduction to referencing with Wiki Markup/1">adding citations to reliable sources</a>. Unsourced material may be challenged and removed.<br/><small><span class="plainlinks"><i>Find sources:</i> <a class="external text" href="//www.google.com/search?as_eq=wikipedia&amp;q=%22Dimensionality+reduction%22" rel="nofollow">"Dimensionality reduction"</a> – <a class="external text" href="//www.google.com/search?tbm=nws&amp;q=%22Dimensionality+reduction%22+-wikipedia" rel="nofollow">news</a> <b>·</b> <a class="external text" href="//www.google.com/search?&amp;q=%22Dimensionality+reduction%22+site:news.google.com/newspapers&amp;source=newspapers" rel="nofollow">newspapers</a> <b>·</b> <a class="external text" href="//www.google.com/search?tbs=bks:1&amp;q=%22Dimensionality+reduction%22+-wikipedia" rel="nofollow">books</a> <b>·</b> <a class="external text" href="//scholar.google.com/scholar?q=%22Dimensionality+reduction%22" rel="nofollow">scholar</a> <b>·</b> <a class="external text" href="https://www.jstor.org/action/doBasicSearch?Query=%22Dimensionality+reduction%22&amp;acc=on&amp;wc=on" rel="nofollow">JSTOR</a></span></small></span> <small class="date-container"><i>(<span class="date">November 2010</span>)</i></small><small class="hide-when-compact"><i> (<a href="/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>
<p>In  <a href="/wiki/Statistics" title="Statistics">statistics</a>, <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, and <a href="/wiki/Information_theory" title="Information theory">information theory</a>, <b>dimensionality reduction</b> or <b>dimension reduction</b> is the process of reducing the number of random variables under consideration<sup class="reference" id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup> by obtaining a set of principal variables. Approaches can be divided into <a href="/wiki/Feature_selection" title="Feature selection">feature selection</a> and <a href="/wiki/Feature_extraction" title="Feature extraction">feature extraction</a>.<sup class="reference" id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup>
</p>
<div aria-labelledby="mw-toc-heading" class="toc" id="toc" role="navigation"><input class="toctogglecheckbox" id="toctogglecheckbox" role="button" style="display:none" type="checkbox"/><div class="toctitle" dir="ltr" lang="en"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Feature_selection"><span class="tocnumber">1</span> <span class="toctext">Feature selection</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Feature_projection"><span class="tocnumber">2</span> <span class="toctext">Feature projection</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Principal_component_analysis_(PCA)"><span class="tocnumber">2.1</span> <span class="toctext">Principal component analysis (PCA)</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Non-negative_matrix_factorization_(NMF)"><span class="tocnumber">2.2</span> <span class="toctext">Non-negative matrix factorization (NMF)</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#Kernel_PCA"><span class="tocnumber">2.3</span> <span class="toctext">Kernel PCA</span></a></li>
<li class="toclevel-2 tocsection-6"><a href="#Graph-based_kernel_PCA"><span class="tocnumber">2.4</span> <span class="toctext">Graph-based kernel PCA</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="#Linear_discriminant_analysis_(LDA)"><span class="tocnumber">2.5</span> <span class="toctext">Linear discriminant analysis (LDA)</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#Generalized_discriminant_analysis_(GDA)"><span class="tocnumber">2.6</span> <span class="toctext">Generalized discriminant analysis (GDA)</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Autoencoder"><span class="tocnumber">2.7</span> <span class="toctext">Autoencoder</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="#t-SNE"><span class="tocnumber">2.8</span> <span class="toctext">t-SNE</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="#UMAP"><span class="tocnumber">2.9</span> <span class="toctext">UMAP</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-12"><a href="#Dimension_reduction"><span class="tocnumber">3</span> <span class="toctext">Dimension reduction</span></a></li>
<li class="toclevel-1 tocsection-13"><a href="#Advantages_of_dimensionality_reduction"><span class="tocnumber">4</span> <span class="toctext">Advantages of dimensionality reduction</span></a></li>
<li class="toclevel-1 tocsection-14"><a href="#Applications"><span class="tocnumber">5</span> <span class="toctext">Applications</span></a></li>
<li class="toclevel-1 tocsection-15"><a href="#See_also"><span class="tocnumber">6</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-16"><a href="#Notes"><span class="tocnumber">7</span> <span class="toctext">Notes</span></a></li>
<li class="toclevel-1 tocsection-17"><a href="#References"><span class="tocnumber">8</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-18"><a href="#External_links"><span class="tocnumber">9</span> <span class="toctext">External links</span></a></li>
</ul>
</div>
<h2><span class="mw-headline" id="Feature_selection">Feature selection</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=1" title="Edit section: Feature selection">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="hatnote navigation-not-searchable" role="note">Main article: <a href="/wiki/Feature_selection" title="Feature selection">Feature selection</a></div><div class="hatnote navigation-not-searchable" role="note">See also: <a href="/wiki/Combinatorial_optimization" title="Combinatorial optimization">Combinatorial optimization</a></div>
<p><a href="/wiki/Feature_selection" title="Feature selection">Feature selection</a> approaches try to find a subset of the input variables (also called features or attributes). The three strategies are: the <i>filter</i> strategy (e.g. <a href="/wiki/Information_gain_in_decision_trees" title="Information gain in decision trees">information gain</a>), the <i>wrapper</i> strategy (e.g. search guided by accuracy), and the <i>embedded</i> strategy (selected features add or are removed while building the model based on prediction errors).
</p><p><a href="/wiki/Data_analysis" title="Data analysis">Data analysis</a> such as <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a> or <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> can be done in the reduced space more accurately than in the original space.<sup class="reference" id="cite_ref-3"><a href="#cite_note-3">[3]</a></sup>
</p>
<h2><span class="mw-headline" id="Feature_projection">Feature projection</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=2" title="Edit section: Feature projection">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="hatnote navigation-not-searchable" role="note">Main article: <a href="/wiki/Feature_extraction" title="Feature extraction">Feature extraction</a></div>
<p>Feature projection (also called Feature extraction) transforms the data in the <a class="mw-redirect" href="/wiki/High-dimensional_space" title="High-dimensional space">high-dimensional space</a> to a space of fewer dimensions. The data transformation may be linear, as in <a href="/wiki/Principal_component_analysis" title="Principal component analysis">principal component analysis</a> (PCA), but many <a href="/wiki/Nonlinear_dimensionality_reduction" title="Nonlinear dimensionality reduction">nonlinear dimensionality reduction</a> techniques also exist.<sup class="reference" id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup><sup class="reference" id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup> For multidimensional data, <a href="/wiki/Tensor" title="Tensor">tensor</a> representation can be used in dimensionality reduction through <a href="/wiki/Multilinear_subspace_learning" title="Multilinear subspace learning">multilinear subspace learning</a>.<sup class="reference" id="cite_ref-MSLsurvey_6-0"><a href="#cite_note-MSLsurvey-6">[6]</a></sup>
</p>
<h3><span id="Principal_component_analysis_.28PCA.29"></span><span class="mw-headline" id="Principal_component_analysis_(PCA)">Principal component analysis (PCA)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=3" title="Edit section: Principal component analysis (PCA)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="hatnote navigation-not-searchable" role="note">Main article: <a href="/wiki/Principal_component_analysis" title="Principal component analysis">Principal component analysis</a></div>
<p>The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the <a href="/wiki/Covariance" title="Covariance">covariance</a> (and sometimes the <a href="/wiki/Correlation_and_dependence" title="Correlation and dependence">correlation</a>) <a href="/wiki/Matrix_(mathematics)" title="Matrix (mathematics)">matrix</a> of the data is constructed and the <a class="mw-redirect" href="/wiki/Eigenvalue,_eigenvector_and_eigenspace" title="Eigenvalue, eigenvector and eigenspace">eigenvectors</a> on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behavior of the system, because they often contribute the vast majority of the system's energy, especially in low-dimensional systems. Still, this must be proven on a case-by-case basis as not all systems exhibit this behavior.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (September 2017)">citation needed</span></a></i>]</sup><sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The reason for this is unclear. (September 2017)">why?</span></a></i>]</sup>. The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors.
</p>
<h3><span id="Non-negative_matrix_factorization_.28NMF.29"></span><span class="mw-headline" id="Non-negative_matrix_factorization_(NMF)">Non-negative matrix factorization (NMF)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=4" title="Edit section: Non-negative matrix factorization (NMF)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="hatnote navigation-not-searchable" role="note">Main article: <a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">Non-negative matrix factorization</a></div>
<p>NMF decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist.<sup class="reference" id="cite_ref-lee-seung_7-0"><a href="#cite_note-lee-seung-7">[7]</a></sup><sup class="reference" id="cite_ref-lee2001algorithms_8-0"><a href="#cite_note-lee2001algorithms-8">[8]</a></sup> such as astronomy<sup class="reference" id="cite_ref-blantonRoweis07_9-0"><a href="#cite_note-blantonRoweis07-9">[9]</a></sup><sup class="reference" id="cite_ref-ren18_10-0"><a href="#cite_note-ren18-10">[10]</a></sup>. NMF is well known since the multiplicative update rule by Lee &amp; Seung<sup class="reference" id="cite_ref-lee-seung_7-1"><a href="#cite_note-lee-seung-7">[7]</a></sup>, which has been continuously developed: the inclusion of uncertainties <sup class="reference" id="cite_ref-blantonRoweis07_9-1"><a href="#cite_note-blantonRoweis07-9">[9]</a></sup>, the consideration of missing data and parallel computation <sup class="reference" id="cite_ref-zhu16_11-0"><a href="#cite_note-zhu16-11">[11]</a></sup>, sequential construction<sup class="reference" id="cite_ref-zhu16_11-1"><a href="#cite_note-zhu16-11">[11]</a></sup> which leads to the stability and linearity of NMF<sup class="reference" id="cite_ref-ren18_10-1"><a href="#cite_note-ren18-10">[10]</a></sup>, as well as other <a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">updates</a> including handling missing data in <a href="/wiki/Digital_image_processing" title="Digital image processing">digital image processing</a><sup class="reference" id="cite_ref-ren20”_12-0"><a href="#cite_note-ren20”-12">[12]</a></sup>.
</p><p>With a stable component basis during construction, and a linear modeling process, <a href="/wiki/Non-negative_matrix_factorization#Sequential_NMF" title="Non-negative matrix factorization">sequential NMF</a><sup class="reference" id="cite_ref-zhu16_11-2"><a href="#cite_note-zhu16-11">[11]</a></sup> is able to preserve the flux in direct imaging of circumstellar structures in astromony<sup class="reference" id="cite_ref-ren18_10-2"><a href="#cite_note-ren18-10">[10]</a></sup>, as one of the <a href="/wiki/Methods_of_detecting_exoplanets" title="Methods of detecting exoplanets">methods of detecting exoplanets</a>, especially for the direct imaging of <a class="mw-redirect" href="/wiki/Circumstellar_disks" title="Circumstellar disks">circumstellar disks</a>. In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA as demonstrated by Ren et al<sup class="reference" id="cite_ref-ren18_10-3"><a href="#cite_note-ren18-10">[10]</a></sup>.
</p>
<h3><span class="mw-headline" id="Kernel_PCA">Kernel PCA</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=5" title="Edit section: Kernel PCA">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="hatnote navigation-not-searchable" role="note">Main article: <a class="mw-redirect" href="/wiki/Kernel_PCA" title="Kernel PCA">Kernel PCA</a></div>
<p>Principal component analysis can be employed in a nonlinear way by means of the <a class="mw-redirect" href="/wiki/Kernel_trick" title="Kernel trick">kernel trick</a>. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is entitled <a class="mw-redirect" href="/wiki/Kernel_PCA" title="Kernel PCA">kernel PCA</a>.
</p>
<h3><span class="mw-headline" id="Graph-based_kernel_PCA">Graph-based kernel PCA</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=6" title="Edit section: Graph-based kernel PCA">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Other prominent nonlinear techniques include <a class="mw-redirect" href="/wiki/Manifold_learning" title="Manifold learning">manifold learning</a> techniques such as <a href="/wiki/Isomap" title="Isomap">Isomap</a>, <a class="mw-redirect" href="/wiki/Locally_linear_embedding" title="Locally linear embedding">locally linear embedding</a> (LLE), Hessian LLE, Laplacian eigenmaps, and methods based on tangent space analysis<sup class="reference" id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup><sup class="reference" id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup>. These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA.
</p><p>More recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using <a href="/wiki/Semidefinite_programming" title="Semidefinite programming">semidefinite programming</a>. The most prominent example of such a technique is <a class="mw-redirect" href="/wiki/Maximum_variance_unfolding" title="Maximum variance unfolding">maximum variance unfolding</a> (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space), while maximizing the distances between points that are not nearest neighbors.
</p><p>An alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include: classical <a href="/wiki/Multidimensional_scaling" title="Multidimensional scaling">multidimensional scaling</a>, which is identical to PCA; <a href="/wiki/Isomap" title="Isomap">Isomap</a>, which uses geodesic distances in the data space; <a href="/wiki/Diffusion_map" title="Diffusion map">diffusion maps</a>, which use diffusion distances in the data space; <a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-distributed stochastic neighbor embedding</a> (t-SNE), which minimizes the divergence between distributions over pairs of points; and curvilinear component analysis.
</p><p>A different approach to nonlinear dimensionality reduction is through the use of <a href="/wiki/Autoencoder" title="Autoencoder">autoencoders</a>, a special kind of feed-forward <a href="/wiki/Neural_network" title="Neural network">neural networks</a> with a bottle-neck hidden layer.<sup class="reference" id="cite_ref-15"><a href="#cite_note-15">[15]</a></sup> The training of deep encoders is typically performed using a greedy layer-wise pre-training (e.g., using a stack of <a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">restricted Boltzmann machines</a>) that is followed by a finetuning stage based on <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a>.
</p>
<h3><span id="Linear_discriminant_analysis_.28LDA.29"></span><span class="mw-headline" id="Linear_discriminant_analysis_(LDA)">Linear discriminant analysis (LDA)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=7" title="Edit section: Linear discriminant analysis (LDA)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="hatnote navigation-not-searchable" role="note">Main article: <a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">Linear discriminant analysis</a></div>
<p>Linear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events.
</p>
<h3><span id="Generalized_discriminant_analysis_.28GDA.29"></span><span class="mw-headline" id="Generalized_discriminant_analysis_(GDA)">Generalized discriminant analysis (GDA)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=8" title="Edit section: Generalized discriminant analysis (GDA)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>GDA deals with nonlinear discriminant analysis using kernel function operator. The underlying theory is close to the <a class="mw-redirect" href="/wiki/Support_vector_machine" title="Support vector machine">support vector machines</a> (SVM) insofar as the GDA method provides a mapping of the input vectors into high-dimensional feature space.<sup class="reference" id="cite_ref-gda_16-0"><a href="#cite_note-gda-16">[16]</a></sup><sup class="reference" id="cite_ref-cloudid_17-0"><a href="#cite_note-cloudid-17">[17]</a></sup> Similar to LDA, the objective of GDA is to find a projection for the features into a lower dimensional space by maximizing the ratio of between-class scatter to within-class scatter.
</p>
<h3><span class="mw-headline" id="Autoencoder">Autoencoder</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=9" title="Edit section: Autoencoder">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="hatnote navigation-not-searchable" role="note">Main article: <a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></div>
<p>Autoencoders can be used to learn non-linear dimension reduction functions and codings together with an inverse function from the coding to the original representation.
</p>
<h3><span class="mw-headline" id="t-SNE">t-SNE</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=10" title="Edit section: t-SNE">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="hatnote navigation-not-searchable" role="note">Main article: <a class="mw-redirect" href="/wiki/TSNE" title="TSNE">tSNE</a></div>
<p>T-distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear dimensionality reduction technique useful for visualization of high-dimensional datasets.
</p>
<h3><span class="mw-headline" id="UMAP">UMAP</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=11" title="Edit section: UMAP">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="hatnote navigation-not-searchable" role="note">Main article: <a class="mw-redirect" href="/wiki/Uniform_Manifold_Approximation_and_Projection" title="Uniform Manifold Approximation and Projection">Uniform Manifold Approximation and Projection</a></div>
<p><a class="mw-redirect" href="/wiki/Uniform_manifold_approximation_and_projection" title="Uniform manifold approximation and projection">Uniform manifold approximation and projection</a> (UMAP) is a nonlinear dimensionality reduction technique. Visually, it is similar to t-SNE, but it assumes that the data is uniformly distributed on a <a class="mw-redirect" href="/wiki/Locally_connected" title="Locally connected">locally connected</a> <a href="/wiki/Riemannian_manifold" title="Riemannian manifold">Riemannian manifold</a> and that the <a class="mw-redirect" href="/wiki/Riemannian_metric" title="Riemannian metric">Riemannian metric</a> is locally constant or approximately locally constant.
</p>
<h2><span class="mw-headline" id="Dimension_reduction">Dimension reduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=12" title="Edit section: Dimension reduction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>For high-dimensional datasets (i.e. with number of dimensions more than 10), dimension reduction is usually performed prior to applying a <a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm">K-nearest neighbors algorithm</a> (k-NN) in order to avoid the effects of the <a href="/wiki/Curse_of_dimensionality" title="Curse of dimensionality">curse of dimensionality</a>.<sup class="reference" id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup>
</p><p><a href="/wiki/Feature_extraction" title="Feature extraction">Feature extraction</a> and  dimension reduction can be combined in one step using <a href="/wiki/Principal_component_analysis" title="Principal component analysis">principal component analysis</a> (PCA),  <a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">linear discriminant analysis</a> (LDA), <a class="mw-redirect" href="/wiki/Canonical_correlation_analysis" title="Canonical correlation analysis">canonical correlation analysis</a> (CCA), or <a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">non-negative matrix factorization</a> (NMF) techniques as a pre-processing step followed by clustering by K-NN on <a href="/wiki/Feature_(machine_learning)" title="Feature (machine learning)">feature vectors</a> in reduced-dimension space. In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> this process is also called low-dimensional <a href="/wiki/Embedding" title="Embedding">embedding</a>.<sup class="reference" id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup>
</p><p>For very-high-dimensional datasets (e.g. when performing similarity search on live video streams, DNA data or high-dimensional <a href="/wiki/Time_series" title="Time series">time series</a>) running a fast <b>approximate</b> K-NN search using <a class="mw-redirect" href="/wiki/Locality_sensitive_hashing" title="Locality sensitive hashing">locality sensitive hashing</a>, <a href="/wiki/Random_projection" title="Random projection">random projection</a>,<sup class="reference" id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup> "sketches" <sup class="reference" id="cite_ref-21"><a href="#cite_note-21">[21]</a></sup> or other high-dimensional similarity search  techniques from the <a class="mw-redirect" href="/wiki/VLDB_conference" title="VLDB conference">VLDB</a> toolbox might be the only feasible option.
</p>
<h2><span class="mw-headline" id="Advantages_of_dimensionality_reduction">Advantages of dimensionality reduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=13" title="Edit section: Advantages of dimensionality reduction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<table class="box-Unreferenced_section plainlinks metadata ambox ambox-content ambox-Unreferenced" role="presentation"><tbody><tr><td class="mbox-image"><div style="width:52px"><a class="image" href="/wiki/File:Question_book-new.svg"><img alt="" data-file-height="399" data-file-width="512" decoding="async" height="39" src="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/75px-Question_book-new.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/100px-Question_book-new.svg.png 2x" width="50"/></a></div></td><td class="mbox-text"><div class="mbox-text-span">This section <b>does not <a href="/wiki/Wikipedia:Citing_sources" title="Wikipedia:Citing sources">cite</a> any <a href="/wiki/Wikipedia:Verifiability" title="Wikipedia:Verifiability">sources</a></b>.<span class="hide-when-compact"> Please help <a class="external text" href="https://en.wikipedia.org/w/index.php?title=Dimensionality_reduction&amp;action=edit">improve this section</a> by <a href="/wiki/Help:Introduction_to_referencing_with_Wiki_Markup/1" title="Help:Introduction to referencing with Wiki Markup/1">adding citations to reliable sources</a>. Unsourced material may be challenged and <a href="/wiki/Wikipedia:Verifiability#Burden_of_evidence" title="Wikipedia:Verifiability">removed</a>.<br/><small><span class="plainlinks"><i>Find sources:</i> <a class="external text" href="//www.google.com/search?as_eq=wikipedia&amp;q=%22Dimensionality+reduction%22" rel="nofollow">"Dimensionality reduction"</a> – <a class="external text" href="//www.google.com/search?tbm=nws&amp;q=%22Dimensionality+reduction%22+-wikipedia" rel="nofollow">news</a> <b>·</b> <a class="external text" href="//www.google.com/search?&amp;q=%22Dimensionality+reduction%22+site:news.google.com/newspapers&amp;source=newspapers" rel="nofollow">newspapers</a> <b>·</b> <a class="external text" href="//www.google.com/search?tbs=bks:1&amp;q=%22Dimensionality+reduction%22+-wikipedia" rel="nofollow">books</a> <b>·</b> <a class="external text" href="//scholar.google.com/scholar?q=%22Dimensionality+reduction%22" rel="nofollow">scholar</a> <b>·</b> <a class="external text" href="https://www.jstor.org/action/doBasicSearch?Query=%22Dimensionality+reduction%22&amp;acc=on&amp;wc=on" rel="nofollow">JSTOR</a></span></small></span> <small class="date-container"><i>(<span class="date">June 2017</span>)</i></small><small class="hide-when-compact"><i> (<a href="/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>
<ol><li>It reduces the time and storage space required.</li>
<li>Removal of multi-collinearity improves the interpretation of the parameters of the machine learning model.</li>
<li>It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D.</li>
<li>It avoids the curse of dimensionality.</li></ol>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=14" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A dimensionality reduction technique that is sometimes used in <a href="/wiki/Neuroscience" title="Neuroscience">neuroscience</a> is <a href="/wiki/Maximally_informative_dimensions" title="Maximally informative dimensions">maximally informative dimensions</a>,<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (June 2017)">citation needed</span></a></i>]</sup> which finds a lower-dimensional representation of a dataset such that as much <a href="/wiki/Mutual_information" title="Mutual information">information</a> as possible about the original data is preserved.
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=15" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:18.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="/wiki/Recommender_system" title="Recommender system">Recommender systems</a></th></tr><tr><th style="padding:0.1em;border-top:1px solid #aaa;">
Concepts</th></tr><tr><td class="hlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/Collective_intelligence" title="Collective intelligence">Collective intelligence</a></li>
<li><a href="/wiki/Relevance" title="Relevance">Relevance</a></li>
<li><a href="/wiki/Star_(classification)" title="Star (classification)">Star ratings</a></li>
<li><a href="/wiki/Long_tail" title="Long tail">Long tail</a></li></ul></td>
</tr><tr><th style="padding:0.1em;border-top:1px solid #aaa;">
Methods and challenges</th></tr><tr><td class="hlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/Cold_start_(recommender_systems)" title="Cold start (recommender systems)">Cold start</a></li>
<li><a href="/wiki/Collaborative_filtering" title="Collaborative filtering">Collaborative filtering</a></li>
<li><a class="mw-selflink selflink">Dimensionality reduction</a></li>
<li><a href="/wiki/Implicit_data_collection" title="Implicit data collection">Implicit data collection</a></li>
<li><a href="/wiki/Item-item_collaborative_filtering" title="Item-item collaborative filtering">Item-item collaborative filtering</a></li>
<li><a href="/wiki/Matrix_factorization_(recommender_systems)" title="Matrix factorization (recommender systems)">Matrix factorization</a></li>
<li><a href="/wiki/Preference_elicitation" title="Preference elicitation">Preference elicitation</a></li>
<li><a href="/wiki/Similarity_search" title="Similarity search">Similarity search</a></li></ul></td>
</tr><tr><th style="padding:0.1em;border-top:1px solid #aaa;">
Implementations</th></tr><tr><td class="hlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/Collaborative_search_engine" title="Collaborative search engine">Collaborative search engine</a></li>
<li><a href="/wiki/Content_discovery_platform" title="Content discovery platform">Content discovery platform</a></li>
<li><a href="/wiki/Decision_support_system" title="Decision support system">Decision support system</a></li>
<li><a href="/wiki/Music_Genome_Project" title="Music Genome Project">Music Genome Project</a></li>
<li><a href="/wiki/Product_finder" title="Product finder">Product finder</a></li></ul></td>
</tr><tr><th style="padding:0.1em;border-top:1px solid #aaa;">
Research</th></tr><tr><td class="hlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/GroupLens_Research" title="GroupLens Research">GroupLens Research</a></li>
<li><a href="/wiki/MovieLens" title="MovieLens">MovieLens</a></li>
<li><a href="/wiki/Netflix_Prize" title="Netflix Prize">Netflix Prize</a></li></ul></td>
</tr><tr><td style="text-align:right;font-size:115%"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Recommender_systems" title="Template:Recommender systems"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Recommender_systems" title="Template talk:Recommender systems"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Recommender_systems&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<div class="div-col columns column-width" style="-moz-column-width: ; -webkit-column-width: ; column-width: ;">
<ul><li><a href="/wiki/Nearest_neighbor_search" title="Nearest neighbor search">Nearest neighbor search</a></li>
<li><a href="/wiki/MinHash" title="MinHash">MinHash</a></li>
<li><a href="/wiki/Information_gain_in_decision_trees" title="Information gain in decision trees">Information gain in decision trees</a></li>
<li><a href="/wiki/Semidefinite_embedding" title="Semidefinite embedding">Semidefinite embedding</a></li>
<li><a href="/wiki/Multifactor_dimensionality_reduction" title="Multifactor dimensionality reduction">Multifactor dimensionality reduction</a></li>
<li><a href="/wiki/Multilinear_subspace_learning" title="Multilinear subspace learning">Multilinear subspace learning</a></li>
<li><a class="mw-redirect" href="/wiki/Multilinear_PCA" title="Multilinear PCA">Multilinear PCA</a></li>
<li><a href="/wiki/Random_projection" title="Random projection">Random projection</a></li>
<li><a href="/wiki/Singular_value_decomposition" title="Singular value decomposition">Singular value decomposition</a></li>
<li><a href="/wiki/Latent_semantic_analysis" title="Latent semantic analysis">Latent semantic analysis</a></li>
<li><a href="/wiki/Semantic_mapping_(statistics)" title="Semantic mapping (statistics)">Semantic mapping</a></li>
<li><a href="/wiki/Topological_data_analysis" title="Topological data analysis">Topological data analysis</a></li>
<li><a class="mw-redirect" href="/wiki/Locality_sensitive_hashing" title="Locality sensitive hashing">Locality sensitive hashing</a></li>
<li><a href="/wiki/Sufficient_dimension_reduction" title="Sufficient dimension reduction">Sufficient dimension reduction</a></li>
<li><a href="/wiki/Data_transformation_(statistics)" title="Data transformation (statistics)">Data transformation (statistics)</a></li>
<li><a href="/wiki/Weighted_correlation_network_analysis" title="Weighted correlation network analysis">Weighted correlation network analysis</a></li>
<li><a href="/wiki/Hyperparameter_optimization" title="Hyperparameter optimization">Hyperparameter optimization</a></li>
<li><a href="/wiki/CUR_matrix_approximation" title="CUR matrix approximation">CUR matrix approximation</a></li>
<li>Envelope model</li>
<li><a href="/wiki/Nonlinear_dimensionality_reduction" title="Nonlinear dimensionality reduction">Nonlinear dimensionality reduction</a></li>
<li><a href="/wiki/Sammon_mapping" title="Sammon mapping">Sammon mapping</a></li>
<li><a href="/wiki/Johnson%E2%80%93Lindenstrauss_lemma" title="Johnson–Lindenstrauss lemma">Johnson–Lindenstrauss lemma</a></li>
<li><a href="/wiki/Local_tangent_space_alignment" title="Local tangent space alignment">Local tangent space alignment</a></li></ul></div>
<h2><span class="mw-headline" id="Notes">Notes</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=16" title="Edit section: Notes">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist" style="list-style-type: decimal;">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><cite class="citation journal">Roweis, S. T.; Saul, L. K. (2000). "Nonlinear Dimensionality Reduction by Locally Linear Embedding". <i>Science</i>. <b>290</b> (5500): 2323–2326. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2000Sci...290.2323R" rel="nofollow">2000Sci...290.2323R</a>. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.111.3313" rel="nofollow">10.1.1.111.3313</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1126%2Fscience.290.5500.2323" rel="nofollow">10.1126/science.290.5500.2323</a>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/11125150" rel="nofollow">11125150</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Nonlinear+Dimensionality+Reduction+by+Locally+Linear+Embedding&amp;rft.volume=290&amp;rft.issue=5500&amp;rft.pages=2323-2326&amp;rft.date=2000&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.111.3313&amp;rft_id=info%3Apmid%2F11125150&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.290.5500.2323&amp;rft_id=info%3Abibcode%2F2000Sci...290.2323R&amp;rft.aulast=Roweis&amp;rft.aufirst=S.+T.&amp;rft.au=Saul%2C+L.+K.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><style data-mw-deduplicate="TemplateStyles:r935243608">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite class="citation book">Pudil, P.; Novovičová, J. (1998). "Novel Methods for Feature Subset Selection with Respect to Problem Knowledge".  In Liu, Huan; Motoda, Hiroshi (eds.). <i>Feature Extraction, Construction and Selection</i>. p. 101. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1007%2F978-1-4615-5725-8_7" rel="nofollow">10.1007/978-1-4615-5725-8_7</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-1-4613-7622-4" title="Special:BookSources/978-1-4613-7622-4"><bdi>978-1-4613-7622-4</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Novel+Methods+for+Feature+Subset+Selection+with+Respect+to+Problem+Knowledge&amp;rft.btitle=Feature+Extraction%2C+Construction+and+Selection&amp;rft.pages=101&amp;rft.date=1998&amp;rft_id=info%3Adoi%2F10.1007%2F978-1-4615-5725-8_7&amp;rft.isbn=978-1-4613-7622-4&amp;rft.aulast=Pudil&amp;rft.aufirst=P.&amp;rft.au=Novovi%C4%8Dov%C3%A1%2C+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><cite class="citation journal">Rico-Sulayes, Antonio (2017). <a class="external text" href="http://rielac.cujae.edu.cu/index.php/rieac/article/download/478/278" rel="nofollow">"Reducing Vector Space Dimensionality in Automatic Classification for Authorship Attribution"</a>. <i>Revista Ingeniería Electrónica, Automática y Comunicaciones</i>. <b>38</b> (3): 26–35.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Revista+Ingenier%C3%ADa+Electr%C3%B3nica%2C+Autom%C3%A1tica+y+Comunicaciones&amp;rft.atitle=Reducing+Vector+Space+Dimensionality+in+Automatic+Classification+for+Authorship+Attribution&amp;rft.volume=38&amp;rft.issue=3&amp;rft.pages=26-35&amp;rft.date=2017&amp;rft.aulast=Rico-Sulayes&amp;rft.aufirst=Antonio&amp;rft_id=http%3A%2F%2Frielac.cujae.edu.cu%2Findex.php%2Frieac%2Farticle%2Fdownload%2F478%2F278&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text">Samet, H. (2006) <i>Foundations of Multidimensional and Metric Data Structures</i>. Morgan Kaufmann. <link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/><a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/0-12-369446-9" title="Special:BookSources/0-12-369446-9">0-12-369446-9</a></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text">C. Ding, X. He, H. Zha, H.D. Simon, <a class="external text" href="https://cloudfront.escholarship.org/dist/prd/content/qt8pv153t1/qt8pv153t1.pdf" rel="nofollow">Adaptive Dimension Reduction for Clustering High Dimensional Data</a>, Proceedings of International Conference on Data Mining, 2002</span>
</li>
<li id="cite_note-MSLsurvey-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-MSLsurvey_6-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Lu, Haiping; Plataniotis, K.N.; Venetsanopoulos, A.N. (2011). <a class="external text" href="http://www.dsp.utoronto.ca/~haiping/Publication/SurveyMSL_PR2011.pdf" rel="nofollow">"A Survey of Multilinear Subspace Learning for Tensor Data"</a> <span class="cs1-format">(PDF)</span>. <i>Pattern Recognition</i>. <b>44</b> (7): 1540–1551. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1016%2Fj.patcog.2011.01.004" rel="nofollow">10.1016/j.patcog.2011.01.004</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Pattern+Recognition&amp;rft.atitle=A+Survey+of+Multilinear+Subspace+Learning+for+Tensor+Data&amp;rft.volume=44&amp;rft.issue=7&amp;rft.pages=1540-1551&amp;rft.date=2011&amp;rft_id=info%3Adoi%2F10.1016%2Fj.patcog.2011.01.004&amp;rft.aulast=Lu&amp;rft.aufirst=Haiping&amp;rft.au=Plataniotis%2C+K.N.&amp;rft.au=Venetsanopoulos%2C+A.N.&amp;rft_id=http%3A%2F%2Fwww.dsp.utoronto.ca%2F~haiping%2FPublication%2FSurveyMSL_PR2011.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-lee-seung-7"><span class="mw-cite-backlink">^ <a href="#cite_ref-lee-seung_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-lee-seung_7-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Daniel D. Lee &amp; <a href="/wiki/Sebastian_Seung" title="Sebastian Seung">H. Sebastian Seung</a> (1999). "Learning the parts of objects by non-negative matrix factorization". <i><a href="/wiki/Nature_(journal)" title="Nature (journal)">Nature</a></i>. <b>401</b> (6755): 788–791. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/1999Natur.401..788L" rel="nofollow">1999Natur.401..788L</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1038%2F44565" rel="nofollow">10.1038/44565</a>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/10548103" rel="nofollow">10548103</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Learning+the+parts+of+objects+by+non-negative+matrix+factorization&amp;rft.volume=401&amp;rft.issue=6755&amp;rft.pages=788-791&amp;rft.date=1999&amp;rft_id=info%3Apmid%2F10548103&amp;rft_id=info%3Adoi%2F10.1038%2F44565&amp;rft_id=info%3Abibcode%2F1999Natur.401..788L&amp;rft.au=Daniel+D.+Lee&amp;rft.au=H.+Sebastian+Seung&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-lee2001algorithms-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-lee2001algorithms_8-0">^</a></b></span> <span class="reference-text"><cite class="citation conference">Daniel D. Lee &amp; H. Sebastian Seung (2001). <a class="external text" href="http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf" rel="nofollow"><i>Algorithms for Non-negative Matrix Factorization</i></a> <span class="cs1-format">(PDF)</span>. Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference. <a href="/wiki/MIT_Press" title="MIT Press">MIT Press</a>. pp. 556–562.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Algorithms+for+Non-negative+Matrix+Factorization&amp;rft.pages=556-562&amp;rft.pub=MIT+Press&amp;rft.date=2001&amp;rft.au=Daniel+D.+Lee&amp;rft.au=H.+Sebastian+Seung&amp;rft_id=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F1861-algorithms-for-non-negative-matrix-factorization.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-blantonRoweis07-9"><span class="mw-cite-backlink">^ <a href="#cite_ref-blantonRoweis07_9-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-blantonRoweis07_9-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Blanton, Michael R.; Roweis, Sam (2007). "K-corrections and filter transformations in the ultraviolet, optical, and near infrared". <i>The Astronomical Journal</i>. <b>133</b> (2): 734–754. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/astro-ph/0606170" rel="nofollow">astro-ph/0606170</a></span>. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2007AJ....133..734B" rel="nofollow">2007AJ....133..734B</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1086%2F510127" rel="nofollow">10.1086/510127</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Astronomical+Journal&amp;rft.atitle=K-corrections+and+filter+transformations+in+the+ultraviolet%2C+optical%2C+and+near+infrared&amp;rft.volume=133&amp;rft.issue=2&amp;rft.pages=734-754&amp;rft.date=2007&amp;rft_id=info%3Aarxiv%2Fastro-ph%2F0606170&amp;rft_id=info%3Adoi%2F10.1086%2F510127&amp;rft_id=info%3Abibcode%2F2007AJ....133..734B&amp;rft.aulast=Blanton&amp;rft.aufirst=Michael+R.&amp;rft.au=Roweis%2C+Sam&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-ren18-10"><span class="mw-cite-backlink">^ <a href="#cite_ref-ren18_10-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ren18_10-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-ren18_10-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-ren18_10-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Ren, Bin; Pueyo, Laurent; Zhu, Guangtun B.; Duchêne, Gaspard (2018). "Non-negative Matrix Factorization: Robust Extraction of Extended Structures". <i>The Astrophysical Journal</i>. <b>852</b> (2): 104. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1712.10317" rel="nofollow">1712.10317</a></span>. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2018ApJ...852..104R" rel="nofollow">2018ApJ...852..104R</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.3847%2F1538-4357%2Faaa1f2" rel="nofollow">10.3847/1538-4357/aaa1f2</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Astrophysical+Journal&amp;rft.atitle=Non-negative+Matrix+Factorization%3A+Robust+Extraction+of+Extended+Structures&amp;rft.volume=852&amp;rft.issue=2&amp;rft.pages=104&amp;rft.date=2018&amp;rft_id=info%3Aarxiv%2F1712.10317&amp;rft_id=info%3Adoi%2F10.3847%2F1538-4357%2Faaa1f2&amp;rft_id=info%3Abibcode%2F2018ApJ...852..104R&amp;rft.aulast=Ren&amp;rft.aufirst=Bin&amp;rft.au=Pueyo%2C+Laurent&amp;rft.au=Zhu%2C+Guangtun+B.&amp;rft.au=Duch%C3%AAne%2C+Gaspard&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-zhu16-11"><span class="mw-cite-backlink">^ <a href="#cite_ref-zhu16_11-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-zhu16_11-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-zhu16_11-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"> <cite class="citation arxiv">Zhu, Guangtun B. (2016-12-19). "Nonnegative Matrix Factorization (NMF) with Heteroscedastic Uncertainties and Missing data". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1612.06037" rel="nofollow">1612.06037</a></span> [<a class="external text" href="//arxiv.org/archive/astro-ph.IM" rel="nofollow">astro-ph.IM</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Nonnegative+Matrix+Factorization+%28NMF%29+with+Heteroscedastic+Uncertainties+and+Missing+data&amp;rft.date=2016-12-19&amp;rft_id=info%3Aarxiv%2F1612.06037&amp;rft.aulast=Zhu&amp;rft.aufirst=Guangtun+B.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-ren20”-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-ren20”_12-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Ren, Bin; Pueyo, Laurent; Chen, Christine; Choquet, Elodie; Debes, John H.; Duechene, Gaspard; Menard, Francois; Perrin, Marshall D. (2020). "Using Data Imputation for Signal Separation in High Contrast Imaging". <i>The Astrophysical Journal</i>. <b>892</b> (2): 74. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/2001.00563" rel="nofollow">2001.00563</a></span>. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2020ApJ...892...74R" rel="nofollow">2020ApJ...892...74R</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.3847%2F1538-4357%2Fab7024" rel="nofollow">10.3847/1538-4357/ab7024</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Astrophysical+Journal&amp;rft.atitle=Using+Data+Imputation+for+Signal+Separation+in+High+Contrast+Imaging&amp;rft.volume=892&amp;rft.issue=2&amp;rft.pages=74&amp;rft.date=2020&amp;rft_id=info%3Aarxiv%2F2001.00563&amp;rft_id=info%3Adoi%2F10.3847%2F1538-4357%2Fab7024&amp;rft_id=info%3Abibcode%2F2020ApJ...892...74R&amp;rft.aulast=Ren&amp;rft.aufirst=Bin&amp;rft.au=Pueyo%2C+Laurent&amp;rft.au=Chen%2C+Christine&amp;rft.au=Choquet%2C+Elodie&amp;rft.au=Debes%2C+John+H.&amp;rft.au=Duechene%2C+Gaspard&amp;rft.au=Menard%2C+Francois&amp;rft.au=Perrin%2C+Marshall+D.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><cite class="citation journal">Zhang, Zhenyue; Zha, Hongyuan (2004). "Principal Manifolds and Nonlinear Dimensionality Reduction via Tangent Space Alignment". <i>SIAM Journal on Scientific Computing</i>. <b>26</b> (1): 313–338. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1137%2Fs1064827502419154" rel="nofollow">10.1137/s1064827502419154</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=SIAM+Journal+on+Scientific+Computing&amp;rft.atitle=Principal+Manifolds+and+Nonlinear+Dimensionality+Reduction+via+Tangent+Space+Alignment&amp;rft.volume=26&amp;rft.issue=1&amp;rft.pages=313-338&amp;rft.date=2004&amp;rft_id=info%3Adoi%2F10.1137%2Fs1064827502419154&amp;rft.aulast=Zhang&amp;rft.aufirst=Zhenyue&amp;rft.au=Zha%2C+Hongyuan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><cite class="citation journal">Bengio, Yoshua; Monperrus, Martin; Larochelle, Hugo (2006). <a class="external text" href="https://hal.archives-ouvertes.fr/hal-01575345/document" rel="nofollow">"Nonlocal Estimation of Manifold Structure"</a>. <i>Neural Computation</i>. <b>18</b> (10): 2509–2528. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.116.4230" rel="nofollow">10.1.1.116.4230</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1162%2Fneco.2006.18.10.2509" rel="nofollow">10.1162/neco.2006.18.10.2509</a>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/16907635" rel="nofollow">16907635</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=Nonlocal+Estimation+of+Manifold+Structure&amp;rft.volume=18&amp;rft.issue=10&amp;rft.pages=2509-2528&amp;rft.date=2006&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.116.4230&amp;rft_id=info%3Apmid%2F16907635&amp;rft_id=info%3Adoi%2F10.1162%2Fneco.2006.18.10.2509&amp;rft.aulast=Bengio&amp;rft.aufirst=Yoshua&amp;rft.au=Monperrus%2C+Martin&amp;rft.au=Larochelle%2C+Hugo&amp;rft_id=https%3A%2F%2Fhal.archives-ouvertes.fr%2Fhal-01575345%2Fdocument&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text">Hongbing Hu, Stephen A. Zahorian, (2010) <a class="external text" href="http://bingweb.binghamton.edu/~hhu1/paper/Hu2010Dimensionality.pdf" rel="nofollow">"Dimensionality Reduction Methods for HMM Phonetic Recognition,"</a> ICASSP 2010, Dallas, TX</span>
</li>
<li id="cite_note-gda-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-gda_16-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Baudat, G.; Anouar, F. (2000). "Generalized Discriminant Analysis Using a Kernel Approach". <i>Neural Computation</i>. <b>12</b> (10): 2385–2404. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.412.760" rel="nofollow">10.1.1.412.760</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1162%2F089976600300014980" rel="nofollow">10.1162/089976600300014980</a>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/11032039" rel="nofollow">11032039</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=Generalized+Discriminant+Analysis+Using+a+Kernel+Approach&amp;rft.volume=12&amp;rft.issue=10&amp;rft.pages=2385-2404&amp;rft.date=2000&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.412.760&amp;rft_id=info%3Apmid%2F11032039&amp;rft_id=info%3Adoi%2F10.1162%2F089976600300014980&amp;rft.aulast=Baudat&amp;rft.aufirst=G.&amp;rft.au=Anouar%2C+F.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-cloudid-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-cloudid_17-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Haghighat, Mohammad; Zonouz, Saman; Abdel-Mottaleb, Mohamed (2015). "CloudID: Trustworthy cloud-based and cross-enterprise biometric identification". <i>Expert Systems with Applications</i>. <b>42</b> (21): 7905–7916. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1016%2Fj.eswa.2015.06.025" rel="nofollow">10.1016/j.eswa.2015.06.025</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Expert+Systems+with+Applications&amp;rft.atitle=CloudID%3A+Trustworthy+cloud-based+and+cross-enterprise+biometric+identification&amp;rft.volume=42&amp;rft.issue=21&amp;rft.pages=7905-7916&amp;rft.date=2015&amp;rft_id=info%3Adoi%2F10.1016%2Fj.eswa.2015.06.025&amp;rft.aulast=Haghighat&amp;rft.aufirst=Mohammad&amp;rft.au=Zonouz%2C+Saman&amp;rft.au=Abdel-Mottaleb%2C+Mohamed&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text">Kevin Beyer, Jonathan Goldstein, Raghu Ramakrishnan, Uri Shaft (1999) <a class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.31.1422" rel="nofollow">"When is “nearest neighbor” meaningful?"</a>. <i>Database Theory—ICDT99</i>,  217–235</span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><cite class="citation book">Shaw, B.; Jebara, T. (2009). <a class="external text" href="https://www.cs.columbia.edu/~jebara/papers/spe-icml09.pdf" rel="nofollow">"Structure preserving embedding"</a> <span class="cs1-format">(PDF)</span>. <i>Proceedings of the 26th Annual International Conference on Machine Learning – ICML '09</i>. p. 1. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.161.451" rel="nofollow">10.1.1.161.451</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1145%2F1553374.1553494" rel="nofollow">10.1145/1553374.1553494</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/9781605585161" title="Special:BookSources/9781605585161"><bdi>9781605585161</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Structure+preserving+embedding&amp;rft.btitle=Proceedings+of+the+26th+Annual+International+Conference+on+Machine+Learning+%E2%80%93+ICML+%2709&amp;rft.pages=1&amp;rft.date=2009&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.161.451&amp;rft_id=info%3Adoi%2F10.1145%2F1553374.1553494&amp;rft.isbn=9781605585161&amp;rft.aulast=Shaw&amp;rft.aufirst=B.&amp;rft.au=Jebara%2C+T.&amp;rft_id=https%3A%2F%2Fwww.cs.columbia.edu%2F~jebara%2Fpapers%2Fspe-icml09.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><cite class="citation book">Bingham, E.; Mannila, H. (2001). "Random projection in dimensionality reduction". <i>Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining – KDD '01</i>. p. 245. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1145%2F502512.502546" rel="nofollow">10.1145/502512.502546</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-1581133912" title="Special:BookSources/978-1581133912"><bdi>978-1581133912</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Random+projection+in+dimensionality+reduction&amp;rft.btitle=Proceedings+of+the+seventh+ACM+SIGKDD+international+conference+on+Knowledge+discovery+and+data+mining+%E2%80%93+KDD+%2701&amp;rft.pages=245&amp;rft.date=2001&amp;rft_id=info%3Adoi%2F10.1145%2F502512.502546&amp;rft.isbn=978-1581133912&amp;rft.aulast=Bingham&amp;rft.aufirst=E.&amp;rft.au=Mannila%2C+H.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text">Shasha, D High (2004) <i>Performance Discovery in Time Series</i> Berlin: Springer. <link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/><a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/0-387-00857-8" title="Special:BookSources/0-387-00857-8">0-387-00857-8</a></span>
</li>
</ol></div></div>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=17" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<style data-mw-deduplicate="TemplateStyles:r886047268">.mw-parser-output .refbegin{font-size:90%;margin-bottom:0.5em}.mw-parser-output .refbegin-hanging-indents>ul{list-style-type:none;margin-left:0}.mw-parser-output .refbegin-hanging-indents>ul>li,.mw-parser-output .refbegin-hanging-indents>dl>dd{margin-left:0;padding-left:3.2em;text-indent:-3.2em;list-style:none}.mw-parser-output .refbegin-100{font-size:100%}</style><div class="refbegin reflist" style="">
<ul><li><cite class="citation book">Boehmke, Brad; Greenwell, Brandon M. (2019). <a class="external text" href="https://books.google.com/books?id=aXC9DwAAQBAJ&amp;pg=PA343" rel="nofollow">"Dimension Reduction"</a>. <i>Hands-On Machine Learning with R</i>. Chapman &amp; Hall. pp. 343–396. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-1-138-49568-5" title="Special:BookSources/978-1-138-49568-5"><bdi>978-1-138-49568-5</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Dimension+Reduction&amp;rft.btitle=Hands-On+Machine+Learning+with+R&amp;rft.pages=343-396&amp;rft.pub=Chapman+%26+Hall&amp;rft.date=2019&amp;rft.isbn=978-1-138-49568-5&amp;rft.aulast=Boehmke&amp;rft.aufirst=Brad&amp;rft.au=Greenwell%2C+Brandon+M.&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DaXC9DwAAQBAJ%26pg%3DPA343&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></li>
<li><cite class="citation techreport">Fodor, I. (2002). <a class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/versions?doi=10.1.1.8.5098" rel="nofollow"><i>A survey of dimension reduction techniques</i></a> (Technical report). Center for Applied Scientific Computing, Lawrence Livermore National. UCRL-ID-148494.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=A+survey+of+dimension+reduction+techniques&amp;rft.pub=Center+for+Applied+Scientific+Computing%2C+Lawrence+Livermore+National&amp;rft.date=2002&amp;rft.aulast=Fodor&amp;rft.aufirst=I.&amp;rft_id=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fversions%3Fdoi%3D10.1.1.8.5098&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></li>
<li><cite class="citation techreport">Cunningham, P. (2007). <a class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.98.1478" rel="nofollow"><i>Dimension Reduction</i></a> (Technical report). University College Dublin. UCD-CSI-2007-7.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=Dimension+Reduction&amp;rft.pub=University+College+Dublin&amp;rft.date=2007&amp;rft.aulast=Cunningham&amp;rft.aufirst=P.&amp;rft_id=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.98.1478&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></li>
<li><cite class="citation book">Lakshmi Padmaja, Dhyaram; Vishnuvardhan, B (2016). "Comparative Study of Feature Subset Selection Methods for Dimensionality Reduction on Scientific Data". <i>2016 IEEE 6th International Conference on Advanced Computing (IACC)</i>. pp. 31–34. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1109%2FIACC.2016.16" rel="nofollow">10.1109/IACC.2016.16</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-1-4673-8286-1" title="Special:BookSources/978-1-4673-8286-1"><bdi>978-1-4673-8286-1</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Comparative+Study+of+Feature+Subset+Selection+Methods+for+Dimensionality+Reduction+on+Scientific+Data&amp;rft.btitle=2016+IEEE+6th+International+Conference+on+Advanced+Computing+%28IACC%29&amp;rft.pages=31-34&amp;rft.date=2016&amp;rft_id=info%3Adoi%2F10.1109%2FIACC.2016.16&amp;rft.isbn=978-1-4673-8286-1&amp;rft.aulast=Lakshmi+Padmaja&amp;rft.aufirst=Dhyaram&amp;rft.au=Vishnuvardhan%2C+B&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ADimensionality+reduction"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></li></ul>
</div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Dimensionality_reduction&amp;action=edit&amp;section=18" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a class="external text" href="http://jmlr.csail.mit.edu/papers/special/feature03.html" rel="nofollow">JMLR Special Issue on Variable and Feature Selection</a></li>
<li><a class="external text" href="http://bioinfo-out.curie.fr/projects/elmap/" rel="nofollow">ELastic MAPs</a></li>
<li><a class="external text" href="http://www.cs.toronto.edu/~roweis/lle" rel="nofollow">Locally Linear Embedding</a></li>
<li><a class="external text" href="https://web.archive.org/web/20040411051530/http://isomap.stanford.edu/" rel="nofollow">A Global Geometric Framework for Nonlinear Dimensionality Reduction</a></li></ul>
<!-- 
NewPP limit report
Parsed by mw1348
Cached time: 20200405090515
Cache expiry: 2592000
Dynamic content: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.592 seconds
Real time usage: 0.811 seconds
Preprocessor visited node count: 2649/1000000
Post‐expand include size: 106497/2097152 bytes
Template argument size: 3756/2097152 bytes
Highest expansion depth: 15/40
Expensive parser function count: 9/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 72887/5000000 bytes
Number of Wikibase entities loaded: 4/400
Lua time usage: 0.296/10.000 seconds
Lua memory usage: 7.84 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  699.676      1 -total
 48.50%  339.374      1 Template:Reflist
 32.74%  229.062     11 Template:Cite_journal
  9.64%   67.459      1 Template:Refimprove
  8.87%   62.090      2 Template:Ambox
  7.73%   54.113      1 Template:Short_description
  6.98%   48.839      1 Template:Machine_learning_bar
  6.48%   45.321      1 Template:Sidebar_with_collapsible_lists
  6.23%   43.612      2 Template:ISBN
  6.18%   43.219      3 Template:Fix
-->
<!-- Saved in parser cache with key enwiki:pcache:idhash:579867-0!canonical and timestamp 20200405090525 and revision id 949231135
 -->
</div><noscript><img alt="" height="1" src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" style="border: none; position: absolute;" title="" width="1"/></noscript></div>
<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Dimensionality_reduction&amp;oldid=949231135">https://en.wikipedia.org/w/index.php?title=Dimensionality_reduction&amp;oldid=949231135</a>"</div>
<div class="catlinks" data-mw="interface" id="catlinks"><div class="mw-normal-catlinks" id="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Dimension_reduction" title="Category:Dimension reduction">Dimension reduction</a></li><li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li></ul></div><div class="mw-hidden-catlinks mw-hidden-cats-hidden" id="mw-hidden-catlinks">Hidden categories: <ul><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:Articles_needing_additional_references_from_November_2010" title="Category:Articles needing additional references from November 2010">Articles needing additional references from November 2010</a></li><li><a href="/wiki/Category:All_articles_needing_additional_references" title="Category:All articles needing additional references">All articles needing additional references</a></li><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_September_2017" title="Category:Articles with unsourced statements from September 2017">Articles with unsourced statements from September 2017</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_September_2017" title="Category:Wikipedia articles needing clarification from September 2017">Wikipedia articles needing clarification from September 2017</a></li><li><a href="/wiki/Category:Articles_needing_additional_references_from_June_2017" title="Category:Articles needing additional references from June 2017">Articles needing additional references from June 2017</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_June_2017" title="Category:Articles with unsourced statements from June 2017">Articles with unsourced statements from June 2017</a></li></ul></div></div>
<div class="visualClear"></div>
</div>
</div>
<div id="mw-data-after-content">
<div class="read-more-container"></div>
</div>
<div id="mw-navigation">
<h2>Navigation menu</h2>
<div id="mw-head">
<div aria-labelledby="p-personal-label" class="" id="p-personal" role="navigation">
<h3 id="p-personal-label">Personal tools</h3>
<ul>
<li id="pt-anonuserpage">Not logged in</li>
<li id="pt-anontalk"><a accesskey="n" href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]">Talk</a></li><li id="pt-anoncontribs"><a accesskey="y" href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Dimensionality+reduction" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a accesskey="o" href="/w/index.php?title=Special:UserLogin&amp;returnto=Dimensionality+reduction" title="You're encouraged to log in; however, it's not mandatory. [o]">Log in</a></li>
</ul>
</div>
<div id="left-navigation">
<div aria-labelledby="p-namespaces-label" class="vectorTabs" id="p-namespaces" role="navigation">
<h3 id="p-namespaces-label">Namespaces</h3>
<ul>
<li class="selected" id="ca-nstab-main"><a accesskey="c" href="/wiki/Dimensionality_reduction" title="View the content page [c]">Article</a></li><li id="ca-talk"><a accesskey="t" href="/wiki/Talk:Dimensionality_reduction" rel="discussion" title="Discussion about the content page [t]">Talk</a></li>
</ul>
</div>
<div aria-labelledby="p-variants-label" class="vectorMenu emptyPortlet" id="p-variants" role="navigation">
<input aria-labelledby="p-variants-label" class="vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-variants-label">
<span>Variants</span>
</h3>
<ul class="menu">
</ul>
</div>
</div>
<div id="right-navigation">
<div aria-labelledby="p-views-label" class="vectorTabs" id="p-views" role="navigation">
<h3 id="p-views-label">Views</h3>
<ul>
<li class="collapsible selected" id="ca-view"><a href="/wiki/Dimensionality_reduction">Read</a></li><li class="collapsible" id="ca-edit"><a accesskey="e" href="/w/index.php?title=Dimensionality_reduction&amp;action=edit" title="Edit this page [e]">Edit</a></li><li class="collapsible" id="ca-history"><a accesskey="h" href="/w/index.php?title=Dimensionality_reduction&amp;action=history" title="Past revisions of this page [h]">View history</a></li>
</ul>
</div>
<div aria-labelledby="p-cactions-label" class="vectorMenu emptyPortlet" id="p-cactions" role="navigation">
<input aria-labelledby="p-cactions-label" class="vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-cactions-label">
<span>More</span>
</h3>
<ul class="menu">
</ul>
</div>
<div id="p-search" role="search">
<h3>
<label for="searchInput">Search</label>
</h3>
<form action="/w/index.php" id="searchform">
<div id="simpleSearch">
<input accesskey="f" id="searchInput" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" type="search"/>
<input name="title" type="hidden" value="Special:Search"/>
<input class="searchButton mw-fallbackSearchButton" id="mw-searchButton" name="fulltext" title="Search Wikipedia for this text" type="submit" value="Search"/>
<input class="searchButton" id="searchButton" name="go" title="Go to a page with this exact name if it exists" type="submit" value="Go"/>
</div>
</form>
</div>
</div>
</div>
<div id="mw-panel">
<div id="p-logo" role="banner">
<a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a>
</div>
<div aria-labelledby="p-navigation-label" class="portal" id="p-navigation" role="navigation">
<h3 id="p-navigation-label">
    			Navigation
    		</h3>
<div class="body">
<ul><li id="n-mainpage-description"><a accesskey="z" href="/wiki/Main_Page" title="Visit the main page [z]">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Wikipedia:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a accesskey="x" href="/wiki/Special:Random" title="Load a random article [x]">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li></ul>
</div>
</div>
<div aria-labelledby="p-interaction-label" class="portal" id="p-interaction" role="navigation">
<h3 id="p-interaction-label">
    			Interaction
    		</h3>
<div class="body">
<ul><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a accesskey="r" href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li></ul>
</div>
</div>
<div aria-labelledby="p-tb-label" class="portal" id="p-tb" role="navigation">
<h3 id="p-tb-label">
    			Tools
    		</h3>
<div class="body">
<ul><li id="t-whatlinkshere"><a accesskey="j" href="/wiki/Special:WhatLinksHere/Dimensionality_reduction" title="List of all English Wikipedia pages containing links to this page [j]">What links here</a></li><li id="t-recentchangeslinked"><a accesskey="k" href="/wiki/Special:RecentChangesLinked/Dimensionality_reduction" rel="nofollow" title="Recent changes in pages linked from this page [k]">Related changes</a></li><li id="t-upload"><a accesskey="u" href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]">Upload file</a></li><li id="t-specialpages"><a accesskey="q" href="/wiki/Special:SpecialPages" title="A list of all special pages [q]">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Dimensionality_reduction&amp;oldid=949231135" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Dimensionality_reduction&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a accesskey="g" href="https://www.wikidata.org/wiki/Special:EntityPage/Q16000077" title="Link to connected data repository item [g]">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Dimensionality_reduction&amp;id=949231135&amp;wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li></ul>
</div>
</div>
<div aria-labelledby="p-coll-print_export-label" class="portal" id="p-coll-print_export" role="navigation">
<h3 id="p-coll-print_export-label">
    			Print/export
    		</h3>
<div class="body">
<ul><li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Dimensionality+reduction">Create a book</a></li><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Dimensionality+reduction&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a accesskey="p" href="/w/index.php?title=Dimensionality_reduction&amp;printable=yes" title="Printable version of this page [p]">Printable version</a></li></ul>
</div>
</div>
<div aria-labelledby="p-lang-label" class="portal" id="p-lang" role="navigation">
<h3 id="p-lang-label">
    			Languages
    		</h3>
<div class="body">
<ul><li class="interlanguage-link interwiki-es"><a class="interlanguage-link-target" href="https://es.wikipedia.org/wiki/Reducci%C3%B3n_de_dimensionalidad" hreflang="es" lang="es" title="Reducción de dimensionalidad – Spanish">Español</a></li><li class="interlanguage-link interwiki-fa"><a class="interlanguage-link-target" href="https://fa.wikipedia.org/wiki/%DA%A9%D8%A7%D9%87%D8%B4_%D8%A7%D8%A8%D8%B9%D8%A7%D8%AF" hreflang="fa" lang="fa" title="کاهش ابعاد – Persian">فارسی</a></li><li class="interlanguage-link interwiki-fr"><a class="interlanguage-link-target" href="https://fr.wikipedia.org/wiki/R%C3%A9duction_de_la_dimensionnalit%C3%A9" hreflang="fr" lang="fr" title="Réduction de la dimensionnalité – French">Français</a></li><li class="interlanguage-link interwiki-he"><a class="interlanguage-link-target" href="https://he.wikipedia.org/wiki/%D7%94%D7%95%D7%A8%D7%93%D7%AA_%D7%9E%D7%9E%D7%93" hreflang="he" lang="he" title="הורדת ממד – Hebrew">עברית</a></li><li class="interlanguage-link interwiki-ru"><a class="interlanguage-link-target" href="https://ru.wikipedia.org/wiki/%D0%A1%D0%BD%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5_%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D0%B8" hreflang="ru" lang="ru" title="Снижение размерности – Russian">Русский</a></li><li class="interlanguage-link interwiki-uk"><a class="interlanguage-link-target" href="https://uk.wikipedia.org/wiki/%D0%97%D0%BD%D0%B8%D0%B6%D0%B5%D0%BD%D0%BD%D1%8F_%D1%80%D0%BE%D0%B7%D0%BC%D1%96%D1%80%D0%BD%D0%BE%D1%81%D1%82%D1%96" hreflang="uk" lang="uk" title="Зниження розмірності – Ukrainian">Українська</a></li><li class="interlanguage-link interwiki-zh"><a class="interlanguage-link-target" href="https://zh.wikipedia.org/wiki/%E9%99%8D%E7%BB%B4" hreflang="zh" lang="zh" title="降维 – Chinese">中文</a></li></ul>
<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a class="wbc-editpage" href="https://www.wikidata.org/wiki/Special:EntityPage/Q16000077#sitelinks-wikipedia" title="Edit interlanguage links">Edit links</a></span></div>
</div>
</div>
</div>
</div>
<div id="footer" role="contentinfo">
<ul class="" id="footer-info">
<li id="footer-info-lastmod"> This page was last edited on 5 April 2020, at 09:05<span class="anonymous-show"> (UTC)</span>.</li>
<li id="footer-info-copyright">Text is available under the <a href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" rel="license">Creative Commons Attribution-ShareAlike License</a><a href="//creativecommons.org/licenses/by-sa/3.0/" rel="license" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>
<ul class="" id="footer-places">
<li id="footer-places-privacy"><a class="extiw" href="https://foundation.wikimedia.org/wiki/Privacy_policy" title="wmf:Privacy policy">Privacy policy</a></li>
<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
<li id="footer-places-mobileview"><a class="noprint stopMobileRedirectToggle" href="//en.m.wikipedia.org/w/index.php?title=Dimensionality_reduction&amp;mobileaction=toggle_view_mobile">Mobile view</a></li>
</ul>
<ul class="noprint" id="footer-icons">
<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img alt="Wikimedia Foundation" height="31" src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88"/></a></li>
<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img alt="Powered by MediaWiki" height="31" src="/static/images/poweredby_mediawiki_88x31.png" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88"/></a></li>
</ul>
<div style="clear: both;"></div>
</div>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.592","walltime":"0.811","ppvisitednodes":{"value":2649,"limit":1000000},"postexpandincludesize":{"value":106497,"limit":2097152},"templateargumentsize":{"value":3756,"limit":2097152},"expansiondepth":{"value":15,"limit":40},"expensivefunctioncount":{"value":9,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":72887,"limit":5000000},"entityaccesscount":{"value":4,"limit":400},"timingprofile":["100.00%  699.676      1 -total"," 48.50%  339.374      1 Template:Reflist"," 32.74%  229.062     11 Template:Cite_journal","  9.64%   67.459      1 Template:Refimprove","  8.87%   62.090      2 Template:Ambox","  7.73%   54.113      1 Template:Short_description","  6.98%   48.839      1 Template:Machine_learning_bar","  6.48%   45.321      1 Template:Sidebar_with_collapsible_lists","  6.23%   43.612      2 Template:ISBN","  6.18%   43.219      3 Template:Fix"]},"scribunto":{"limitreport-timeusage":{"value":"0.296","limit":"10.000"},"limitreport-memusage":{"value":8225206,"limit":52428800}},"cachereport":{"origin":"mw1348","timestamp":"20200405090515","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Dimensionality reduction","url":"https:\/\/en.wikipedia.org\/wiki\/Dimensionality_reduction","sameAs":"http:\/\/www.wikidata.org\/entity\/Q16000077","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q16000077","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2004-04-06T15:10:25Z","dateModified":"2020-04-05T09:05:25Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Kernel_Machine.svg","headline":"process of reducing the number of random variables under consideration"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":88,"wgHostname":"mw1369"});});</script></body></html>
