<!DOCTYPE html>
<html class="client-nojs" dir="ltr" lang="en">
<head>
<meta charset="utf-8"/>
<title>Bias–variance tradeoff - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"Xo-C2gpAEKIAA3AouPcAAAAL","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Bias–variance_tradeoff","wgTitle":"Bias–variance tradeoff","wgCurRevisionId":948382224,"wgRevisionId":948382224,"wgArticleId":40678189,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Dilemmas","Model selection","Machine learning","Statistical classification"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Bias–variance_tradeoff","wgRelevantArticleId":40678189,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,
"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgRedirectedFrom":"Bias–variance_dilemma","wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgInternalRedirectTargetUrl":"/wiki/Bias%E2%80%93variance_tradeoff","wgWikibaseItemId":"Q17003119","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","mediawiki.toc.styles":"ready","skins.vector.styles.legacy":
"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready"};RLPAGEMODULES=["mediawiki.action.view.redirect","ext.cite.ux-enhancements","ext.math.scripts","site","mediawiki.page.startup","skins.vector.js","mediawiki.page.ready","mediawiki.toc","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector" rel="stylesheet"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta content="" name="ResourceLoaderDynamicStyles"/>
<link href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector" rel="stylesheet"/>
<meta content="MediaWiki 1.35.0-wmf.27" name="generator"/>
<meta content="origin" name="referrer"/>
<meta content="origin-when-crossorigin" name="referrer"/>
<meta content="origin-when-cross-origin" name="referrer"/>
<meta content="https://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Test_function_and_noisy_data.png/1200px-Test_function_and_noisy_data.png" property="og:image"/>
<link href="/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;action=edit" rel="alternate" title="Edit this page" type="application/x-wiki"/>
<link href="/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;action=edit" rel="edit" title="Edit this page"/>
<link href="/static/apple-touch/wikipedia.png" rel="apple-touch-icon"/>
<link href="/static/favicon/wikipedia.ico" rel="shortcut icon"/>
<link href="/w/opensearch_desc.php" rel="search" title="Wikipedia (en)" type="application/opensearchdescription+xml"/>
<link href="//en.wikipedia.org/w/api.php?action=rsd" rel="EditURI" type="application/rsd+xml"/>
<link href="//creativecommons.org/licenses/by-sa/3.0/" rel="license"/>
<link href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" rel="alternate" title="Wikipedia Atom feed" type="application/atom+xml"/>
<link href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff" rel="canonical"/>
<link href="//login.wikimedia.org" rel="dns-prefetch"/>
<link href="//meta.wikimedia.org" rel="dns-prefetch"/>
<!--[if lt IE 9]><script src="/w/resources/lib/html5shiv/html5shiv.js"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Bias–variance_tradeoff rootpage-Bias–variance_tradeoff skin-vector action-view">
<div class="noprint" id="mw-page-base"></div>
<div class="noprint" id="mw-head-base"></div>
<div class="mw-body" id="content" role="main">
<a id="top"></a>
<div class="mw-body-content" id="siteNotice"><!-- CentralNotice --></div>
<div class="mw-indicators mw-body-content">
</div>
<h1 class="firstHeading" id="firstHeading" lang="en">Bias–variance tradeoff</h1>
<div class="mw-body-content" id="bodyContent">
<div class="noprint" id="siteSub">From Wikipedia, the free encyclopedia</div>
<div id="contentSub"><span class="mw-redirectedfrom">  (Redirected from <a class="mw-redirect" href="/w/index.php?title=Bias%E2%80%93variance_dilemma&amp;redirect=no" title="Bias–variance dilemma">Bias–variance dilemma</a>)</span></div>
<div id="jump-to-nav"></div>
<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
<a class="mw-jump-link" href="#p-search">Jump to search</a>
<div class="mw-content-ltr" dir="ltr" id="mw-content-text" lang="en"><div class="mw-parser-output"><table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a> and<br/><a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;"><a class="image" href="/wiki/File:Kernel_Machine.svg"><img alt="Kernel Machine.svg" data-file-height="233" data-file-width="512" decoding="async" height="100" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/220px-Kernel_Machine.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x" width="220"/></a></td></tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><div style="padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br/><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b> • <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>
<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>
<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>
<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="/wiki/Support-vector_machine" title="Support-vector machine">Support vector machine (SVM)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a class="mw-redirect" href="/wiki/CURE_data_clustering_algorithm" title="CURE data clustering algorithm">CURE</a></li>
<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li>
<li><br/><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a class="mw-redirect" href="/wiki/Mean-shift" title="Mean-shift">Mean-shift</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>
<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>
<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>
<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a class="mw-redirect" href="/wiki/K-nearest_neighbors_classification" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>
<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li></ul></li>
<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>
<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a class="mw-redirect" href="/wiki/Bias%E2%80%93variance_dilemma" title="Bias–variance dilemma">Bias–variance dilemma</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine-learning venues</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>
<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>
<li><a class="external text" href="https://arxiv.org/list/cs.LG/recent" rel="nofollow">ArXiv:cs.LG</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Related articles</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">
<ul><li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>
<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul>
</div></div></div></td>
</tr><tr><td style="text-align:right;font-size:115%;padding-top: 0.6em;"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<style data-mw-deduplicate="TemplateStyles:r923042769/mw-parser-output/.tmulti">.mw-parser-output .tmulti .thumbinner{display:flex;flex-direction:column}.mw-parser-output .tmulti .trow{display:flex;flex-direction:row;clear:left;flex-wrap:wrap;width:100%;box-sizing:border-box}.mw-parser-output .tmulti .tsingle{margin:1px;float:left}.mw-parser-output .tmulti .theader{clear:both;font-weight:bold;text-align:center;align-self:center;background-color:transparent;width:100%}.mw-parser-output .tmulti .thumbcaption{text-align:left;background-color:transparent}.mw-parser-output .tmulti .thumbcaption-center{text-align:center;background-color:transparent}.mw-parser-output .tmulti .text-align-left{text-align:left}.mw-parser-output .tmulti .text-align-right{text-align:right}.mw-parser-output .tmulti .text-align-center{text-align:center}@media all and (max-width:720px){.mw-parser-output .tmulti .thumbinner{width:100%!important;box-sizing:border-box;max-width:none!important;align-items:center}.mw-parser-output .tmulti .trow{justify-content:center}.mw-parser-output .tmulti .tsingle{float:none!important;max-width:100%!important;box-sizing:border-box;text-align:center}.mw-parser-output .tmulti .thumbcaption{text-align:center}}</style><div class="thumb tmulti tright"><div class="thumbinner" style="width:204px;max-width:204px"><div class="trow"><div class="tsingle" style="width:202px;max-width:202px"><div class="thumbimage"><a class="image" href="/wiki/File:Test_function_and_noisy_data.png"><img alt="" data-file-height="901" data-file-width="1201" decoding="async" height="150" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/64/Test_function_and_noisy_data.png/200px-Test_function_and_noisy_data.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/64/Test_function_and_noisy_data.png/300px-Test_function_and_noisy_data.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/64/Test_function_and_noisy_data.png/400px-Test_function_and_noisy_data.png 2x" width="200"/></a></div><div class="thumbcaption">Function and noisy data.</div></div></div><div class="trow"><div class="tsingle" style="width:202px;max-width:202px"><div class="thumbimage"><a class="image" href="/wiki/File:Radial_basis_function_fit,_spread%3D5.png"><img alt="" data-file-height="901" data-file-width="1201" decoding="async" height="150" src="//upload.wikimedia.org/wikipedia/commons/thumb/9/91/Radial_basis_function_fit%2C_spread%3D5.png/200px-Radial_basis_function_fit%2C_spread%3D5.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/9/91/Radial_basis_function_fit%2C_spread%3D5.png/300px-Radial_basis_function_fit%2C_spread%3D5.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/9/91/Radial_basis_function_fit%2C_spread%3D5.png/400px-Radial_basis_function_fit%2C_spread%3D5.png 2x" width="200"/></a></div><div class="thumbcaption">spread=5</div></div></div><div class="trow"><div class="tsingle" style="width:202px;max-width:202px"><div class="thumbimage"><a class="image" href="/wiki/File:Radial_basis_function_fit,_spread%3D1.png"><img alt="" data-file-height="901" data-file-width="1201" decoding="async" height="150" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/63/Radial_basis_function_fit%2C_spread%3D1.png/200px-Radial_basis_function_fit%2C_spread%3D1.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/63/Radial_basis_function_fit%2C_spread%3D1.png/300px-Radial_basis_function_fit%2C_spread%3D1.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/63/Radial_basis_function_fit%2C_spread%3D1.png/400px-Radial_basis_function_fit%2C_spread%3D1.png 2x" width="200"/></a></div><div class="thumbcaption">spread=1</div></div></div><div class="trow"><div class="tsingle" style="width:202px;max-width:202px"><div class="thumbimage"><a class="image" href="/wiki/File:Radial_basis_function_fit,_spread%3D0.1.png"><img alt="" data-file-height="901" data-file-width="1201" decoding="async" height="150" src="//upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Radial_basis_function_fit%2C_spread%3D0.1.png/200px-Radial_basis_function_fit%2C_spread%3D0.1.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Radial_basis_function_fit%2C_spread%3D0.1.png/300px-Radial_basis_function_fit%2C_spread%3D0.1.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Radial_basis_function_fit%2C_spread%3D0.1.png/400px-Radial_basis_function_fit%2C_spread%3D0.1.png 2x" width="200"/></a></div><div class="thumbcaption">spread=0.1</div></div></div><div class="trow"><div class="thumbcaption" style="background-color:transparent">A function (red) is approximated using <a class="mw-redirect" href="/wiki/Radial_basis_functions" title="Radial basis functions">radial basis functions</a> (blue). Several trials are shown in each graph. For each trial, a few noisy data points are provided as a training set (top). For a wide spread (image 2) the bias is high: the RBFs cannot fully approximate the function (especially the central dip), but the variance between different trials is low. As spread decreases (image 3 and 4) the bias decreases: the blue curves more closely approximate the red. However, depending on the noise in different trials the variance between trials increases. In the lowermost image the approximated values for x=0 varies wildly depending on where the data points were located.</div></div></div></div>
<p>In <a href="/wiki/Statistics" title="Statistics">statistics</a> and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, the <b>bias–variance tradeoff</b> is the property of a set of predictive models whereby models with a lower <a href="/wiki/Bias_of_an_estimator" title="Bias of an estimator">bias</a> in <a href="/wiki/Statistical_parameter" title="Statistical parameter">parameter</a> <a href="/wiki/Estimation_theory" title="Estimation theory">estimation</a> have a higher <a href="/wiki/Variance" title="Variance">variance</a> of the parameter estimates across <a href="/wiki/Sample_(statistics)" title="Sample (statistics)">samples</a>, and vice versa. The <b>bias–variance dilemma</b> or <b>bias–variance problem</b> is the conflict in trying to simultaneously minimize these two sources of <a class="mw-redirect" href="/wiki/Errors_and_residuals_in_statistics" title="Errors and residuals in statistics">error</a> that prevent <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> algorithms from generalizing beyond their <a class="mw-redirect" href="/wiki/Training_set" title="Training set">training set</a><sup class="reference" id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup><sup class="reference" id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup>:
</p>
<ul><li>The <a href="/wiki/Bias_of_an_estimator" title="Bias of an estimator"><i>bias error</i></a> is an error from erroneous assumptions in the learning <a href="/wiki/Algorithm" title="Algorithm">algorithm</a>. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).</li>
<li>The <i><a href="/wiki/Variance" title="Variance">variance</a></i> is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random <a href="/wiki/Noise_(signal_processing)" title="Noise (signal processing)">noise</a> in the training data, rather than the intended outputs (<a href="/wiki/Overfitting" title="Overfitting">overfitting</a>).</li></ul>
<p>The <b>bias–variance decomposition</b> is a way of analyzing a learning algorithm's <a href="/wiki/Expected_value" title="Expected value">expected</a> <a href="/wiki/Generalization_error" title="Generalization error">generalization error</a> with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the <i>irreducible error</i>, resulting from noise in the problem itself.
</p><p>This tradeoff applies to all forms of <a href="/wiki/Supervised_learning" title="Supervised learning">supervised learning</a>: <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a>, <a href="/wiki/Regression_analysis" title="Regression analysis">regression</a> (function fitting),<sup class="reference" id="cite_ref-geman_3-0"><a href="#cite_note-geman-3">[3]</a></sup><sup class="reference" id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup> and <a href="/wiki/Structured_prediction" title="Structured prediction">structured output learning</a>, though, it does not apply in all learning algorithms.<sup class="reference" id="cite_ref-nealThesis2019_5-0"><a href="#cite_note-nealThesis2019-5">[5]</a></sup><sup class="reference" id="cite_ref-nealBlog2020_6-0"><a href="#cite_note-nealBlog2020-6">[6]</a></sup> It has also been invoked to explain the effectiveness of heuristics in human learning.<sup class="reference" id="cite_ref-ReferenceA_7-0"><a href="#cite_note-ReferenceA-7">[7]</a></sup>
</p><p>It is important to note that the bias-variance tradeoff is not universal.<sup class="reference" id="cite_ref-nealThesis2019_5-1"><a href="#cite_note-nealThesis2019-5">[5]</a></sup><sup class="reference" id="cite_ref-nealBlog2020_6-1"><a href="#cite_note-nealBlog2020-6">[6]</a></sup><sup class="reference" id="cite_ref-neal2018_8-0"><a href="#cite_note-neal2018-8">[8]</a></sup> For example, <i>both</i> bias <i>and</i> variance decrease when increasing the width of a neural network.<sup class="reference" id="cite_ref-neal2018_8-1"><a href="#cite_note-neal2018-8">[8]</a></sup> This means that it is not necessary to control the size of a neural network to control variance. This does not contradict the bias-variance decomposition because the bias-variance decomposition does not imply a bias-variance tradeoff.<sup class="reference" id="cite_ref-nealBlog2020_6-2"><a href="#cite_note-nealBlog2020-6">[6]</a></sup><sup class="reference" id="cite_ref-nealThesis2019_5-2"><a href="#cite_note-nealThesis2019-5">[5]</a></sup>
</p>
<div aria-labelledby="mw-toc-heading" class="toc" id="toc" role="navigation"><input class="toctogglecheckbox" id="toctogglecheckbox" role="button" style="display:none" type="checkbox"/><div class="toctitle" dir="ltr" lang="en"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Motivation"><span class="tocnumber">1</span> <span class="toctext">Motivation</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Bias–variance_decomposition_of_squared_error"><span class="tocnumber">2</span> <span class="toctext">Bias–variance decomposition of squared error</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Derivation"><span class="tocnumber">2.1</span> <span class="toctext">Derivation</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-4"><a href="#Application_to_regression"><span class="tocnumber">3</span> <span class="toctext">Application to regression</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#Application_to_classification"><span class="tocnumber">4</span> <span class="toctext">Application to classification</span></a></li>
<li class="toclevel-1 tocsection-6"><a href="#Application_to_reinforcement_learning"><span class="tocnumber">5</span> <span class="toctext">Application to reinforcement learning</span></a></li>
<li class="toclevel-1 tocsection-7"><a href="#Approaches"><span class="tocnumber">6</span> <span class="toctext">Approaches</span></a>
<ul>
<li class="toclevel-2 tocsection-8"><a href="#k-nearest_neighbors"><span class="tocnumber">6.1</span> <span class="toctext"><i>k</i>-nearest neighbors</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-9"><a href="#Application_to_human_learning"><span class="tocnumber">7</span> <span class="toctext">Application to human learning</span></a></li>
<li class="toclevel-1 tocsection-10"><a href="#See_also"><span class="tocnumber">8</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-11"><a href="#References"><span class="tocnumber">9</span> <span class="toctext">References</span></a></li>
</ul>
</div>
<h2><span class="mw-headline" id="Motivation">Motivation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;action=edit&amp;section=1" title="Edit section: Motivation">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The bias-variance tradeoff is a central problem in supervised learning. Ideally, one wants to <a href="/wiki/Model_selection" title="Model selection">choose a model</a> that both accurately captures the regularities in its training data, but also <a href="/wiki/Generalization" title="Generalization">generalizes</a> well to unseen data. Unfortunately, it is typically impossible to do both simultaneously. High-variance learning methods may be able to represent their training set well but are at risk of overfitting to noisy or unrepresentative training data. In contrast, algorithms with high bias typically produce simpler models that don't tend to overfit but may <i>underfit</i> their training data, failing to capture important regularities.
</p><p>Models with high variance are usually more complex (e.g. higher-order regression polynomials), enabling them to represent the training set more accurately. In the process, however, they may also represent a large <a href="/wiki/Noise_(signal_processing)" title="Noise (signal processing)">noise</a> component in the training set, making their predictions less accurate – despite their added complexity. In contrast, models with higher bias tend to be relatively simple (low-order or even linear regression polynomials) but may produce lower variance predictions when applied beyond the training set.
</p>
<h2><span id="Bias.E2.80.93variance_decomposition_of_squared_error"></span><span class="mw-headline" id="Bias–variance_decomposition_of_squared_error">Bias–variance decomposition of squared error</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;action=edit&amp;section=2" title="Edit section: Bias–variance decomposition of squared error">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Suppose that we have a training set consisting of a set of points <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle x_{1},\dots ,x_{n}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle x_{1},\dots ,x_{n}}</annotation>
</semantics>
</math></span><img alt="x_{1},\dots ,x_{n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e5afdbc2d248d8fa9ba2c4f5188d946a0537e753" style="vertical-align: -0.671ex; width:10.11ex; height:2.009ex;"/></span> and real values <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle y_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle y_{i}}</annotation>
</semantics>
</math></span><img alt="y_{i}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/67d30d30b6c2dbe4d6f150d699de040937ecc95f" style="vertical-align: -0.671ex; width:1.939ex; height:2.009ex;"/></span> associated with each point <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle x_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle x_{i}}</annotation>
</semantics>
</math></span><img alt="x_{i}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e87000dd6142b81d041896a30fe58f0c3acb2158" style="vertical-align: -0.671ex; width:2.129ex; height:2.009ex;"/></span>. We assume that there is a function with noise <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle y=f(x)+\varepsilon }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>y</mi>
<mo>=</mo>
<mi>f</mi>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
<mo>+</mo>
<mi>ε<!-- ε --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle y=f(x)+\varepsilon }</annotation>
</semantics>
</math></span><img alt="{\displaystyle y=f(x)+\varepsilon }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/93f385c9e8b0b5372dd00cfcaa57b1fe20bb812b" style="vertical-align: -0.838ex; width:12.595ex; height:2.843ex;"/></span>, where the noise, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \varepsilon }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ε<!-- ε --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \varepsilon }</annotation>
</semantics>
</math></span><img alt="\varepsilon " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a30c89172e5b88edbd45d3e2772c7f5e562e5173" style="vertical-align: -0.338ex; width:1.083ex; height:1.676ex;"/></span>, has zero mean and variance <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \sigma ^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \sigma ^{2}}</annotation>
</semantics>
</math></span><img alt="\sigma ^{2}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/53a5c55e536acf250c1d3e0f754be5692b843ef5" style="vertical-align: -0.338ex; width:2.385ex; height:2.676ex;"/></span>.
</p><p>We want to find a function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {f}}(x;D)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo>;</mo>
<mi>D</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\hat {f}}(x;D)}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\hat {f}}(x;D)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/104cc1ab5f9c97a82bdf05c6e5999d69b9db08a7" style="vertical-align: -0.838ex; width:7.796ex; height:3.343ex;"/></span>, that approximates the true function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle f(x)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>f</mi>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle f(x)}</annotation>
</semantics>
</math></span><img alt="f(x)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/202945cce41ecebb6f643f31d119c514bec7a074" style="vertical-align: -0.838ex; width:4.418ex; height:2.843ex;"/></span> as well as possible, by means of some learning algorithm based on a training dataset (sample) <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle D=\{(x_{1},y_{1})\dots ,(x_{n},y_{n})\}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>D</mi>
<mo>=</mo>
<mo fence="false" stretchy="false">{</mo>
<mo stretchy="false">(</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<mo stretchy="false">(</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mo fence="false" stretchy="false">}</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle D=\{(x_{1},y_{1})\dots ,(x_{n},y_{n})\}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle D=\{(x_{1},y_{1})\dots ,(x_{n},y_{n})\}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/757190ec5d200f1ca40a0d23e32f090a12352b6d" style="vertical-align: -0.838ex; width:27.049ex; height:2.843ex;"/></span>. We make "as well as possible" precise by measuring the <a href="/wiki/Mean_squared_error" title="Mean squared error">mean squared error</a> between <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle y}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>y</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle y}</annotation>
</semantics>
</math></span><img alt="y" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d" style="vertical-align: -0.671ex; width:1.155ex; height:2.009ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {f}}(x;D)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo>;</mo>
<mi>D</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\hat {f}}(x;D)}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\hat {f}}(x;D)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/104cc1ab5f9c97a82bdf05c6e5999d69b9db08a7" style="vertical-align: -0.838ex; width:7.796ex; height:3.343ex;"/></span>: we want <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle (y-{\hat {f}}(x;D))^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo stretchy="false">(</mo>
<mi>y</mi>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo>;</mo>
<mi>D</mi>
<mo stretchy="false">)</mo>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle (y-{\hat {f}}(x;D))^{2}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle (y-{\hat {f}}(x;D))^{2}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fcff55548f0c80eb86405dd0e2e894b6d3734f1a" style="vertical-align: -0.838ex; width:14.656ex; height:3.343ex;"/></span> to be minimal, both for <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle x_{1},\dots ,x_{n}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle x_{1},\dots ,x_{n}}</annotation>
</semantics>
</math></span><img alt="x_{1},\dots ,x_{n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e5afdbc2d248d8fa9ba2c4f5188d946a0537e753" style="vertical-align: -0.671ex; width:10.11ex; height:2.009ex;"/></span> <i>and for points outside of our sample</i>. Of course, we cannot hope to do so perfectly, since the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle y_{i}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle y_{i}}</annotation>
</semantics>
</math></span><img alt="y_{i}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/67d30d30b6c2dbe4d6f150d699de040937ecc95f" style="vertical-align: -0.671ex; width:1.939ex; height:2.009ex;"/></span> contain noise <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \varepsilon }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ε<!-- ε --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \varepsilon }</annotation>
</semantics>
</math></span><img alt="\varepsilon " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a30c89172e5b88edbd45d3e2772c7f5e562e5173" style="vertical-align: -0.338ex; width:1.083ex; height:1.676ex;"/></span>; this means we must be prepared to accept an <i>irreducible error</i> in any function we come up with.
</p><p>Finding an <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {f}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\hat {f}}}</annotation>
</semantics>
</math></span><img alt="{\hat {f}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/14ce989fd75da938ec6f95a0cdb71037b23a11cb" style="vertical-align: -0.671ex; width:1.699ex; height:3.176ex;"/></span> that generalizes to points outside of the training set can be done with any of the countless algorithms used for supervised learning. It turns out that whichever function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {f}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\hat {f}}}</annotation>
</semantics>
</math></span><img alt="{\hat {f}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/14ce989fd75da938ec6f95a0cdb71037b23a11cb" style="vertical-align: -0.671ex; width:1.699ex; height:3.176ex;"/></span> we select, we can decompose its <a href="/wiki/Expected_value" title="Expected value">expected</a> error on an unseen sample <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle x}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>x</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle x}</annotation>
</semantics>
</math></span><img alt="x" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;"/></span> as follows:<sup class="reference" id="cite_ref-islr_9-0"><a href="#cite_note-islr-9">[9]</a></sup><sup class="reference" style="white-space:nowrap;">:<span>34</span></sup><sup class="reference" id="cite_ref-ESL_10-0"><a href="#cite_note-ESL-10">[10]</a></sup><sup class="reference" style="white-space:nowrap;">:<span>223</span></sup>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {E} _{D}{\Big [}{\big (}y-{\hat {f}}(x;D){\big )}^{2}{\Big ]}={\Big (}\operatorname {Bias} _{D}{\big [}{\hat {f}}(x;D){\big ]}{\Big )}^{2}+\operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}+\sigma ^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi mathvariant="normal">E</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>D</mi>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.623em" minsize="1.623em">[</mo>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">(</mo>
</mrow>
</mrow>
<mi>y</mi>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo>;</mo>
<mi>D</mi>
<mo stretchy="false">)</mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">)</mo>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.623em" minsize="1.623em">]</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.623em" minsize="1.623em">(</mo>
</mrow>
</mrow>
<msub>
<mi>Bias</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>D</mi>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo>;</mo>
<mi>D</mi>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.623em" minsize="1.623em">)</mo>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>+</mo>
<msub>
<mi>Var</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>D</mi>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo>;</mo>
<mi>D</mi>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
<mo>+</mo>
<msup>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {E} _{D}{\Big [}{\big (}y-{\hat {f}}(x;D){\big )}^{2}{\Big ]}={\Big (}\operatorname {Bias} _{D}{\big [}{\hat {f}}(x;D){\big ]}{\Big )}^{2}+\operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}+\sigma ^{2}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {E} _{D}{\Big [}{\big (}y-{\hat {f}}(x;D){\big )}^{2}{\Big ]}={\Big (}\operatorname {Bias} _{D}{\big [}{\hat {f}}(x;D){\big ]}{\Big )}^{2}+\operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}+\sigma ^{2}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d5a2f3d7e452720a1f105dff963ad490221a4a80" style="vertical-align: -1.838ex; width:67.734ex; height:5.176ex;"/></span></dd></dl>
<p>where
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {Bias} _{D}{\big [}{\hat {f}}(x;D){\big ]}=\operatorname {E} _{D}{\big [}{\hat {f}}(x;D){\big ]}-f(x)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>Bias</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>D</mi>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo>;</mo>
<mi>D</mi>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
<mo>=</mo>
<msub>
<mi mathvariant="normal">E</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>D</mi>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo>;</mo>
<mi>D</mi>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
<mo>−<!-- − --></mo>
<mi>f</mi>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {Bias} _{D}{\big [}{\hat {f}}(x;D){\big ]}=\operatorname {E} _{D}{\big [}{\hat {f}}(x;D){\big ]}-f(x)}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {Bias} _{D}{\big [}{\hat {f}}(x;D){\big ]}=\operatorname {E} _{D}{\big [}{\hat {f}}(x;D){\big ]}-f(x)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a0013efa4f5587aa74b8c5511bf4b5864c3f5c56" style="vertical-align: -1.005ex; width:39.742ex; height:3.509ex;"/></span></dd></dl>
<p>and
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}=\operatorname {E} _{D}[{\hat {f}}(x;D)^{2}]-\operatorname {E} _{D}[{\hat {f}}(x;D)]^{2}.}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>Var</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>D</mi>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo>;</mo>
<mi>D</mi>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
<mo>=</mo>
<msub>
<mi mathvariant="normal">E</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>D</mi>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo>;</mo>
<mi>D</mi>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo stretchy="false">]</mo>
<mo>−<!-- − --></mo>
<msub>
<mi mathvariant="normal">E</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>D</mi>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo>;</mo>
<mi>D</mi>
<mo stretchy="false">)</mo>
<msup>
<mo stretchy="false">]</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}=\operatorname {E} _{D}[{\hat {f}}(x;D)^{2}]-\operatorname {E} _{D}[{\hat {f}}(x;D)]^{2}.}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}=\operatorname {E} _{D}[{\hat {f}}(x;D)^{2}]-\operatorname {E} _{D}[{\hat {f}}(x;D)]^{2}.}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/39401f53289460d9bd9892bf39911da7689095b2" style="vertical-align: -1.005ex; width:48.758ex; height:3.509ex;"/></span></dd></dl>
<p>The expectation ranges over different choices of the training set <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle D=\{(x_{1},y_{1})\dots ,(x_{n},y_{n})\}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>D</mi>
<mo>=</mo>
<mo fence="false" stretchy="false">{</mo>
<mo stretchy="false">(</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<mo stretchy="false">(</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mo fence="false" stretchy="false">}</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle D=\{(x_{1},y_{1})\dots ,(x_{n},y_{n})\}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle D=\{(x_{1},y_{1})\dots ,(x_{n},y_{n})\}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/757190ec5d200f1ca40a0d23e32f090a12352b6d" style="vertical-align: -0.838ex; width:27.049ex; height:2.843ex;"/></span>, all sampled from the same joint distribution <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle P(x,y)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>P</mi>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo>,</mo>
<mi>y</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle P(x,y)}</annotation>
</semantics>
</math></span><img alt="P(x,y)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d5b3d8f37f5458c22b61eaf26e5af0523acb63e2" style="vertical-align: -0.838ex; width:7.074ex; height:2.843ex;"/></span>. The three terms represent:
</p>
<ul><li>the square of the <i>bias</i> of the learning method, which can be thought of as the error caused by the simplifying assumptions built into the method. E.g., when approximating a non-linear function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle f(x)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>f</mi>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle f(x)}</annotation>
</semantics>
</math></span><img alt="f(x)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/202945cce41ecebb6f643f31d119c514bec7a074" style="vertical-align: -0.838ex; width:4.418ex; height:2.843ex;"/></span> using a learning method for <a href="/wiki/Linear_model" title="Linear model">linear models</a>, there will be error in the estimates <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {f}}(x)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\hat {f}}(x)}</annotation>
</semantics>
</math></span><img alt="\hat{f}(x)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/def60945c876ae17c5a91167971245424f2006f6" style="vertical-align: -0.838ex; width:4.838ex; height:3.343ex;"/></span> due to this assumption;</li>
<li>the <i>variance</i> of the learning method, or, intuitively, how much the learning method <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {f}}(x)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\hat {f}}(x)}</annotation>
</semantics>
</math></span><img alt="\hat{f}(x)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/def60945c876ae17c5a91167971245424f2006f6" style="vertical-align: -0.838ex; width:4.838ex; height:3.343ex;"/></span> will move around its mean;</li>
<li>the irreducible error <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \sigma ^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \sigma ^{2}}</annotation>
</semantics>
</math></span><img alt="\sigma ^{2}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/53a5c55e536acf250c1d3e0f754be5692b843ef5" style="vertical-align: -0.338ex; width:2.385ex; height:2.676ex;"/></span>.</li></ul>
<p>Since all three terms are non-negative, this forms a lower bound on the expected error on unseen samples.<sup class="reference" id="cite_ref-islr_9-1"><a href="#cite_note-islr-9">[9]</a></sup><sup class="reference" style="white-space:nowrap;">:<span>34</span></sup>
</p><p>The more complex the model <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {f}}(x)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\hat {f}}(x)}</annotation>
</semantics>
</math></span><img alt="\hat{f}(x)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/def60945c876ae17c5a91167971245424f2006f6" style="vertical-align: -0.838ex; width:4.838ex; height:3.343ex;"/></span> is, the more data points it will capture, and the lower the bias will be. However, complexity will make the model "move" more to capture the data points, and hence its variance will be larger.
</p>
<h3><span class="mw-headline" id="Derivation">Derivation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;action=edit&amp;section=3" title="Edit section: Derivation">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The derivation of the bias–variance decomposition for squared error proceeds as follows.<sup class="reference" id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup><sup class="reference" id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup> For notational convenience, we abbreviate <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle f=f(x)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>f</mi>
<mo>=</mo>
<mi>f</mi>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle f=f(x)}</annotation>
</semantics>
</math></span><img alt="f = f(x)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a8982ea0271b9a6b8036cd2a883000a59f5a1ab9" style="vertical-align: -0.838ex; width:8.795ex; height:2.843ex;"/></span>, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {f}}={\hat {f}}(x;D)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo>;</mo>
<mi>D</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\hat {f}}={\hat {f}}(x;D)}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\hat {f}}={\hat {f}}(x;D)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5d13a9cdcbd8adcdb91352a02eb61129c669bf74" style="vertical-align: -0.838ex; width:12.594ex; height:3.343ex;"/></span> and we drop the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle D}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>D</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle D}</annotation>
</semantics>
</math></span><img alt="D" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f34a0c600395e5d4345287e21fb26efd386990e6" style="vertical-align: -0.338ex; width:1.924ex; height:2.176ex;"/></span> subscript on our expectation operators. First, recall that, by definition, for any random variable <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle X}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>X</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle X}</annotation>
</semantics>
</math></span><img alt="X" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" style="vertical-align: -0.338ex; width:1.98ex; height:2.176ex;"/></span>, we have
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {Var} [X]=\operatorname {E} [X^{2}]-{\Big (}\operatorname {E} [X]{\Big )}^{2}.}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>Var</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>X</mi>
<mo stretchy="false">]</mo>
<mo>=</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<msup>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo stretchy="false">]</mo>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.623em" minsize="1.623em">(</mo>
</mrow>
</mrow>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>X</mi>
<mo stretchy="false">]</mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.623em" minsize="1.623em">)</mo>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {Var} [X]=\operatorname {E} [X^{2}]-{\Big (}\operatorname {E} [X]{\Big )}^{2}.}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {Var} [X]=\operatorname {E} [X^{2}]-{\Big (}\operatorname {E} [X]{\Big )}^{2}.}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d297a102ff965a2b1f0c5cf7fd46d3fc2ae639d8" style="vertical-align: -1.838ex; width:28.678ex; height:5.176ex;"/></span></dd></dl>
<p>Rearranging, we get:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {E} [X^{2}]=\operatorname {Var} [X]+{\Big (}\operatorname {E} [X]{\Big )}^{2}.}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<msup>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo stretchy="false">]</mo>
<mo>=</mo>
<mi>Var</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>X</mi>
<mo stretchy="false">]</mo>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.623em" minsize="1.623em">(</mo>
</mrow>
</mrow>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>X</mi>
<mo stretchy="false">]</mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.623em" minsize="1.623em">)</mo>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {E} [X^{2}]=\operatorname {Var} [X]+{\Big (}\operatorname {E} [X]{\Big )}^{2}.}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {E} [X^{2}]=\operatorname {Var} [X]+{\Big (}\operatorname {E} [X]{\Big )}^{2}.}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3cc35b8c4a5bfb7ac920b37b441b02732006cee0" style="vertical-align: -1.838ex; width:28.678ex; height:5.176ex;"/></span></dd></dl>
<p>Since <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle f}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>f</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle f}</annotation>
</semantics>
</math></span><img alt="f" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;"/></span> is <a href="/wiki/Deterministic_algorithm" title="Deterministic algorithm">deterministic</a>, i.e. independent of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle D}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>D</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle D}</annotation>
</semantics>
</math></span><img alt="D" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f34a0c600395e5d4345287e21fb26efd386990e6" style="vertical-align: -0.338ex; width:1.924ex; height:2.176ex;"/></span>,
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {E} [f]=f.}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>f</mi>
<mo stretchy="false">]</mo>
<mo>=</mo>
<mi>f</mi>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {E} [f]=f.}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {E} [f]=f.}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e4cc9a0668ccec054ec20f2d518495ea0aa46e61" style="vertical-align: -0.838ex; width:9.179ex; height:2.843ex;"/></span></dd></dl>
<p>Thus, given <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle y=f+\varepsilon }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>y</mi>
<mo>=</mo>
<mi>f</mi>
<mo>+</mo>
<mi>ε<!-- ε --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle y=f+\varepsilon }</annotation>
</semantics>
</math></span><img alt="{\displaystyle y=f+\varepsilon }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f4e3b37184854b796cb4323f3f1c6285760c70d9" style="vertical-align: -0.671ex; width:9.456ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {E} [\varepsilon ]=0}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>ε<!-- ε --></mi>
<mo stretchy="false">]</mo>
<mo>=</mo>
<mn>0</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {E} [\varepsilon ]=0}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {E} [\varepsilon ]=0}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a9494698413723204253e82aaff8b02684a64ed6" style="vertical-align: -0.838ex; width:8.221ex; height:2.843ex;"/></span> (because <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \varepsilon }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ε<!-- ε --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \varepsilon }</annotation>
</semantics>
</math></span><img alt="\varepsilon " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a30c89172e5b88edbd45d3e2772c7f5e562e5173" style="vertical-align: -0.338ex; width:1.083ex; height:1.676ex;"/></span> is noise), implies <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {E} [y]=\operatorname {E} [f+\varepsilon ]=\operatorname {E} [f]=f.}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>y</mi>
<mo stretchy="false">]</mo>
<mo>=</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>f</mi>
<mo>+</mo>
<mi>ε<!-- ε --></mi>
<mo stretchy="false">]</mo>
<mo>=</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>f</mi>
<mo stretchy="false">]</mo>
<mo>=</mo>
<mi>f</mi>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {E} [y]=\operatorname {E} [f+\varepsilon ]=\operatorname {E} [f]=f.}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {E} [y]=\operatorname {E} [f+\varepsilon ]=\operatorname {E} [f]=f.}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fd5be0da3a9300d8822afa6080b3d404cf33bb18" style="vertical-align: -0.838ex; width:27.487ex; height:2.843ex;"/></span>
</p><p>Also, since <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {Var} [\varepsilon ]=\sigma ^{2},}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>Var</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>ε<!-- ε --></mi>
<mo stretchy="false">]</mo>
<mo>=</mo>
<msup>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>,</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {Var} [\varepsilon ]=\sigma ^{2},}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {Var} [\varepsilon ]=\sigma ^{2},}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e1d4c278d7174310f125f7f1625b204d6c9ef6b7" style="vertical-align: -0.838ex; width:12.324ex; height:3.176ex;"/></span>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {Var} [y]=\operatorname {E} [(y-\operatorname {E} [y])^{2}]=\operatorname {E} [(y-f)^{2}]=\operatorname {E} [(f+\varepsilon -f)^{2}]=\operatorname {E} [\varepsilon ^{2}]=\operatorname {Var} [\varepsilon ]+{\Big (}\operatorname {E} [\varepsilon ]{\Big )}^{2}=\sigma ^{2}+0^{2}=\sigma ^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>Var</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>y</mi>
<mo stretchy="false">]</mo>
<mo>=</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mo stretchy="false">(</mo>
<mi>y</mi>
<mo>−<!-- − --></mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>y</mi>
<mo stretchy="false">]</mo>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo stretchy="false">]</mo>
<mo>=</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mo stretchy="false">(</mo>
<mi>y</mi>
<mo>−<!-- − --></mo>
<mi>f</mi>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo stretchy="false">]</mo>
<mo>=</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mo stretchy="false">(</mo>
<mi>f</mi>
<mo>+</mo>
<mi>ε<!-- ε --></mi>
<mo>−<!-- − --></mo>
<mi>f</mi>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo stretchy="false">]</mo>
<mo>=</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<msup>
<mi>ε<!-- ε --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo stretchy="false">]</mo>
<mo>=</mo>
<mi>Var</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>ε<!-- ε --></mi>
<mo stretchy="false">]</mo>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.623em" minsize="1.623em">(</mo>
</mrow>
</mrow>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>ε<!-- ε --></mi>
<mo stretchy="false">]</mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.623em" minsize="1.623em">)</mo>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>=</mo>
<msup>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>+</mo>
<msup>
<mn>0</mn>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>=</mo>
<msup>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {Var} [y]=\operatorname {E} [(y-\operatorname {E} [y])^{2}]=\operatorname {E} [(y-f)^{2}]=\operatorname {E} [(f+\varepsilon -f)^{2}]=\operatorname {E} [\varepsilon ^{2}]=\operatorname {Var} [\varepsilon ]+{\Big (}\operatorname {E} [\varepsilon ]{\Big )}^{2}=\sigma ^{2}+0^{2}=\sigma ^{2}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {Var} [y]=\operatorname {E} [(y-\operatorname {E} [y])^{2}]=\operatorname {E} [(y-f)^{2}]=\operatorname {E} [(f+\varepsilon -f)^{2}]=\operatorname {E} [\varepsilon ^{2}]=\operatorname {Var} [\varepsilon ]+{\Big (}\operatorname {E} [\varepsilon ]{\Big )}^{2}=\sigma ^{2}+0^{2}=\sigma ^{2}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/22a04d2cb3d3cd11aca31f8595439cb559d12975" style="vertical-align: -1.838ex; width:99.852ex; height:5.176ex;"/></span></dd></dl>
<p>Thus, since <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \varepsilon }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ε<!-- ε --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \varepsilon }</annotation>
</semantics>
</math></span><img alt="\varepsilon " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a30c89172e5b88edbd45d3e2772c7f5e562e5173" style="vertical-align: -0.338ex; width:1.083ex; height:1.676ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {f}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\hat {f}}}</annotation>
</semantics>
</math></span><img alt="{\hat {f}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/14ce989fd75da938ec6f95a0cdb71037b23a11cb" style="vertical-align: -0.671ex; width:1.699ex; height:3.176ex;"/></span> are independent, we can write
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\begin{aligned}\operatorname {E} {\big [}(y-{\hat {f}})^{2}{\big ]}&amp;=\operatorname {E} {\big [}(f+\varepsilon -{\hat {f}})^{2}{\big ]}\\[5pt]&amp;=\operatorname {E} {\big [}(f+\varepsilon -{\hat {f}}+\operatorname {E} [{\hat {f}}]-\operatorname {E} [{\hat {f}}])^{2}{\big ]}\\[5pt]&amp;=\operatorname {E} {\big [}(f-\operatorname {E} [{\hat {f}}])^{2}{\big ]}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}+2\operatorname {E} {\big [}(f-\operatorname {E} [{\hat {f}}])\varepsilon {\big ]}+2\operatorname {E} {\big [}\varepsilon (\operatorname {E} [{\hat {f}}]-{\hat {f}}){\big ]}+2\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})(f-\operatorname {E} [{\hat {f}}]){\big ]}\\[5pt]&amp;=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}+2(f-\operatorname {E} [{\hat {f}}])\operatorname {E} [\varepsilon ]+2\operatorname {E} [\varepsilon ]\operatorname {E} {\big [}\operatorname {E} [{\hat {f}}]-{\hat {f}}{\big ]}+2\operatorname {E} {\big [}\operatorname {E} [{\hat {f}}]-{\hat {f}}{\big ]}(f-\operatorname {E} [{\hat {f}}])\\[5pt]&amp;=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}\\[5pt]&amp;=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {Var} [\varepsilon ]+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\[5pt]&amp;=\operatorname {Bias} [{\hat {f}}]^{2}+\operatorname {Var} [\varepsilon ]+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\[5pt]&amp;=\operatorname {Bias} [{\hat {f}}]^{2}+\sigma ^{2}+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\end{aligned}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="0.8em 0.8em 0.8em 0.8em 0.8em 0.8em 0.8em 0.3em">
<mtr>
<mtd>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>y</mi>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
</mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>f</mi>
<mo>+</mo>
<mi>ε<!-- ε --></mi>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd></mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>f</mi>
<mo>+</mo>
<mi>ε<!-- ε --></mi>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo>+</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">]</mo>
<mo>−<!-- − --></mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">]</mo>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd></mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>f</mi>
<mo>−<!-- − --></mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">]</mo>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
<mo>+</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<msup>
<mi>ε<!-- ε --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo stretchy="false">]</mo>
<mo>+</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">]</mo>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
<mo>+</mo>
<mn>2</mn>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>f</mi>
<mo>−<!-- − --></mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">]</mo>
<mo stretchy="false">)</mo>
<mi>ε<!-- ε --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
<mo>+</mo>
<mn>2</mn>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mi>ε<!-- ε --></mi>
<mo stretchy="false">(</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">]</mo>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
<mo>+</mo>
<mn>2</mn>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">]</mo>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">)</mo>
<mo stretchy="false">(</mo>
<mi>f</mi>
<mo>−<!-- − --></mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">]</mo>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd></mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<mo stretchy="false">(</mo>
<mi>f</mi>
<mo>−<!-- − --></mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">]</mo>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>+</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<msup>
<mi>ε<!-- ε --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo stretchy="false">]</mo>
<mo>+</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">]</mo>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
<mo>+</mo>
<mn>2</mn>
<mo stretchy="false">(</mo>
<mi>f</mi>
<mo>−<!-- − --></mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">]</mo>
<mo stretchy="false">)</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>ε<!-- ε --></mi>
<mo stretchy="false">]</mo>
<mo>+</mo>
<mn>2</mn>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>ε<!-- ε --></mi>
<mo stretchy="false">]</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">]</mo>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
<mo>+</mo>
<mn>2</mn>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">]</mo>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>f</mi>
<mo>−<!-- − --></mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">]</mo>
<mo stretchy="false">)</mo>
</mtd>
</mtr>
<mtr>
<mtd></mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<mo stretchy="false">(</mo>
<mi>f</mi>
<mo>−<!-- − --></mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">]</mo>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>+</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<msup>
<mi>ε<!-- ε --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo stretchy="false">]</mo>
<mo>+</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">]</mo>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd></mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<mo stretchy="false">(</mo>
<mi>f</mi>
<mo>−<!-- − --></mo>
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">]</mo>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>+</mo>
<mi>Var</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>ε<!-- ε --></mi>
<mo stretchy="false">]</mo>
<mo>+</mo>
<mi>Var</mi>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd></mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<mi>Bias</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<msup>
<mo stretchy="false">]</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>+</mo>
<mi>Var</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mi>ε<!-- ε --></mi>
<mo stretchy="false">]</mo>
<mo>+</mo>
<mi>Var</mi>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd></mtd>
<mtd>
<mi></mi>
<mo>=</mo>
<mi>Bias</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<msup>
<mo stretchy="false">]</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>+</mo>
<msup>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>+</mo>
<mi>Var</mi>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
</mtd>
</mtr>
</mtable>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\begin{aligned}\operatorname {E} {\big [}(y-{\hat {f}})^{2}{\big ]}&amp;=\operatorname {E} {\big [}(f+\varepsilon -{\hat {f}})^{2}{\big ]}\\[5pt]&amp;=\operatorname {E} {\big [}(f+\varepsilon -{\hat {f}}+\operatorname {E} [{\hat {f}}]-\operatorname {E} [{\hat {f}}])^{2}{\big ]}\\[5pt]&amp;=\operatorname {E} {\big [}(f-\operatorname {E} [{\hat {f}}])^{2}{\big ]}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}+2\operatorname {E} {\big [}(f-\operatorname {E} [{\hat {f}}])\varepsilon {\big ]}+2\operatorname {E} {\big [}\varepsilon (\operatorname {E} [{\hat {f}}]-{\hat {f}}){\big ]}+2\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})(f-\operatorname {E} [{\hat {f}}]){\big ]}\\[5pt]&amp;=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}+2(f-\operatorname {E} [{\hat {f}}])\operatorname {E} [\varepsilon ]+2\operatorname {E} [\varepsilon ]\operatorname {E} {\big [}\operatorname {E} [{\hat {f}}]-{\hat {f}}{\big ]}+2\operatorname {E} {\big [}\operatorname {E} [{\hat {f}}]-{\hat {f}}{\big ]}(f-\operatorname {E} [{\hat {f}}])\\[5pt]&amp;=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}\\[5pt]&amp;=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {Var} [\varepsilon ]+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\[5pt]&amp;=\operatorname {Bias} [{\hat {f}}]^{2}+\operatorname {Var} [\varepsilon ]+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\[5pt]&amp;=\operatorname {Bias} [{\hat {f}}]^{2}+\sigma ^{2}+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\end{aligned}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\begin{aligned}\operatorname {E} {\big [}(y-{\hat {f}})^{2}{\big ]}&amp;=\operatorname {E} {\big [}(f+\varepsilon -{\hat {f}})^{2}{\big ]}\\[5pt]&amp;=\operatorname {E} {\big [}(f+\varepsilon -{\hat {f}}+\operatorname {E} [{\hat {f}}]-\operatorname {E} [{\hat {f}}])^{2}{\big ]}\\[5pt]&amp;=\operatorname {E} {\big [}(f-\operatorname {E} [{\hat {f}}])^{2}{\big ]}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}+2\operatorname {E} {\big [}(f-\operatorname {E} [{\hat {f}}])\varepsilon {\big ]}+2\operatorname {E} {\big [}\varepsilon (\operatorname {E} [{\hat {f}}]-{\hat {f}}){\big ]}+2\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})(f-\operatorname {E} [{\hat {f}}]){\big ]}\\[5pt]&amp;=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}+2(f-\operatorname {E} [{\hat {f}}])\operatorname {E} [\varepsilon ]+2\operatorname {E} [\varepsilon ]\operatorname {E} {\big [}\operatorname {E} [{\hat {f}}]-{\hat {f}}{\big ]}+2\operatorname {E} {\big [}\operatorname {E} [{\hat {f}}]-{\hat {f}}{\big ]}(f-\operatorname {E} [{\hat {f}}])\\[5pt]&amp;=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}\\[5pt]&amp;=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {Var} [\varepsilon ]+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\[5pt]&amp;=\operatorname {Bias} [{\hat {f}}]^{2}+\operatorname {Var} [\varepsilon ]+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\[5pt]&amp;=\operatorname {Bias} [{\hat {f}}]^{2}+\sigma ^{2}+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\end{aligned}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/30dd0e093cc889a45834d04b04456688244b50fb" style="vertical-align: -18.505ex; width:128.289ex; height:38.176ex;"/></span></dd></dl>
<p>Finally, MSE loss function (or negative log-likelihood) is obtained by taking the expectation value over <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle x\sim P}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>x</mi>
<mo>∼<!-- ∼ --></mo>
<mi>P</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle x\sim P}</annotation>
</semantics>
</math></span><img alt="{\displaystyle x\sim P}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/22dca9c851a0f2939e608ad215bf42408aa95be8" style="vertical-align: -0.338ex; width:6.174ex; height:2.176ex;"/></span>:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\text{MSE}}=\operatorname {E} _{x}{\bigg \{}\operatorname {Bias} _{D}[{\hat {f}}(x;D)]^{2}+\operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}{\bigg \}}+\sigma ^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mtext>MSE</mtext>
</mrow>
<mo>=</mo>
<msub>
<mi mathvariant="normal">E</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>x</mi>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="2.047em" minsize="2.047em">{</mo>
</mrow>
</mrow>
<msub>
<mi>Bias</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>D</mi>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo>;</mo>
<mi>D</mi>
<mo stretchy="false">)</mo>
<msup>
<mo stretchy="false">]</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>+</mo>
<msub>
<mi>Var</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>D</mi>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">[</mo>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo>;</mo>
<mi>D</mi>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="1.2em" minsize="1.2em">]</mo>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mo maxsize="2.047em" minsize="2.047em">}</mo>
</mrow>
</mrow>
<mo>+</mo>
<msup>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\text{MSE}}=\operatorname {E} _{x}{\bigg \{}\operatorname {Bias} _{D}[{\hat {f}}(x;D)]^{2}+\operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}{\bigg \}}+\sigma ^{2}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\text{MSE}}=\operatorname {E} _{x}{\bigg \{}\operatorname {Bias} _{D}[{\hat {f}}(x;D)]^{2}+\operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}{\bigg \}}+\sigma ^{2}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/10b361231391b0297bcbaf5dc30688f349d64a0b" style="vertical-align: -2.505ex; width:54.827ex; height:6.176ex;"/></span></dd></dl>
<h2><span class="mw-headline" id="Application_to_regression">Application to regression</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;action=edit&amp;section=4" title="Edit section: Application to regression">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The bias–variance decomposition forms the conceptual basis for regression <a href="/wiki/Regularization_(mathematics)" title="Regularization (mathematics)">regularization</a> methods such as <a href="/wiki/Lasso_(statistics)" title="Lasso (statistics)">Lasso</a> and <a class="mw-redirect" href="/wiki/Ridge_regression" title="Ridge regression">ridge regression</a>. Regularization methods introduce bias into the regression solution that can reduce variance considerably relative to the <a href="/wiki/Ordinary_least_squares" title="Ordinary least squares">ordinary least squares (OLS)</a> solution.  Although the OLS solution provides non-biased regression estimates, the lower variance solutions produced by regularization techniques provide superior MSE performance.
</p>
<h2><span class="mw-headline" id="Application_to_classification">Application to classification</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;action=edit&amp;section=5" title="Edit section: Application to classification">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The bias–variance decomposition was originally formulated for least-squares regression. For the case of <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> under the <a class="new" href="/w/index.php?title=0%E2%80%931_loss&amp;action=edit&amp;redlink=1" title="0–1 loss (page does not exist)">0–1 loss</a> (misclassification rate), it is possible to find a similar decomposition.<sup class="reference" id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup><sup class="reference" id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup> Alternatively, if the classification problem can be phrased as <a href="/wiki/Probabilistic_classification" title="Probabilistic classification">probabilistic classification</a>, then the expected squared error of the predicted probabilities with respect to the true probabilities can be decomposed as before.<sup class="reference" id="cite_ref-15"><a href="#cite_note-15">[15]</a></sup>
</p>
<h2><span class="mw-headline" id="Application_to_reinforcement_learning">Application to reinforcement learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;action=edit&amp;section=6" title="Edit section: Application to reinforcement learning">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Even though the bias–variance decomposition does not directly apply in reinforcement learning, a similar tradeoff can also characterize generalization. When an agent has limited information on its environment, the suboptimality of an RL algorithm can be decomposed into the sum of two terms: a term related to an asymptotic bias and a term due to overfitting. The asymptotic bias is directly related to the learning algorithm (independently of the quantity of data) while the overfitting term comes from the fact that the amount of data is limited.<sup class="reference" id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup>
</p>
<h2><span class="mw-headline" id="Approaches">Approaches</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;action=edit&amp;section=7" title="Edit section: Approaches">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a> and <a href="/wiki/Feature_selection" title="Feature selection">feature selection</a> can decrease variance by simplifying models. Similarly, a larger training set tends to decrease variance. Adding features (predictors) tends to decrease bias, at the expense of introducing additional variance. Learning algorithms typically have some tunable parameters that control bias and variance; for example,
</p>
<ul><li><a href="/wiki/Linear_model" title="Linear model">linear</a>  and <a href="/wiki/Generalized_linear_model" title="Generalized linear model">Generalized linear</a> models can be <a href="/wiki/Regularization_(mathematics)" title="Regularization (mathematics)">regularized</a> to decrease their variance at the cost of increasing their bias.<sup class="reference" id="cite_ref-17"><a href="#cite_note-17">[17]</a></sup></li>
<li>In <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural networks</a>, the variance increases and the bias decreases as the number of hidden units increase,<sup class="reference" id="cite_ref-geman_3-1"><a href="#cite_note-geman-3">[3]</a></sup> although this classical assumption has been the subject of recent debate.<sup class="reference" id="cite_ref-neal2018_8-2"><a href="#cite_note-neal2018-8">[8]</a></sup> Like in GLMs, regularization is typically applied.</li>
<li>In <a class="mw-redirect" href="/wiki/K-nearest_neighbor" title="K-nearest neighbor"><i>k</i>-nearest neighbor</a> models, a high value of <span class="texhtml mvar" style="font-style:italic;">k</span> leads to high bias and low variance (see below).</li>
<li>In <a href="/wiki/Instance-based_learning" title="Instance-based learning">instance-based learning</a>, regularization can be achieved varying the mixture of <a href="/wiki/Prototype" title="Prototype">prototypes</a> and exemplars.<sup class="reference" id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup></li>
<li>In <a href="/wiki/Decision_tree" title="Decision tree">decision trees</a>, the depth of the tree determines the variance. Decision trees are commonly pruned to control variance.<sup class="reference" id="cite_ref-islr_9-2"><a href="#cite_note-islr-9">[9]</a></sup><sup class="reference" style="white-space:nowrap;">:<span>307</span></sup></li></ul>
<p>One way of resolving the trade-off is to use <a class="mw-redirect" href="/wiki/Mixture_models" title="Mixture models">mixture models</a> and <a href="/wiki/Ensemble_learning" title="Ensemble learning">ensemble learning</a>.<sup class="reference" id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup><sup class="reference" id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup> For example, <a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">boosting</a> combines many "weak" (high bias) models in an ensemble that has lower bias than the individual models, while <a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">bagging</a> combines "strong" learners in a way that reduces their variance.  
</p><p><a class="mw-redirect" href="/wiki/Model_validation" title="Model validation">Model validation</a> methods such as <a href="/wiki/Cross-validation_(statistics)" title="Cross-validation (statistics)">cross-validation (statistics)</a> can be used to tune models so as to optimize the trade-off. 
</p>
<h3><span class="mw-headline" id="k-nearest_neighbors"><i>k</i>-nearest neighbors</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;action=edit&amp;section=8" title="Edit section: k-nearest neighbors">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In the case of <a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><span class="texhtml mvar" style="font-style:italic;">k</span>-nearest neighbors regression</a>, when the expectation is taken over the possible labeling of a fixed training set, a <a href="/wiki/Closed-form_expression" title="Closed-form expression">closed-form expression</a> exists that relates the bias–variance decomposition to the parameter <span class="texhtml mvar" style="font-style:italic;">k</span>:<sup class="reference" id="cite_ref-ESL_10-1"><a href="#cite_note-ESL-10">[10]</a></sup><sup class="reference" style="white-space:nowrap;">:<span>37, 223</span></sup>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {E} [(y-{\hat {f}}(x))^{2}\mid X=x]=\left(f(x)-{\frac {1}{k}}\sum _{i=1}^{k}f(N_{i}(x))\right)^{2}+{\frac {\sigma ^{2}}{k}}+\sigma ^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi mathvariant="normal">E</mi>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">[</mo>
<mo stretchy="false">(</mo>
<mi>y</mi>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>f</mi>
<mo stretchy="false">^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
<msup>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>∣<!-- ∣ --></mo>
<mi>X</mi>
<mo>=</mo>
<mi>x</mi>
<mo stretchy="false">]</mo>
<mo>=</mo>
<msup>
<mrow>
<mo>(</mo>
<mrow>
<mi>f</mi>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>k</mi>
</mfrac>
</mrow>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>k</mi>
</mrow>
</munderover>
<mi>f</mi>
<mo stretchy="false">(</mo>
<msub>
<mi>N</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
<mo stretchy="false">)</mo>
</mrow>
<mo>)</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<msup>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mi>k</mi>
</mfrac>
</mrow>
<mo>+</mo>
<msup>
<mi>σ<!-- σ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {E} [(y-{\hat {f}}(x))^{2}\mid X=x]=\left(f(x)-{\frac {1}{k}}\sum _{i=1}^{k}f(N_{i}(x))\right)^{2}+{\frac {\sigma ^{2}}{k}}+\sigma ^{2}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {E} [(y-{\hat {f}}(x))^{2}\mid X=x]=\left(f(x)-{\frac {1}{k}}\sum _{i=1}^{k}f(N_{i}(x))\right)^{2}+{\frac {\sigma ^{2}}{k}}+\sigma ^{2}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/46dd9ffaa7af7d8738d2799f9f91df7c00d2118a" style="vertical-align: -3.171ex; width:64.367ex; height:8.009ex;"/></span></dd></dl>
<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle N_{1}(x),\dots ,N_{k}(x)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>N</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi>N</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>k</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle N_{1}(x),\dots ,N_{k}(x)}</annotation>
</semantics>
</math></span><img alt="N_1(x), \dots, N_k(x)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5835207a9f81fe493e2b77a4a3ab0ca773e4fc50" style="vertical-align: -0.838ex; width:17.332ex; height:2.843ex;"/></span> are the <span class="texhtml mvar" style="font-style:italic;">k</span> nearest neighbors of <span class="texhtml mvar" style="font-style:italic;">x</span> in the training set. The bias (first term) is a monotone rising function of <span class="texhtml mvar" style="font-style:italic;">k</span>, while the variance (second term) drops off as <span class="texhtml mvar" style="font-style:italic;">k</span> is increased. In fact, under "reasonable assumptions" the bias of the first-nearest neighbor (1-NN) estimator vanishes entirely as the size of the training set approaches infinity.<sup class="reference" id="cite_ref-geman_3-2"><a href="#cite_note-geman-3">[3]</a></sup>
</p>
<h2><span class="mw-headline" id="Application_to_human_learning">Application to human learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;action=edit&amp;section=9" title="Edit section: Application to human learning">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>While widely discussed in the context of machine learning, the bias-variance dilemma has been examined in the context of <a href="/wiki/Cognitive_science" title="Cognitive science">human cognition</a>, most notably by <a href="/wiki/Gerd_Gigerenzer" title="Gerd Gigerenzer">Gerd Gigerenzer</a> and co-workers in the context of learned heuristics. They have argued (see references below) that the human brain resolves the dilemma in the case of the typically sparse, poorly-characterised training-sets provided by experience by adopting high-bias/low variance heuristics. This reflects the fact that a zero-bias approach has poor generalisability to new situations, and also unreasonably presumes precise knowledge of the true state of the world. The resulting heuristics are relatively simple, but produce better inferences in a wider variety of situations.<sup class="reference" id="cite_ref-ReferenceA_7-1"><a href="#cite_note-ReferenceA-7">[7]</a></sup>
</p><p><a href="/wiki/Stuart_Geman" title="Stuart Geman">Geman</a> et al.<sup class="reference" id="cite_ref-geman_3-3"><a href="#cite_note-geman-3">[3]</a></sup> argue that the bias-variance dilemma implies that abilities such as generic <a class="mw-redirect" href="/wiki/Object_recognition" title="Object recognition">object recognition</a> cannot be learned from scratch, but require a certain degree of “hard wiring”   that is later tuned by experience.  This is because model-free approaches to inference require impractically large training sets if they are to avoid high variance.
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;action=edit&amp;section=10" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="div-col columns column-width" style="-moz-column-width: 25em; -webkit-column-width: 25em; column-width: 25em;">
<ul><li><a href="/wiki/Accuracy_and_precision" title="Accuracy and precision">Accuracy and precision</a></li>
<li><a href="/wiki/Bias_of_an_estimator" title="Bias of an estimator">Bias of an estimator</a></li>
<li><a href="/wiki/Gauss%E2%80%93Markov_theorem" title="Gauss–Markov theorem">Gauss–Markov theorem</a></li>
<li><a href="/wiki/Hyperparameter_optimization" title="Hyperparameter optimization">Hyperparameter optimization</a></li>
<li><a href="/wiki/Minimum-variance_unbiased_estimator" title="Minimum-variance unbiased estimator">Minimum-variance unbiased estimator</a></li>
<li><a href="/wiki/Model_selection" title="Model selection">Model selection</a></li>
<li><a class="mw-redirect" href="/wiki/Regression_model_validation" title="Regression model validation">Regression model validation</a></li>
<li><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a></li></ul>
</div>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;action=edit&amp;section=11" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist" style="list-style-type: decimal;">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><cite class="citation journal">Kohavi, Ron; Wolpert, David H. (1996). "Bias Plus Variance Decomposition for Zero-One Loss Functions". <i>ICML</i>. <b>96</b>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ICML&amp;rft.atitle=Bias+Plus+Variance+Decomposition+for+Zero-One+Loss+Functions&amp;rft.volume=96&amp;rft.date=1996&amp;rft.aulast=Kohavi&amp;rft.aufirst=Ron&amp;rft.au=Wolpert%2C+David+H.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><style data-mw-deduplicate="TemplateStyles:r935243608">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite class="citation journal">Luxburg, Ulrike V.; Schölkopf, B. (2011). "Statistical learning theory: Models, concepts, and results". <i>Handbook of the History of Logic</i>. <b>10</b>: Section 2.4.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Handbook+of+the+History+of+Logic&amp;rft.atitle=Statistical+learning+theory%3A+Models%2C+concepts%2C+and+results&amp;rft.volume=10&amp;rft.pages=Section+2.4&amp;rft.date=2011&amp;rft.aulast=Luxburg&amp;rft.aufirst=Ulrike+V.&amp;rft.au=Sch%C3%B6lkopf%2C+B.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-geman-3"><span class="mw-cite-backlink">^ <a href="#cite_ref-geman_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-geman_3-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-geman_3-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-geman_3-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal"><a href="/wiki/Stuart_Geman" title="Stuart Geman">Geman, Stuart</a>; Bienenstock, Élie; Doursat, René (1992). <a class="external text" href="http://web.mit.edu/6.435/www/Geman92.pdf" rel="nofollow">"Neural networks and the bias/variance dilemma"</a> <span class="cs1-format">(PDF)</span>. <i>Neural Computation</i>. <b>4</b>: 1–58. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1162%2Fneco.1992.4.1.1" rel="nofollow">10.1162/neco.1992.4.1.1</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=Neural+networks+and+the+bias%2Fvariance+dilemma&amp;rft.volume=4&amp;rft.pages=1-58&amp;rft.date=1992&amp;rft_id=info%3Adoi%2F10.1162%2Fneco.1992.4.1.1&amp;rft.aulast=Geman&amp;rft.aufirst=Stuart&amp;rft.au=Bienenstock%2C+%C3%89lie&amp;rft.au=Doursat%2C+Ren%C3%A9&amp;rft_id=http%3A%2F%2Fweb.mit.edu%2F6.435%2Fwww%2FGeman92.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><cite class="citation book">Sammut, Claude; Webb, Geoffrey I., eds. (2011). "Bias–Variance Decomposition". <i>Encyclopedia of Machine Learning</i>. Springer. pp. 100–101. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2010eoml.book.....S" rel="nofollow">2010eoml.book.....S</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Bias%E2%80%93Variance+Decomposition&amp;rft.btitle=Encyclopedia+of+Machine+Learning&amp;rft.pages=100-101&amp;rft.pub=Springer&amp;rft.date=2011&amp;rft_id=info%3Abibcode%2F2010eoml.book.....S&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-nealThesis2019-5"><span class="mw-cite-backlink">^ <a href="#cite_ref-nealThesis2019_5-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-nealThesis2019_5-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-nealThesis2019_5-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation arxiv">Neal, Brady (2019). "On the Bias-Variance Tradeoff: Textbooks Need an Update". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1912.08286" rel="nofollow">1912.08286</a></span> [<a class="external text" href="//arxiv.org/archive/cs.LG" rel="nofollow">cs.LG</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=On+the+Bias-Variance+Tradeoff%3A+Textbooks+Need+an+Update&amp;rft.date=2019&amp;rft_id=info%3Aarxiv%2F1912.08286&amp;rft.aulast=Neal&amp;rft.aufirst=Brady&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-nealBlog2020-6"><span class="mw-cite-backlink">^ <a href="#cite_ref-nealBlog2020_6-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-nealBlog2020_6-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-nealBlog2020_6-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation web">Neal, Brady (5 January 2020). <a class="external text" href="https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update" rel="nofollow">"On the Bias-Variance Tradeoff: Textbooks Need an Update (Blog Post)"</a><span class="reference-accessdate">. Retrieved <span class="nowrap">12 March</span> 2020</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=On+the+Bias-Variance+Tradeoff%3A+Textbooks+Need+an+Update+%28Blog+Post%29&amp;rft.date=2020-01-05&amp;rft.aulast=Neal&amp;rft.aufirst=Brady&amp;rft_id=https%3A%2F%2Fwww.bradyneal.com%2Fbias-variance-tradeoff-textbooks-update&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-ReferenceA-7"><span class="mw-cite-backlink">^ <a href="#cite_ref-ReferenceA_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ReferenceA_7-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal"><a href="/wiki/Gerd_Gigerenzer" title="Gerd Gigerenzer">Gigerenzer, Gerd</a>; Brighton, Henry (2009). "Homo Heuristicus: Why Biased Minds Make Better Inferences". <i>Topics in Cognitive Science</i>. <b>1</b> (1): 107–143. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1111%2Fj.1756-8765.2008.01006.x" rel="nofollow">10.1111/j.1756-8765.2008.01006.x</a>. <a href="/wiki/Handle_System" title="Handle System">hdl</a>:<a class="external text" href="//hdl.handle.net/11858%2F00-001M-0000-0024-F678-0" rel="nofollow">11858/00-001M-0000-0024-F678-0</a>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/25164802" rel="nofollow">25164802</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Topics+in+Cognitive+Science&amp;rft.atitle=Homo+Heuristicus%3A+Why+Biased+Minds+Make+Better+Inferences&amp;rft.volume=1&amp;rft.issue=1&amp;rft.pages=107-143&amp;rft.date=2009&amp;rft_id=info%3Ahdl%2F11858%2F00-001M-0000-0024-F678-0&amp;rft_id=info%3Apmid%2F25164802&amp;rft_id=info%3Adoi%2F10.1111%2Fj.1756-8765.2008.01006.x&amp;rft.aulast=Gigerenzer&amp;rft.aufirst=Gerd&amp;rft.au=Brighton%2C+Henry&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-neal2018-8"><span class="mw-cite-backlink">^ <a href="#cite_ref-neal2018_8-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-neal2018_8-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-neal2018_8-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation arxiv">Neal, Brady; Mittal, Sarthak; Baratin, Aristide; Tantia, Vinayak; Scicluna, Matthew; Lacoste-Julien, Simon; Mitliagkas, Ioannis (2018). "A Modern Take on the Bias-Variance Tradeoff in Neural Networks". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1810.08591" rel="nofollow">1810.08591</a></span> [<a class="external text" href="//arxiv.org/archive/cs.LG" rel="nofollow">cs.LG</a>].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=A+Modern+Take+on+the+Bias-Variance+Tradeoff+in+Neural+Networks&amp;rft.date=2018&amp;rft_id=info%3Aarxiv%2F1810.08591&amp;rft.aulast=Neal&amp;rft.aufirst=Brady&amp;rft.au=Mittal%2C+Sarthak&amp;rft.au=Baratin%2C+Aristide&amp;rft.au=Tantia%2C+Vinayak&amp;rft.au=Scicluna%2C+Matthew&amp;rft.au=Lacoste-Julien%2C+Simon&amp;rft.au=Mitliagkas%2C+Ioannis&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-islr-9"><span class="mw-cite-backlink">^ <a href="#cite_ref-islr_9-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-islr_9-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-islr_9-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation book">James, Gareth; <a href="/wiki/Daniela_Witten" title="Daniela Witten">Witten, Daniela</a>; <a href="/wiki/Trevor_Hastie" title="Trevor Hastie">Hastie, Trevor</a>; <a href="/wiki/Robert_Tibshirani" title="Robert Tibshirani">Tibshirani, Robert</a> (2013). <a class="external text" href="http://www-bcf.usc.edu/~gareth/ISL/" rel="nofollow"><i>An Introduction to Statistical Learning</i></a>. Springer.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=An+Introduction+to+Statistical+Learning&amp;rft.pub=Springer&amp;rft.date=2013&amp;rft.aulast=James&amp;rft.aufirst=Gareth&amp;rft.au=Witten%2C+Daniela&amp;rft.au=Hastie%2C+Trevor&amp;rft.au=Tibshirani%2C+Robert&amp;rft_id=http%3A%2F%2Fwww-bcf.usc.edu%2F~gareth%2FISL%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-ESL-10"><span class="mw-cite-backlink">^ <a href="#cite_ref-ESL_10-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ESL_10-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation book">Hastie, Trevor; Tibshirani, Robert; <a href="/wiki/Jerome_H._Friedman" title="Jerome H. Friedman">Friedman, Jerome H.</a> (2009). <a class="external text" href="https://web.archive.org/web/20150126123924/http://statweb.stanford.edu/~tibs/ElemStatLearn/" rel="nofollow"><i>The Elements of Statistical Learning</i></a>. Archived from <a class="external text" href="http://statweb.stanford.edu/~tibs/ElemStatLearn/" rel="nofollow">the original</a> on 2015-01-26<span class="reference-accessdate">. Retrieved <span class="nowrap">2014-08-20</span></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Elements+of+Statistical+Learning&amp;rft.date=2009&amp;rft.aulast=Hastie&amp;rft.aufirst=Trevor&amp;rft.au=Tibshirani%2C+Robert&amp;rft.au=Friedman%2C+Jerome+H.&amp;rft_id=http%3A%2F%2Fstatweb.stanford.edu%2F~tibs%2FElemStatLearn%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><cite class="citation web"><a href="/wiki/Sethu_Vijayakumar" title="Sethu Vijayakumar">Vijayakumar, Sethu</a> (2007). <a class="external text" href="http://www.inf.ed.ac.uk/teaching/courses/mlsc/Notes/Lecture4/BiasVariance.pdf" rel="nofollow">"The Bias–Variance Tradeoff"</a> <span class="cs1-format">(PDF)</span>. <a href="/wiki/University_of_Edinburgh" title="University of Edinburgh">University of Edinburgh</a><span class="reference-accessdate">. Retrieved <span class="nowrap">19 August</span> 2014</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+Bias%E2%80%93Variance+Tradeoff&amp;rft.pub=University+of+Edinburgh&amp;rft.date=2007&amp;rft.aulast=Vijayakumar&amp;rft.aufirst=Sethu&amp;rft_id=http%3A%2F%2Fwww.inf.ed.ac.uk%2Fteaching%2Fcourses%2Fmlsc%2FNotes%2FLecture4%2FBiasVariance.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><cite class="citation web">Shakhnarovich, Greg (2011). <a class="external text" href="https://web.archive.org/web/20140821063842/http://ttic.uchicago.edu/~gregory/courses/wis-ml2012/lectures/biasVarDecom.pdf" rel="nofollow">"Notes on derivation of bias-variance decomposition in linear regression"</a> <span class="cs1-format">(PDF)</span>. Archived from <a class="external text" href="http://ttic.uchicago.edu/~gregory/courses/wis-ml2012/lectures/biasVarDecom.pdf" rel="nofollow">the original</a> <span class="cs1-format">(PDF)</span> on 21 August 2014<span class="reference-accessdate">. Retrieved <span class="nowrap">20 August</span> 2014</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Notes+on+derivation+of+bias-variance+decomposition+in+linear+regression&amp;rft.date=2011&amp;rft.aulast=Shakhnarovich&amp;rft.aufirst=Greg&amp;rft_id=http%3A%2F%2Fttic.uchicago.edu%2F~gregory%2Fcourses%2Fwis-ml2012%2Flectures%2FbiasVarDecom.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><cite class="citation conference"><a href="/wiki/Pedro_Domingos" title="Pedro Domingos">Domingos, Pedro</a> (2000). <a class="external text" href="http://homes.cs.washington.edu/~pedrod/bvd.pdf" rel="nofollow"><i>A unified bias-variance decomposition</i></a> <span class="cs1-format">(PDF)</span>. ICML.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=A+unified+bias-variance+decomposition&amp;rft.date=2000&amp;rft.aulast=Domingos&amp;rft.aufirst=Pedro&amp;rft_id=http%3A%2F%2Fhomes.cs.washington.edu%2F~pedrod%2Fbvd.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><cite class="citation journal">Valentini, Giorgio; <a href="/wiki/Thomas_G._Dietterich" title="Thomas G. Dietterich">Dietterich, Thomas G.</a> (2004). <a class="external text" href="http://www.jmlr.org/papers/volume5/valentini04a/valentini04a.pdf" rel="nofollow">"Bias–variance analysis of support vector machines for the development of SVM-based ensemble methods"</a> <span class="cs1-format">(PDF)</span>. <i><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">Journal of Machine Learning Research</a></i>. <b>5</b>: 725–775.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Machine+Learning+Research&amp;rft.atitle=Bias%E2%80%93variance+analysis+of+support+vector+machines+for+the+development+of+SVM-based+ensemble+methods&amp;rft.volume=5&amp;rft.pages=725-775&amp;rft.date=2004&amp;rft.aulast=Valentini&amp;rft.aufirst=Giorgio&amp;rft.au=Dietterich%2C+Thomas+G.&amp;rft_id=http%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume5%2Fvalentini04a%2Fvalentini04a.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><cite class="citation book">Manning, Christopher D.; Raghavan, Prabhakar; Schütze, Hinrich (2008). <a class="external text" href="http://nlp.stanford.edu/IR-book/" rel="nofollow"><i>Introduction to Information Retrieval</i></a>. Cambridge University Press. pp. 308–314.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Introduction+to+Information+Retrieval&amp;rft.pages=308-314&amp;rft.pub=Cambridge+University+Press&amp;rft.date=2008&amp;rft.aulast=Manning&amp;rft.aufirst=Christopher+D.&amp;rft.au=Raghavan%2C+Prabhakar&amp;rft.au=Sch%C3%BCtze%2C+Hinrich&amp;rft_id=http%3A%2F%2Fnlp.stanford.edu%2FIR-book%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><cite class="citation journal">Francois-Lavet, Vincent; Rabusseau, Guillaume; Pineau, Joelle; Ernst, Damien; Fonteneau, Raphael (2019). <a class="external text" href="https://jair.org/index.php/jair/article/view/11478" rel="nofollow">"On Overﬁtting and Asymptotic Bias in Batch Reinforcement Learning with Partial Observability"</a>. <i>Journal of AI Research</i>. <b>65</b>: 1–30. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1613%2Fjair.1.11478" rel="nofollow">10.1613/jair.1.11478</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+AI+Research&amp;rft.atitle=On+Over%EF%AC%81tting+and+Asymptotic+Bias+in+Batch+Reinforcement+Learning+with+Partial+Observability&amp;rft.volume=65&amp;rft.pages=1-30&amp;rft.date=2019&amp;rft_id=info%3Adoi%2F10.1613%2Fjair.1.11478&amp;rft.aulast=Francois-Lavet&amp;rft.aufirst=Vincent&amp;rft.au=Rabusseau%2C+Guillaume&amp;rft.au=Pineau%2C+Joelle&amp;rft.au=Ernst%2C+Damien&amp;rft.au=Fonteneau%2C+Raphael&amp;rft_id=https%3A%2F%2Fjair.org%2Findex.php%2Fjair%2Farticle%2Fview%2F11478&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><cite class="citation book">Belsley, David (1991). <i>Conditioning diagnostics : collinearity and weak data in regression</i>. New York (NY): Wiley. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-0471528890" title="Special:BookSources/978-0471528890"><bdi>978-0471528890</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Conditioning+diagnostics+%3A+collinearity+and+weak+data+in+regression&amp;rft.place=New+York+%28NY%29&amp;rft.pub=Wiley&amp;rft.date=1991&amp;rft.isbn=978-0471528890&amp;rft.aulast=Belsley&amp;rft.aufirst=David&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><cite class="citation journal">Gagliardi, Francesco (May 2011). <a class="external text" href="https://www.researchgate.net/publication/51173579" rel="nofollow">"Instance-based classifiers applied to medical databases: diagnosis and knowledge extraction"</a>. <i>Artificial Intelligence in Medicine</i>. <b>52</b> (3): 123–139. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="https://doi.org/10.1016%2Fj.artmed.2011.04.002" rel="nofollow">10.1016/j.artmed.2011.04.002</a>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//pubmed.ncbi.nlm.nih.gov/21621400" rel="nofollow">21621400</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Artificial+Intelligence+in+Medicine&amp;rft.atitle=Instance-based+classifiers+applied+to+medical+databases%3A+diagnosis+and+knowledge+extraction&amp;rft.volume=52&amp;rft.issue=3&amp;rft.pages=123-139&amp;rft.date=2011-05&amp;rft_id=info%3Adoi%2F10.1016%2Fj.artmed.2011.04.002&amp;rft_id=info%3Apmid%2F21621400&amp;rft.aulast=Gagliardi&amp;rft.aufirst=Francesco&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F51173579&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><cite class="citation book">Ting, Jo-Anne; Vijaykumar, Sethu; Schaal, Stefan (2011). "Locally Weighted Regression for Control".  In Sammut, Claude; Webb, Geoffrey I. (eds.). <a class="external text" href="http://homepages.inf.ed.ac.uk/svijayak/publications/ting-EMLDM2016.pdf" rel="nofollow"><i>Encyclopedia of Machine Learning</i></a> <span class="cs1-format">(PDF)</span>. Springer. p. 615. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="https://ui.adsabs.harvard.edu/abs/2010eoml.book.....S" rel="nofollow">2010eoml.book.....S</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Locally+Weighted+Regression+for+Control&amp;rft.btitle=Encyclopedia+of+Machine+Learning&amp;rft.pages=615&amp;rft.pub=Springer&amp;rft.date=2011&amp;rft_id=info%3Abibcode%2F2010eoml.book.....S&amp;rft.aulast=Ting&amp;rft.aufirst=Jo-Anne&amp;rft.au=Vijaykumar%2C+Sethu&amp;rft.au=Schaal%2C+Stefan&amp;rft_id=http%3A%2F%2Fhomepages.inf.ed.ac.uk%2Fsvijayak%2Fpublications%2Fting-EMLDM2016.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><cite class="citation web">Fortmann-Roe, Scott (2012). <a class="external text" href="http://scott.fortmann-roe.com/docs/BiasVariance.html" rel="nofollow">"Understanding the Bias–Variance Tradeoff"</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Understanding+the+Bias%E2%80%93Variance+Tradeoff&amp;rft.date=2012&amp;rft.aulast=Fortmann-Roe&amp;rft.aufirst=Scott&amp;rft_id=http%3A%2F%2Fscott.fortmann-roe.com%2Fdocs%2FBiasVariance.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABias%E2%80%93variance+tradeoff"></span><link href="mw-data:TemplateStyles:r935243608" rel="mw-deduplicated-inline-style"/></span>
</li>
</ol></div></div>
<!-- 
NewPP limit report
Parsed by mw1387
Cached time: 20200402163351
Cache expiry: 2592000
Dynamic content: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.388 seconds
Real time usage: 0.599 seconds
Preprocessor visited node count: 1833/1000000
Post‐expand include size: 67727/2097152 bytes
Template argument size: 952/2097152 bytes
Highest expansion depth: 11/40
Expensive parser function count: 4/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 69800/5000000 bytes
Number of Wikibase entities loaded: 4/400
Lua time usage: 0.172/10.000 seconds
Lua memory usage: 4.49 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  365.446      1 -total
 66.55%  243.210      1 Template:Reflist
 35.15%  128.448      7 Template:Cite_journal
 15.64%   57.149      1 Template:Machine_learning_bar
 14.89%   54.421      1 Template:Sidebar_with_collapsible_lists
 10.10%   36.927      2 Template:Cite_arXiv
  8.27%   30.235      6 Template:Cite_book
  6.03%   22.040      1 Template:Multiple_image
  5.98%   21.871      1 Template:Longitem
  5.33%   19.480      1 Template:Nobold
-->
<!-- Saved in parser cache with key enwiki:pcache:idhash:40678189-0!canonical!math=5 and timestamp 20200402163350 and revision id 948382224
 -->
</div><noscript><img alt="" height="1" src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" style="border: none; position: absolute;" title="" width="1"/></noscript></div>
<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Bias–variance_tradeoff&amp;oldid=948382224">https://en.wikipedia.org/w/index.php?title=Bias–variance_tradeoff&amp;oldid=948382224</a>"</div>
<div class="catlinks" data-mw="interface" id="catlinks"><div class="mw-normal-catlinks" id="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Dilemmas" title="Category:Dilemmas">Dilemmas</a></li><li><a href="/wiki/Category:Model_selection" title="Category:Model selection">Model selection</a></li><li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li><li><a href="/wiki/Category:Statistical_classification" title="Category:Statistical classification">Statistical classification</a></li></ul></div></div>
<div class="visualClear"></div>
</div>
</div>
<div id="mw-data-after-content">
<div class="read-more-container"></div>
</div>
<div id="mw-navigation">
<h2>Navigation menu</h2>
<div id="mw-head">
<div aria-labelledby="p-personal-label" class="" id="p-personal" role="navigation">
<h3 id="p-personal-label">Personal tools</h3>
<ul>
<li id="pt-anonuserpage">Not logged in</li>
<li id="pt-anontalk"><a accesskey="n" href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]">Talk</a></li><li id="pt-anoncontribs"><a accesskey="y" href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Bias%E2%80%93variance+tradeoff" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a accesskey="o" href="/w/index.php?title=Special:UserLogin&amp;returnto=Bias%E2%80%93variance+tradeoff" title="You're encouraged to log in; however, it's not mandatory. [o]">Log in</a></li>
</ul>
</div>
<div id="left-navigation">
<div aria-labelledby="p-namespaces-label" class="vectorTabs" id="p-namespaces" role="navigation">
<h3 id="p-namespaces-label">Namespaces</h3>
<ul>
<li class="selected" id="ca-nstab-main"><a accesskey="c" href="/wiki/Bias%E2%80%93variance_tradeoff" title="View the content page [c]">Article</a></li><li id="ca-talk"><a accesskey="t" href="/wiki/Talk:Bias%E2%80%93variance_tradeoff" rel="discussion" title="Discussion about the content page [t]">Talk</a></li>
</ul>
</div>
<div aria-labelledby="p-variants-label" class="vectorMenu emptyPortlet" id="p-variants" role="navigation">
<input aria-labelledby="p-variants-label" class="vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-variants-label">
<span>Variants</span>
</h3>
<ul class="menu">
</ul>
</div>
</div>
<div id="right-navigation">
<div aria-labelledby="p-views-label" class="vectorTabs" id="p-views" role="navigation">
<h3 id="p-views-label">Views</h3>
<ul>
<li class="collapsible selected" id="ca-view"><a href="/wiki/Bias%E2%80%93variance_tradeoff">Read</a></li><li class="collapsible" id="ca-edit"><a accesskey="e" href="/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;action=edit" title="Edit this page [e]">Edit</a></li><li class="collapsible" id="ca-history"><a accesskey="h" href="/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;action=history" title="Past revisions of this page [h]">View history</a></li>
</ul>
</div>
<div aria-labelledby="p-cactions-label" class="vectorMenu emptyPortlet" id="p-cactions" role="navigation">
<input aria-labelledby="p-cactions-label" class="vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-cactions-label">
<span>More</span>
</h3>
<ul class="menu">
</ul>
</div>
<div id="p-search" role="search">
<h3>
<label for="searchInput">Search</label>
</h3>
<form action="/w/index.php" id="searchform">
<div id="simpleSearch">
<input accesskey="f" id="searchInput" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" type="search"/>
<input name="title" type="hidden" value="Special:Search"/>
<input class="searchButton mw-fallbackSearchButton" id="mw-searchButton" name="fulltext" title="Search Wikipedia for this text" type="submit" value="Search"/>
<input class="searchButton" id="searchButton" name="go" title="Go to a page with this exact name if it exists" type="submit" value="Go"/>
</div>
</form>
</div>
</div>
</div>
<div id="mw-panel">
<div id="p-logo" role="banner">
<a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a>
</div>
<div aria-labelledby="p-navigation-label" class="portal" id="p-navigation" role="navigation">
<h3 id="p-navigation-label">
    			Navigation
    		</h3>
<div class="body">
<ul><li id="n-mainpage-description"><a accesskey="z" href="/wiki/Main_Page" title="Visit the main page [z]">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Wikipedia:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a accesskey="x" href="/wiki/Special:Random" title="Load a random article [x]">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li></ul>
</div>
</div>
<div aria-labelledby="p-interaction-label" class="portal" id="p-interaction" role="navigation">
<h3 id="p-interaction-label">
    			Interaction
    		</h3>
<div class="body">
<ul><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a accesskey="r" href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li></ul>
</div>
</div>
<div aria-labelledby="p-tb-label" class="portal" id="p-tb" role="navigation">
<h3 id="p-tb-label">
    			Tools
    		</h3>
<div class="body">
<ul><li id="t-whatlinkshere"><a accesskey="j" href="/wiki/Special:WhatLinksHere/Bias%E2%80%93variance_tradeoff" title="List of all English Wikipedia pages containing links to this page [j]">What links here</a></li><li id="t-recentchangeslinked"><a accesskey="k" href="/wiki/Special:RecentChangesLinked/Bias%E2%80%93variance_tradeoff" rel="nofollow" title="Recent changes in pages linked from this page [k]">Related changes</a></li><li id="t-upload"><a accesskey="u" href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]">Upload file</a></li><li id="t-specialpages"><a accesskey="q" href="/wiki/Special:SpecialPages" title="A list of all special pages [q]">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;oldid=948382224" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a accesskey="g" href="https://www.wikidata.org/wiki/Special:EntityPage/Q17003119" title="Link to connected data repository item [g]">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Bias%E2%80%93variance_tradeoff&amp;id=948382224&amp;wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li></ul>
</div>
</div>
<div aria-labelledby="p-coll-print_export-label" class="portal" id="p-coll-print_export" role="navigation">
<h3 id="p-coll-print_export-label">
    			Print/export
    		</h3>
<div class="body">
<ul><li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Bias%E2%80%93variance+tradeoff">Create a book</a></li><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Bias%E2%80%93variance+tradeoff&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a accesskey="p" href="/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;printable=yes" title="Printable version of this page [p]">Printable version</a></li></ul>
</div>
</div>
<div aria-labelledby="p-lang-label" class="portal" id="p-lang" role="navigation">
<h3 id="p-lang-label">
    			Languages
    		</h3>
<div class="body">
<ul><li class="interlanguage-link interwiki-de"><a class="interlanguage-link-target" href="https://de.wikipedia.org/wiki/Verzerrung-Varianz-Dilemma" hreflang="de" lang="de" title="Verzerrung-Varianz-Dilemma – German">Deutsch</a></li><li class="interlanguage-link interwiki-fr"><a class="interlanguage-link-target" href="https://fr.wikipedia.org/wiki/Dilemme_biais-variance" hreflang="fr" lang="fr" title="Dilemme biais-variance – French">Français</a></li><li class="interlanguage-link interwiki-ko"><a class="interlanguage-link-target" href="https://ko.wikipedia.org/wiki/%ED%8E%B8%ED%96%A5-%EB%B6%84%EC%82%B0_%ED%8A%B8%EB%A0%88%EC%9D%B4%EB%93%9C%EC%98%A4%ED%94%84" hreflang="ko" lang="ko" title="편향-분산 트레이드오프 – Korean">한국어</a></li><li class="interlanguage-link interwiki-ja"><a class="interlanguage-link-target" href="https://ja.wikipedia.org/wiki/%E5%81%8F%E3%82%8A%E3%81%A8%E5%88%86%E6%95%A3" hreflang="ja" lang="ja" title="偏りと分散 – Japanese">日本語</a></li><li class="interlanguage-link interwiki-pl"><a class="interlanguage-link-target" href="https://pl.wikipedia.org/wiki/Kompromis_mi%C4%99dzy_obci%C4%85%C5%BCeniem_a_wariancj%C4%85" hreflang="pl" lang="pl" title="Kompromis między obciążeniem a wariancją – Polish">Polski</a></li><li class="interlanguage-link interwiki-ru"><a class="interlanguage-link-target" href="https://ru.wikipedia.org/wiki/%D0%94%D0%B8%D0%BB%D0%B5%D0%BC%D0%BC%D0%B0_%D1%81%D0%BC%D0%B5%D1%89%D0%B5%D0%BD%D0%B8%D1%8F%E2%80%93%D0%B4%D0%B8%D1%81%D0%BF%D0%B5%D1%80%D1%81%D0%B8%D0%B8" hreflang="ru" lang="ru" title="Дилемма смещения–дисперсии – Russian">Русский</a></li><li class="interlanguage-link interwiki-uk"><a class="interlanguage-link-target" href="https://uk.wikipedia.org/wiki/%D0%9A%D0%BE%D0%BC%D0%BF%D1%80%D0%BE%D0%BC%D1%96%D1%81_%D0%B7%D1%81%D1%83%D0%B2%D1%83_%D1%82%D0%B0_%D0%B4%D0%B8%D1%81%D0%BF%D0%B5%D1%80%D1%81%D1%96%D1%97" hreflang="uk" lang="uk" title="Компроміс зсуву та дисперсії – Ukrainian">Українська</a></li></ul>
<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a class="wbc-editpage" href="https://www.wikidata.org/wiki/Special:EntityPage/Q17003119#sitelinks-wikipedia" title="Edit interlanguage links">Edit links</a></span></div>
</div>
</div>
</div>
</div>
<div id="footer" role="contentinfo">
<ul class="" id="footer-info">
<li id="footer-info-lastmod"> This page was last edited on 31 March 2020, at 18:59<span class="anonymous-show"> (UTC)</span>.</li>
<li id="footer-info-copyright">Text is available under the <a href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" rel="license">Creative Commons Attribution-ShareAlike License</a><a href="//creativecommons.org/licenses/by-sa/3.0/" rel="license" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>
<ul class="" id="footer-places">
<li id="footer-places-privacy"><a class="extiw" href="https://foundation.wikimedia.org/wiki/Privacy_policy" title="wmf:Privacy policy">Privacy policy</a></li>
<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
<li id="footer-places-mobileview"><a class="noprint stopMobileRedirectToggle" href="//en.m.wikipedia.org/w/index.php?title=Bias%E2%80%93variance_tradeoff&amp;mobileaction=toggle_view_mobile">Mobile view</a></li>
</ul>
<ul class="noprint" id="footer-icons">
<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img alt="Wikimedia Foundation" height="31" src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88"/></a></li>
<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img alt="Powered by MediaWiki" height="31" src="/static/images/poweredby_mediawiki_88x31.png" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88"/></a></li>
</ul>
<div style="clear: both;"></div>
</div>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.388","walltime":"0.599","ppvisitednodes":{"value":1833,"limit":1000000},"postexpandincludesize":{"value":67727,"limit":2097152},"templateargumentsize":{"value":952,"limit":2097152},"expansiondepth":{"value":11,"limit":40},"expensivefunctioncount":{"value":4,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":69800,"limit":5000000},"entityaccesscount":{"value":4,"limit":400},"timingprofile":["100.00%  365.446      1 -total"," 66.55%  243.210      1 Template:Reflist"," 35.15%  128.448      7 Template:Cite_journal"," 15.64%   57.149      1 Template:Machine_learning_bar"," 14.89%   54.421      1 Template:Sidebar_with_collapsible_lists"," 10.10%   36.927      2 Template:Cite_arXiv","  8.27%   30.235      6 Template:Cite_book","  6.03%   22.040      1 Template:Multiple_image","  5.98%   21.871      1 Template:Longitem","  5.33%   19.480      1 Template:Nobold"]},"scribunto":{"limitreport-timeusage":{"value":"0.172","limit":"10.000"},"limitreport-memusage":{"value":4710563,"limit":52428800}},"cachereport":{"origin":"mw1387","timestamp":"20200402163351","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Bias\u2013variance tradeoff","url":"https:\/\/en.wikipedia.org\/wiki\/Bias%E2%80%93variance_tradeoff","sameAs":"http:\/\/www.wikidata.org\/entity\/Q17003119","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q17003119","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2013-10-01T13:44:23Z","dateModified":"2020-03-31T18:59:37Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/64\/Test_function_and_noisy_data.png","headline":"property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":86,"wgHostname":"mw1401"});});</script></body></html>
