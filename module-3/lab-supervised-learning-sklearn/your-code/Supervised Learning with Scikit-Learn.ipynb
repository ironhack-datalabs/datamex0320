{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning with Scikit-Learn\n",
    "\n",
    "\n",
    "**Lesson Goals**\n",
    "\n",
    "This lesson will serve as an introduction to supervised learning using Scikit-learn. Two important algorithms will be covered along with implementation and examples.\n",
    "\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "Supervised learning is an extremely important part of machine learning. This is because a large portion of machine learning algorithms are used for classification and regression. The scikit-learn has implementations for a large number of supervised learning algorithms. In this lesson we will explore two algorithms in depth.\n",
    "\n",
    "\n",
    "**Linear Regression**\n",
    "\n",
    "Definition\n",
    "\n",
    "Linear regression is one of the most used models in statistics. The general idea behind this model is that we have a predictor (or independent) variables and one or more response (also known as target or dependent) variables. We would like to to predict our response variable using a linear combination of the predictor variables. Typically, for a set of predictor variables X 1, X 2,..., X n, and a response variable Y, we construct the following model: \n",
    "\n",
    "![](../linreg.png)\n",
    "\n",
    "Where β 0, β 1,...,β n are constants that we compute. We find the optimal values of these constants for each model based on the data. We then generate predictions using this model. The difference between the observed values and the predicted values is called the error (or residual). Our goal is to minimize the error.\n",
    "Linear Regression with Scikit-learn\n",
    "\n",
    "Linear regression in scikit-learn is performed using the linear_regression submodule. To demonstrate a linear model with scikit-learn, we will use the beer dataset.\n",
    "\n",
    "First we import the dataset using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beer</th>\n",
       "      <th>tpc</th>\n",
       "      <th>ma</th>\n",
       "      <th>dsa</th>\n",
       "      <th>asa</th>\n",
       "      <th>orac</th>\n",
       "      <th>rp</th>\n",
       "      <th>mca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>148.23</td>\n",
       "      <td>13.37</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.81</td>\n",
       "      <td>3.81</td>\n",
       "      <td>0.45</td>\n",
       "      <td>10.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>160.38</td>\n",
       "      <td>10.96</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.41</td>\n",
       "      <td>15.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>170.41</td>\n",
       "      <td>9.22</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.81</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.48</td>\n",
       "      <td>15.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>208.65</td>\n",
       "      <td>9.65</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.50</td>\n",
       "      <td>76.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>146.03</td>\n",
       "      <td>11.72</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.90</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.47</td>\n",
       "      <td>9.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   beer     tpc     ma   dsa   asa  orac    rp    mca\n",
       "0     1  148.23  13.37  0.66  0.81  3.81  0.45  10.65\n",
       "1     2  160.38  10.96  0.63  0.64  2.85  0.41  15.47\n",
       "2     3  170.41   9.22  0.62  0.81  3.34  0.48  15.70\n",
       "3     4  208.65   9.65  0.90  1.01  3.34  0.50  76.65\n",
       "4     5  146.03  11.72  0.64  0.90  3.18  0.47   9.39"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "beer = pd.read_csv('../lager_antioxidant_reg.csv')\n",
    "beer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 7 variables :\n",
    "\n",
    "    tpc - Total phenolic content\n",
    "    ma - melanoidin content\n",
    "    dsa - DPPH radical scavenging activity\n",
    "    asa - ABTS radical cation scavenging activity\n",
    "    orac - Oxygen radical absorbance activity\n",
    "    rp - Reducing Power\n",
    "    mca - Metal Chelaing Activity\n",
    "\n",
    "The next step for scikit-learn is to separate the dataset into two parts - the predictor variables and the response variable. In this case we would like to predict the level of total phenolic content using the remaining 6 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns = [col for col in beer.columns.values if col != \"tpc\"]\n",
    "beer_x = beer[x_columns]\n",
    "beer_y = beer[\"tpc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.830383913148154"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_model = LinearRegression()\n",
    "#create the model\n",
    "beer_model.fit(beer_x, beer_y)\n",
    "#now we print the model coefficients\n",
    "beer_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.84731786e-02,  1.28827809e+00,  1.27650959e+02, -6.14737240e-01,\n",
       "       -1.09375291e+00,  7.35403422e+01,  3.76892085e-01])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8219280156188545"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score returns the coefficient of determination or r squared. \n",
    "#This number tells us what proportion of the variation in the data is explained by the model\n",
    "beer_model.score(beer_x, beer_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What these coefficients mean is that our linear model is:\n",
    "\n",
    "tpc = 19.049664352739313 = 1.28791969 * ma + 125.33843146 * dsa + (-0.92370963) * asa + (-0.93261523) * orac + 76.61686364 * rp + 0.38036155 * mca\n",
    "\n",
    "Typically, we perform a few diagnostic tests to ensure that a linear model is the most appropriate choice for this data.\n",
    "\n",
    "    The predictor variables are linearly independent\n",
    "    There is a linear relationship between predictors and response\n",
    "    The errors have a constant variance\n",
    "    The errors are normally distributed\n",
    "\n",
    "As far as testing assumptions, we will focus on the last two. We will plot the residuals vs. fit plot to diagnose a problem with assumption number 3. A model that meets this assumption will have a random pattern of points in this plot. This means that there is no trend in the variance of the residuals.\n",
    "\n",
    "This plot exists in the yellowbrick library. We will install this library and then use our existing linear model to plot the residual vs. fit graph.\n",
    "\n",
    "#!pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yellowbrick'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-70471f039a9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myellowbrick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregressor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResidualsPlot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvisualizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResidualsPlot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeer_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvisualizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeer_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeer_y\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Fit the training data to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvisualizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeer_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeer_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yellowbrick'"
     ]
    }
   ],
   "source": [
    "from yellowbrick.regressor import ResidualsPlot\n",
    "\n",
    "visualizer = ResidualsPlot(beer_model, hist=False)\n",
    "visualizer.fit(beer_x, beer_y)  # Fit the training data to the model\n",
    "print (visualizer.score(beer_x, beer_y))  \n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that except for one outlier, we have a fairly random pattern. So the assumption is met.\n",
    "\n",
    "Now we will look at the 4th assumption. In order to examine the distribution of the residuals, we can plot a Normal QQ plot of the residuals. This plot will compare the residuals with a theoretical normal distribution. If the graph of the actual vs. the theoretical will produce a linear pattern, this means that the residuals are approximately normally distributed.\n",
    "\n",
    "To do this, we use the statsmodels library\n",
    "\n",
    "#!pip install patsy\n",
    "#!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "predictions = beer_model.predict(beer_x)\n",
    "residuals = beer_y - predictions\n",
    "plot=sm.qqplot(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a linear relationship, we can assume that the residuals are normally distributed.\n",
    "\n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "While linear regression is used for predicting a numeric variable, logistic regression is used for classification. Logistic is used to explain a relationship between the predictor variables and a response variable(s) that can take values of either 0 or 1. Logistic regression does not need to satisfy the same assumptions as linear regression. The only assumptions we need to satisfy are that the predictor variables are independent of each other and not correlated with each other. We also need the response variable to be binary (meaning, have only two possible values) and the residuals to be independent of each other.\n",
    "\n",
    "Our regression equation is:\n",
    "    \n",
    "![](../logreg.png)\n",
    "\n",
    "Where p̂ (pronounced p hat) is the predicted probability of success. Notice that we have our regression equation in the exponent.\n",
    "Logistic Regression with Scikit-learn\n",
    "\n",
    "Here we use the linear_model submodule from scikit-learn as well. We will be applying the logistic regression model to the famous Titanic dataset from Kaggle.\n",
    "\n",
    "Before we apply the model to the data, we must do some essential munging.\n",
    "\n",
    "First, let's look at the data using the head function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('../titanic.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is a number of columns that convey information that cannot be modeled. Particularly the Name and Ticket columns. We will delete these features from the dataset. Additionally, the PassengerId column contains a number that is simply incremented with every row and contains no information about the data. We will drop this column as well.\n",
    "\n",
    "We also see that there are quite a few NaNs in the Cabin column. Let's investigate how many NaNs we have in each column to evaluate how to address the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_drop = titanic.drop(columns=['Name', 'Ticket', 'PassengerId'])\n",
    "titanic_drop.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the NaN count for each column. The Cabin column has 687 NaNs. With so much missing data, we are better off just dropping this column all together.\n",
    "\n",
    "We have identified 4 columns for dropping. Let's drop them using the drop function in Pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_drop = titanic.drop(columns=['Name', 'Ticket', 'PassengerId', 'Cabin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the remaining missing data, we will drop all rows that contain at least one NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_missing = titanic_drop.dropna()\n",
    "titanic_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_missing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 712 rows and 8 columns\n",
    "\n",
    "As we can see, there is still one more step before we can model the data, we need to create dummy variables out of the Pclass, Sex, and Embarked columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_with_dummies = pd.get_dummies(titanic_missing, columns=['Pclass', 'Sex', 'Embarked'], drop_first=True)\n",
    "titanic_with_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can perform the logistic regression. We start, as before, by separating the data into predictor and response variables. Then we create a model. We look at the r squared for the model using the score function. This number explains what percent of the variation in the data is explained by our model. The more variation our model can explain, the better it is at producing predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x_columns = [col for col in titanic_with_dummies.columns.values if col != \"Survived\"]\n",
    "titanic_x = titanic_with_dummies[x_columns]\n",
    "titanic_y = titanic_with_dummies[\"Survived\"]\n",
    "titanic_model = LogisticRegression(solver='lbfgs', max_iter=400)\n",
    "titanic_model.fit(titanic_x, titanic_y)\n",
    "titanic_model.score(titanic_x, titanic_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model predicts almost 80% of the variation in the data.\n",
    "\n",
    "\n",
    "**ROC Curve**\n",
    "\n",
    "The ROC (or Receiving Operator Characteristic) curve is a graph that gives us more information about how well our classification algorithm classifies our data. The goal is to increase the area under the curve as much as possible. If the area under the curve is below the y = x line, this means that our algorithm is worse than a coin flip. Therefore, we must aspire to be at least above that line. However, what we really aspire to is an area of 0.9 or higher.\n",
    "\n",
    "This plot utilizes matplotlib. Additionally, we will compute the true positive rate and false positive rate (tpr, fpr) to generate this plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred_proba = titanic_model.predict_proba(titanic_x)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(titanic_y,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(titanic_y, y_pred_proba)\n",
    "plt.plot(fpr,tpr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the area under the curve is larger than the x = y diagonal. In fact, we have computed it to be over 0.85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
