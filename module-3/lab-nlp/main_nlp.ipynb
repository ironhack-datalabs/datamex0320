{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "w-DDRME_YFcI",
        "ssT1s0qhHnVP",
        "6rXJbw-eYe8C"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-DDRME_YFcI",
        "colab_type": "text"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CoRWI6G-vwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgVax2nmXvQ0",
        "colab_type": "code",
        "outputId": "9555e138-d607-4def-9b51-7a499f0c8e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "from nltk import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords \n",
        "nltk.download('stopwords')\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer as sia\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssT1s0qhHnVP",
        "colab_type": "text"
      },
      "source": [
        "## Function text cleaning, tokenization, and lematization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDApbuHcGL4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "string = \"@Ironhack's-#Q website 776-is http://ironhack.com [(2018)]\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHpM158eYBIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function ctsl: Cleaning, Tokenization, Steaming and Lematization\n",
        "\n",
        "def ctsl(x):\n",
        "    \n",
        "    #Cleaning: Obtainig only letters and words\n",
        "    \n",
        "    x = str(x.lower())\n",
        "    words = []\n",
        "    replace_web = re.sub('http:(.*?)com|^[A-Za-z]+',' ',x)\n",
        "    extract_words = ' '.join(re.findall('[A-Z,a-z]+',replace_web))\n",
        "    extract_words = re.sub(',', ' ', extract_words)\n",
        "    #Tokenization: Seting to a words list format\n",
        "    \n",
        "    tokenization = word_tokenize(extract_words)\n",
        "    \n",
        "    # Steamming and lematization: Keeping only common terms in simmilar words  \n",
        "    \n",
        "    ste_and_lem = [WordNetLemmatizer().lemmatize(SnowballStemmer('english').stem(x)) for x in tokenization]\n",
        "    \n",
        "    # Stop words removal\n",
        "    \n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    removed_st = [w for w in ste_and_lem if not w in stop_words]  \n",
        "    \n",
        "    return removed_st"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhRrinZaCENk",
        "colab_type": "code",
        "outputId": "c919885b-ff72-499e-df99-0449d4e48e9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(ctsl(string))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ironhack', 'q', 'websit']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rXJbw-eYe8C",
        "colab_type": "text"
      },
      "source": [
        "## Conducting Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAsBJdWkGRpV",
        "colab_type": "code",
        "outputId": "66a7f027-63f2-4de4-db79-d0d9faf4276f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df = pd.read_csv('Sentiment140.csv', engine='python', encoding='utf-8', error_bad_lines=False).sample(20000)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>236161</th>\n",
              "      <td>0</td>\n",
              "      <td>1979927602</td>\n",
              "      <td>Sun May 31 05:03:25 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>KOLtwitbot</td>\n",
              "      <td>ZADIG ForEVER: ....I just realized I don't kno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25211</th>\n",
              "      <td>0</td>\n",
              "      <td>1558259660</td>\n",
              "      <td>Sun Apr 19 07:48:40 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>H_I_M</td>\n",
              "      <td>http://twitpic.com/3lh5q - It Won't Kill You To</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1329015</th>\n",
              "      <td>4</td>\n",
              "      <td>2015598936</td>\n",
              "      <td>Wed Jun 03 05:04:28 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>jiyoonee</td>\n",
              "      <td>@mindows98 awww good luck min  btw .. im here!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>541214</th>\n",
              "      <td>0</td>\n",
              "      <td>2200006819</td>\n",
              "      <td>Tue Jun 16 18:18:30 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Espinoza_</td>\n",
              "      <td>My computer officially doesn't not turn on</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516063</th>\n",
              "      <td>0</td>\n",
              "      <td>2190979977</td>\n",
              "      <td>Tue Jun 16 04:13:48 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>LoveShante</td>\n",
              "      <td>hey twitterboos i'm sick ya'll cramps</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         target  ...                                               text\n",
              "236161        0  ...  ZADIG ForEVER: ....I just realized I don't kno...\n",
              "25211         0  ...   http://twitpic.com/3lh5q - It Won't Kill You To \n",
              "1329015       4  ...     @mindows98 awww good luck min  btw .. im here!\n",
              "541214        0  ...        My computer officially doesn't not turn on \n",
              "516063        0  ...             hey twitterboos i'm sick ya'll cramps \n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZfjy25JGlO6",
        "colab_type": "code",
        "outputId": "3b40ad48-90f8-4cbc-dc79-5970e875fdb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "df['text_processed'] = df['text'].apply(ctsl)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "      <th>text_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>236161</th>\n",
              "      <td>0</td>\n",
              "      <td>1979927602</td>\n",
              "      <td>Sun May 31 05:03:25 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>KOLtwitbot</td>\n",
              "      <td>ZADIG ForEVER: ....I just realized I don't kno...</td>\n",
              "      <td>[forev, realiz, know, broken, car, antenna]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25211</th>\n",
              "      <td>0</td>\n",
              "      <td>1558259660</td>\n",
              "      <td>Sun Apr 19 07:48:40 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>H_I_M</td>\n",
              "      <td>http://twitpic.com/3lh5q - It Won't Kill You To</td>\n",
              "      <td>[lh, q, kill]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1329015</th>\n",
              "      <td>4</td>\n",
              "      <td>2015598936</td>\n",
              "      <td>Wed Jun 03 05:04:28 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>jiyoonee</td>\n",
              "      <td>@mindows98 awww good luck min  btw .. im here!</td>\n",
              "      <td>[mindow, awww, good, luck, min, btw, im]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>541214</th>\n",
              "      <td>0</td>\n",
              "      <td>2200006819</td>\n",
              "      <td>Tue Jun 16 18:18:30 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Espinoza_</td>\n",
              "      <td>My computer officially doesn't not turn on</td>\n",
              "      <td>[comput, offici, turn]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516063</th>\n",
              "      <td>0</td>\n",
              "      <td>2190979977</td>\n",
              "      <td>Tue Jun 16 04:13:48 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>LoveShante</td>\n",
              "      <td>hey twitterboos i'm sick ya'll cramps</td>\n",
              "      <td>[twitterboo, sick, ya, cramp]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         target  ...                               text_processed\n",
              "236161        0  ...  [forev, realiz, know, broken, car, antenna]\n",
              "25211         0  ...                                [lh, q, kill]\n",
              "1329015       4  ...     [mindow, awww, good, luck, min, btw, im]\n",
              "541214        0  ...                       [comput, offici, turn]\n",
              "516063        0  ...                [twitterboo, sick, ya, cramp]\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ6Va6k9Yy3I",
        "colab_type": "text"
      },
      "source": [
        "### Creating Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzNabQGyGptE",
        "colab_type": "code",
        "outputId": "83c32fe8-dc3b-4292-91f6-a1ce0a3d80ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "df2 = df.copy()\n",
        "words_list = []\n",
        "\n",
        "def bag(x):\n",
        "    for e in x:\n",
        "        words_list.append(e)\n",
        "        \n",
        "df2['text_processed'] = df2['text_processed'].apply(bag)\n",
        "\n",
        "\n",
        "counts = Counter(words_list).most_common(5000)\n",
        "counts[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('go', 1596),\n",
              " ('day', 1364),\n",
              " ('get', 1330),\n",
              " ('wa', 1286),\n",
              " ('work', 1006),\n",
              " ('like', 1001),\n",
              " ('love', 959),\n",
              " ('good', 948),\n",
              " ('quot', 910),\n",
              " ('u', 903)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVrZkjkRGs5E",
        "colab_type": "code",
        "outputId": "68f96dad-3c27-498a-d35f-d8297d0a2a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bag_words = [i for i,j in counts]\n",
        "bag_words[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['go', 'day', 'get', 'wa', 'work', 'like', 'love', 'good', 'quot', 'u']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2V0eEmuZFsP",
        "colab_type": "text"
      },
      "source": [
        "### Building Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAnKA5RaKvP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def features(x):\n",
        "    words = set(x)\n",
        "    features = {}\n",
        "    for i in bag_words:\n",
        "        features[i] = (i in words)\n",
        "        \n",
        "    s = sia().polarity_scores(\" \".join(x)) # nltk.sentiment.vader.SentimentIntensityAnalyzer: Give a sentiment intensity score to sentences.\n",
        "    if s[\"pos\"] > 0.2:\n",
        "        s = True\n",
        "    else:\n",
        "        s = False\n",
        "\n",
        "    return (features, s)\n",
        "\n",
        "feature = df.text_processed.apply(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxmkkIjga6Hy",
        "colab_type": "code",
        "outputId": "1e8271cf-59b7-4ded-985d-10ffb1dd3763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "feature[236161][1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03kZdlC8bp3J",
        "colab_type": "text"
      },
      "source": [
        "### Building and Traininng Naive Bayes Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfJFSAtea6LU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Link: https://pythonprogramming.net/naive-bayes-classifier-nltk-tutorial/?completed=/words-as-features-nltk-tutorial/\n",
        "\n",
        "# set that we'll train our classifier with\n",
        "training_set = feature[:1900]\n",
        "\n",
        "# set that we'll test against.\n",
        "testing_set = feature[1900:]\n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(training_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eyP70ooa6RG",
        "colab_type": "code",
        "outputId": "289f0dae-cc58-457b-c284-5e7ede522ba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier accuracy percent: 81.82872928176795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un6d4RpXQigs",
        "colab_type": "code",
        "outputId": "fbafacf0-6a68-489e-a506-44b7613ba2dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "classifier.show_most_informative_features(15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "                    haha = True             True : False  =     39.6 : 1.0\n",
            "                    good = True             True : False  =     28.9 : 1.0\n",
            "                    love = True             True : False  =     27.4 : 1.0\n",
            "                      ha = True             True : False  =     19.1 : 1.0\n",
            "                    wish = True             True : False  =     18.5 : 1.0\n",
            "                  friend = True             True : False  =     14.9 : 1.0\n",
            "                     yes = True             True : False  =     14.4 : 1.0\n",
            "                   thank = True             True : False  =     12.3 : 1.0\n",
            "                    hope = True             True : False  =     11.7 : 1.0\n",
            "                    play = True             True : False  =     11.2 : 1.0\n",
            "                    join = True             True : False  =      8.3 : 1.0\n",
            "                   sweet = True             True : False  =      8.3 : 1.0\n",
            "                    well = True             True : False  =      7.9 : 1.0\n",
            "                   dream = True             True : False  =      7.4 : 1.0\n",
            "                    like = True             True : False  =      6.7 : 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}